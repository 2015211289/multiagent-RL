2021-10-23 23:54:00.332246: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-23 23:54:00.336367: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-23 23:54:00.337784: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55a4dfbc4600 executing computations on platform Host. Devices:
2021-10-23 23:54:00.337815: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 2500.0, agent episode reward: [400.0, 2100.0], [401.7968690609323, 2100.0], time: 11.913
steps: 2000, episodes: 2, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.2623428473273, 200.0], time: 9.553
steps: 3000, episodes: 3, mean episode reward: 4400.0, agent episode reward: [4100.0, 300.0], [4101.622440354901, 300.0], time: 10.075
steps: 4000, episodes: 4, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.37966472954534, 400.0], time: 9.019
steps: 5000, episodes: 5, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.486652833056, 500.0], time: 9.899
steps: 6000, episodes: 6, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.4647980724907, 1500.0], time: 9.932
steps: 7000, episodes: 7, mean episode reward: 1600.0, agent episode reward: [1200.0, 400.0], [1201.6520803655105, 400.0], time: 10.32
steps: 8000, episodes: 8, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.44786574050653, 1400.0], time: 10.341
steps: 9000, episodes: 9, mean episode reward: 3300.0, agent episode reward: [1800.0, 1500.0], [1801.4358277812846, 1500.0], time: 10.554
steps: 10000, episodes: 10, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.4516462499901, 200.0], time: 10.806
steps: 11000, episodes: 11, mean episode reward: 400.0, agent episode reward: [400.0, 0.0], [401.0861248530347, 0.0], time: 10.981
steps: 11973, episodes: 12, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.12105151380365, 200.0], time: 11.253
StopIteration()
steps: 12973, episodes: 13, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.21633396810347, 200.0], time: 11.95
steps: 13973, episodes: 14, mean episode reward: 800.0, agent episode reward: [0.0, 800.0], [1.0759009968822815, 800.0], time: 11.594
steps: 14877, episodes: 15, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.00279393080618, 300.0], time: 10.185
steps: 15877, episodes: 16, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.35588718724966, 200.0], time: 11.645
steps: 16877, episodes: 17, mean episode reward: 1500.0, agent episode reward: [1000.0, 500.0], [1001.432004473446, 500.0], time: 12.254
steps: 17877, episodes: 18, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.41565367761793, 700.0], time: 11.733
steps: 18877, episodes: 19, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.51411378244234, 1500.0], time: 12.027
steps: 19877, episodes: 20, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.25769417052892, 700.0], time: 11.913
steps: 20877, episodes: 21, mean episode reward: 2800.0, agent episode reward: [1300.0, 1500.0], [1301.2633688239637, 1500.0], time: 12.744
steps: 21877, episodes: 22, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.55887664120365, 300.0], time: 12.247
steps: 22877, episodes: 23, mean episode reward: 2500.0, agent episode reward: [2100.0, 400.0], [2101.507154258261, 400.0], time: 12.504
StopIteration()
steps: 23877, episodes: 24, mean episode reward: 2600.0, agent episode reward: [1400.0, 1200.0], [1401.0738204404588, 1200.0], time: 13.03
steps: 24877, episodes: 25, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.45056842360196, 1500.0], time: 12.654
steps: 25877, episodes: 26, mean episode reward: 3500.0, agent episode reward: [1200.0, 2300.0], [1201.228840018174, 2300.0], time: 12.979
steps: 26877, episodes: 27, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.0843971116845, 400.0], time: 13.409
steps: 27550, episodes: 28, mean episode reward: 1400.0, agent episode reward: [100.0, 1300.0], [100.9683043337132, 1300.0], time: 9.304
steps: 28550, episodes: 29, mean episode reward: 700.0, agent episode reward: [200.0, 500.0], [201.42321728185584, 500.0], time: 13.02
steps: 29550, episodes: 30, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.30685678043164, 300.0], time: 13.674
steps: 30550, episodes: 31, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.4697783430599, 200.0], time: 13.802
steps: 31550, episodes: 32, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.32704232116546, 1400.0], time: 14.402
steps: 32550, episodes: 33, mean episode reward: 700.0, agent episode reward: [400.0, 300.0], [401.45303903302704, 300.0], time: 13.861
steps: 33550, episodes: 34, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.1852150427085, 400.0], time: 14.384
steps: 34550, episodes: 35, mean episode reward: 1400.0, agent episode reward: [800.0, 600.0], [801.4787256601479, 600.0], time: 14.23
steps: 35550, episodes: 36, mean episode reward: 1500.0, agent episode reward: [200.0, 1300.0], [201.237298130937, 1300.0], time: 14.899
steps: 36550, episodes: 37, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.4337340849168, 200.0], time: 14.193
steps: 37550, episodes: 38, mean episode reward: 1000.0, agent episode reward: [900.0, 100.0], [901.2019677969669, 100.0], time: 14.727
steps: 38550, episodes: 39, mean episode reward: 1700.0, agent episode reward: [1500.0, 200.0], [1501.322534448549, 200.0], time: 14.817
steps: 39550, episodes: 40, mean episode reward: 1300.0, agent episode reward: [200.0, 1100.0], [201.6155800335513, 1100.0], time: 15.527
steps: 40460, episodes: 41, mean episode reward: 1500.0, agent episode reward: [1300.0, 200.0], [1301.2578656069388, 200.0], time: 13.608
steps: 41460, episodes: 42, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.26268952392485, 100.0], time: 15.378
steps: 42460, episodes: 43, mean episode reward: 1100.0, agent episode reward: [100.0, 1000.0], [101.09302787306207, 1000.0], time: 15.579
steps: 43460, episodes: 44, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.3878348267085, 300.0], time: 16.492
steps: 44460, episodes: 45, mean episode reward: 3600.0, agent episode reward: [1400.0, 2200.0], [1401.4883566781225, 2200.0], time: 16.024
steps: 45460, episodes: 46, mean episode reward: 700.0, agent episode reward: [200.0, 500.0], [201.21268990083917, 500.0], time: 15.969
steps: 46460, episodes: 47, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.1678820602584, 200.0], time: 16.66
steps: 47460, episodes: 48, mean episode reward: 3500.0, agent episode reward: [2200.0, 1300.0], [2201.370784417242, 1300.0], time: 16.609
steps: 48460, episodes: 49, mean episode reward: 1800.0, agent episode reward: [1400.0, 400.0], [1401.5948758044672, 400.0], time: 16.837
steps: 49460, episodes: 50, mean episode reward: 1300.0, agent episode reward: [600.0, 700.0], [601.4295509695463, 700.0], time: 17.228
steps: 50460, episodes: 51, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.56764896593576, 500.0], time: 17.656
steps: 51460, episodes: 52, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.46303681184895, 300.0], time: 17.642
steps: 52460, episodes: 53, mean episode reward: 3400.0, agent episode reward: [1900.0, 1500.0], [1901.3353005253184, 1500.0], time: 17.883
steps: 53460, episodes: 54, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.4738176430444, 400.0], time: 17.939
steps: 54460, episodes: 55, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.38056190508195, 200.0], time: 19.577
steps: 55460, episodes: 56, mean episode reward: 1600.0, agent episode reward: [100.0, 1500.0], [101.59634544904284, 1500.0], time: 20.111
steps: 56460, episodes: 57, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.776664147612, 200.0], time: 19.261
steps: 57460, episodes: 58, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.5932109548636, 100.0], time: 19.755
steps: 58460, episodes: 59, mean episode reward: 1800.0, agent episode reward: [1600.0, 200.0], [1601.3796471827795, 200.0], time: 19.581
steps: 59460, episodes: 60, mean episode reward: 1300.0, agent episode reward: [100.0, 1200.0], [101.36967472599956, 1200.0], time: 19.737/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 60460, episodes: 61, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.3503274529734, 500.0], time: 20.157
steps: 61460, episodes: 62, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.28166589123734, 300.0], time: 20.635
steps: 62425, episodes: 63, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.24633299029483, 200.0], time: 20.155
steps: 63425, episodes: 64, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.411563971663, 200.0], time: 20.863
steps: 64425, episodes: 65, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.53992827767115, 300.0], time: 21.142
steps: 65425, episodes: 66, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.58393854066622, 400.0], time: 21.564
steps: 66324, episodes: 67, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [300.98194217260055, 100.0], time: 19.594
steps: 67324, episodes: 68, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.34703767435346, 400.0], time: 22.079
steps: 68324, episodes: 69, mean episode reward: 1500.0, agent episode reward: [1200.0, 300.0], [1201.3138995747036, 300.0], time: 22.515
steps: 69324, episodes: 70, mean episode reward: 2800.0, agent episode reward: [2600.0, 200.0], [2601.4350880482184, 200.0], time: 23.015
steps: 70324, episodes: 71, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.5329365084179, 1200.0], time: 22.748
steps: 71324, episodes: 72, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.3586139979789, 200.0], time: 23.681
steps: 72324, episodes: 73, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.4402533138033, 500.0], time: 23.255
steps: 73324, episodes: 74, mean episode reward: 1800.0, agent episode reward: [500.0, 1300.0], [501.50687876422205, 1300.0], time: 23.767
steps: 74324, episodes: 75, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.3901083491062, 300.0], time: 24.279
steps: 75324, episodes: 76, mean episode reward: 1500.0, agent episode reward: [1200.0, 300.0], [1201.6432834185796, 300.0], time: 24.299
steps: 76324, episodes: 77, mean episode reward: 1100.0, agent episode reward: [200.0, 900.0], [201.19714598163114, 900.0], time: 25.908
steps: 77324, episodes: 78, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.5917895730853, 200.0], time: 25.372
steps: 78324, episodes: 79, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.18562093229224, 200.0], time: 25.648
steps: 79324, episodes: 80, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.22187454164185, 400.0], time: 25.52
steps: 80324, episodes: 81, mean episode reward: 1100.0, agent episode reward: [800.0, 300.0], [801.2225734110913, 300.0], time: 25.959
steps: 81324, episodes: 82, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.2923667511727, 100.0], time: 26.167
steps: 82324, episodes: 83, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.2446095713038, 200.0], time: 26.349
steps: 83324, episodes: 84, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.48138709526745, 400.0], time: 26.769
steps: 84324, episodes: 85, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.40555067094772, 300.0], time: 27.058
steps: 85324, episodes: 86, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.5101609376053, 400.0], time: 27.487
steps: 86324, episodes: 87, mean episode reward: 1300.0, agent episode reward: [700.0, 600.0], [701.4984751008469, 600.0], time: 27.658
steps: 87318, episodes: 88, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.2472477460618, 1200.0], time: 28.035
steps: 88318, episodes: 89, mean episode reward: 1300.0, agent episode reward: [300.0, 1000.0], [301.5435664417154, 1000.0], time: 28.436
steps: 89318, episodes: 90, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.2999602477451, 100.0], time: 28.483
steps: 90318, episodes: 91, mean episode reward: 1800.0, agent episode reward: [500.0, 1300.0], [501.31952366959797, 1300.0], time: 29.042
steps: 91318, episodes: 92, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.59845478195295, 700.0], time: 29.972
steps: 92318, episodes: 93, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.58580996868926, 400.0], time: 29.76
steps: 93291, episodes: 94, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.1815423220246, 200.0], time: 29.101
steps: 94291, episodes: 95, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.63948655769137, 500.0], time: 30.165
steps: 95291, episodes: 96, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.5554315923278, 700.0], time: 30.537
steps: 96291, episodes: 97, mean episode reward: 2300.0, agent episode reward: [400.0, 1900.0], [401.4969570229678, 1900.0], time: 30.964
steps: 97291, episodes: 98, mean episode reward: 1800.0, agent episode reward: [1300.0, 500.0], [1301.5314434105692, 500.0], time: 31.466
steps: 98291, episodes: 99, mean episode reward: 3100.0, agent episode reward: [200.0, 2900.0], [201.42439379164406, 2900.0], time: 31.751
steps: 99291, episodes: 100, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.4947840232617, 500.0], time: 31.846
...Finished total of 101 episodes.
2021-10-24 00:25:16.461698: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 00:25:16.465723: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 00:25:16.466210: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5610aa7945f0 executing computations on platform Host. Devices:
2021-10-24 00:25:16.466232: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 3500.0, agent episode reward: [1600.0, 1900.0], [1601.8468757753142, 1900.0], time: 5.633
steps: 2000, episodes: 2, mean episode reward: 500.0, agent episode reward: [0.0, 500.0], [1.5307589918732876, 500.0], time: 8.501
steps: 3000, episodes: 3, mean episode reward: 2300.0, agent episode reward: [1900.0, 400.0], [1901.8059323159498, 400.0], time: 8.632
steps: 4000, episodes: 4, mean episode reward: 1300.0, agent episode reward: [300.0, 1000.0], [301.8393825913181, 1000.0], time: 8.994
steps: 5000, episodes: 5, mean episode reward: 2600.0, agent episode reward: [1200.0, 1400.0], [1201.6121044079534, 1400.0], time: 8.801
steps: 6000, episodes: 6, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.544738394119, 400.0], time: 9.77
steps: 7000, episodes: 7, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.5526869383257, 300.0], time: 9.876
steps: 8000, episodes: 8, mean episode reward: 1300.0, agent episode reward: [700.0, 600.0], [701.4222397763823, 600.0], time: 9.744
steps: 9000, episodes: 9, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.43524644763045, 300.0], time: 9.898
steps: 10000, episodes: 10, mean episode reward: 1300.0, agent episode reward: [200.0, 1100.0], [201.38972189761526, 1100.0], time: 11.0
StopIteration()
steps: 11000, episodes: 11, mean episode reward: 2200.0, agent episode reward: [2000.0, 200.0], [2001.3340975505373, 200.0], time: 10.605
steps: 12000, episodes: 12, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [101.68804005587981, 1400.0], time: 10.705
steps: 13000, episodes: 13, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.44829043924693, 200.0], time: 11.087
steps: 14000, episodes: 14, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.37583523251982, 400.0], time: 11.637
steps: 15000, episodes: 15, mean episode reward: 3300.0, agent episode reward: [1200.0, 2100.0], [1201.4879092541762, 2100.0], time: 11.235
steps: 16000, episodes: 16, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.4059864997065, 200.0], time: 11.2
steps: 17000, episodes: 17, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.6558283264103, 300.0], time: 11.414
steps: 18000, episodes: 18, mean episode reward: 2200.0, agent episode reward: [100.0, 2100.0], [101.25968079315733, 2100.0], time: 12.169
steps: 18785, episodes: 19, mean episode reward: 1400.0, agent episode reward: [100.0, 1300.0], [101.13006485604069, 1300.0], time: 9.034
steps: 19626, episodes: 20, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.31516527477487, 100.0], time: 10.016
steps: 20626, episodes: 21, mean episode reward: 1300.0, agent episode reward: [700.0, 600.0], [701.4819299213693, 600.0], time: 11.771
steps: 21626, episodes: 22, mean episode reward: 600.0, agent episode reward: [600.0, 0.0], [601.553536280583, 0.0], time: 12.293
steps: 22626, episodes: 23, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.5055724772429, 1600.0], time: 12.019
steps: 23626, episodes: 24, mean episode reward: 800.0, agent episode reward: [800.0, 0.0], [801.4046113091008, 0.0], time: 12.441
steps: 24626, episodes: 25, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.5544832382983, 400.0], time: 12.523
steps: 25626, episodes: 26, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.38576747824885, 400.0], time: 13.049
StopIteration()
steps: 26626, episodes: 27, mean episode reward: 2300.0, agent episode reward: [2300.0, 0.0], [2301.2401797944844, 0.0], time: 12.626
steps: 27626, episodes: 28, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.4864231701945, 1400.0], time: 12.92
steps: 28626, episodes: 29, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.3794064153916, 300.0], time: 12.847
steps: 29626, episodes: 30, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.65137870534537, 600.0], time: 13.685
steps: 30626, episodes: 31, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.4446517121655, 400.0], time: 13.129
steps: 31626, episodes: 32, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.42898781599663, 600.0], time: 13.564
steps: 32626, episodes: 33, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.61332875491092, 500.0], time: 13.631
steps: 33626, episodes: 34, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.54407697687313, 400.0], time: 14.338
steps: 34626, episodes: 35, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.37022886257543, 400.0], time: 13.995
steps: 35626, episodes: 36, mean episode reward: 1500.0, agent episode reward: [500.0, 1000.0], [501.90915692177555, 1000.0], time: 14.05
StopIteration()
steps: 36626, episodes: 37, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.35566329270257, 200.0], time: 14.791
steps: 37626, episodes: 38, mean episode reward: 1800.0, agent episode reward: [1400.0, 400.0], [1401.4301140054927, 400.0], time: 14.71
steps: 38626, episodes: 39, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.6011974330116, 500.0], time: 14.56
steps: 39626, episodes: 40, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.7482589240461, 200.0], time: 14.755
steps: 40554, episodes: 41, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.23868143110037, 700.0], time: 14.352
steps: 41554, episodes: 42, mean episode reward: 2600.0, agent episode reward: [1200.0, 1400.0], [1201.6250112865364, 1400.0], time: 15.714
steps: 42554, episodes: 43, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.28943720597005, 300.0], time: 15.426
steps: 43554, episodes: 44, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.779425827006, 200.0], time: 15.681
steps: 44554, episodes: 45, mean episode reward: 1600.0, agent episode reward: [400.0, 1200.0], [401.839596614364, 1200.0], time: 16.45
steps: 45554, episodes: 46, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.58994810178186, 400.0], time: 16.297
steps: 46554, episodes: 47, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.65318031130084, 200.0], time: 16.688
steps: 47554, episodes: 48, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.4498166912707, 600.0], time: 16.532
steps: 48554, episodes: 49, mean episode reward: 2500.0, agent episode reward: [2300.0, 200.0], [2301.7972502510747, 200.0], time: 16.754
steps: 49554, episodes: 50, mean episode reward: 1300.0, agent episode reward: [200.0, 1100.0], [201.4208891505088, 1100.0], time: 17.421
steps: 50554, episodes: 51, mean episode reward: 600.0, agent episode reward: [0.0, 600.0], [1.1948250162951741, 600.0], time: 17.521
steps: 51554, episodes: 52, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.85361473936723, 1200.0], time: 17.292
steps: 52554, episodes: 53, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.67081316217417, 500.0], time: 17.682
steps: 53554, episodes: 54, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.79255107426593, 300.0], time: 18.031
steps: 54554, episodes: 55, mean episode reward: 1800.0, agent episode reward: [100.0, 1700.0], [101.41846825723299, 1700.0], time: 18.778
steps: 55554, episodes: 56, mean episode reward: 800.0, agent episode reward: [500.0, 300.0], [501.38788877197976, 300.0], time: 18.488
steps: 56554, episodes: 57, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.39719671979856, 500.0], time: 18.866
steps: 57554, episodes: 58, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.66206091487004, 300.0], time: 19.484
steps: 58554, episodes: 59, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.5172551063307, 100.0], time: 19.197
steps: 59554, episodes: 60, mean episode reward: 2500.0, agent episode reward: [1300.0, 1200.0], [1301.7819942036622, 1200.0], time: 19.583/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 60554, episodes: 61, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.6002093213806, 300.0], time: 19.735
steps: 61554, episodes: 62, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.41979716425604, 700.0], time: 20.405
steps: 62554, episodes: 63, mean episode reward: 1500.0, agent episode reward: [1300.0, 200.0], [1301.779707749126, 200.0], time: 20.577
steps: 63554, episodes: 64, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.7934636704473, 300.0], time: 20.731
steps: 64554, episodes: 65, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.7588105991603, 200.0], time: 20.847
steps: 65343, episodes: 66, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.00308240704214, 200.0], time: 16.84
steps: 66343, episodes: 67, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.46972653129924, 300.0], time: 21.393
steps: 67343, episodes: 68, mean episode reward: 1400.0, agent episode reward: [100.0, 1300.0], [101.7285597252572, 1300.0], time: 21.551
steps: 68343, episodes: 69, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.38789883348552, 400.0], time: 21.846
steps: 69343, episodes: 70, mean episode reward: 1800.0, agent episode reward: [0.0, 1800.0], [1.2086719102845558, 1800.0], time: 22.731
steps: 70343, episodes: 71, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.6893242398605, 700.0], time: 23.007
steps: 71343, episodes: 72, mean episode reward: 300.0, agent episode reward: [200.0, 100.0], [201.21329226387613, 100.0], time: 23.002
steps: 72343, episodes: 73, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.48596484213533, 500.0], time: 23.22
steps: 73343, episodes: 74, mean episode reward: 1300.0, agent episode reward: [900.0, 400.0], [901.4305175906095, 400.0], time: 23.896
steps: 74343, episodes: 75, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.55475841130882, 300.0], time: 23.789
steps: 75343, episodes: 76, mean episode reward: 500.0, agent episode reward: [0.0, 500.0], [1.242557511140158, 500.0], time: 24.068
steps: 76343, episodes: 77, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.5543717505448, 400.0], time: 24.403
steps: 77343, episodes: 78, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.40078208306545, 300.0], time: 25.203
steps: 78343, episodes: 79, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.52036211903564, 200.0], time: 25.269
steps: 79343, episodes: 80, mean episode reward: 1800.0, agent episode reward: [1300.0, 500.0], [1301.5927657054044, 500.0], time: 25.594
steps: 80343, episodes: 81, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.4927792515242, 400.0], time: 26.196
steps: 81313, episodes: 82, mean episode reward: 1400.0, agent episode reward: [1200.0, 200.0], [1201.638104840458, 200.0], time: 25.671
steps: 82313, episodes: 83, mean episode reward: 1300.0, agent episode reward: [1000.0, 300.0], [1001.2829194978693, 300.0], time: 26.195
steps: 83313, episodes: 84, mean episode reward: 900.0, agent episode reward: [0.0, 900.0], [1.2396019948699817, 900.0], time: 26.443
steps: 84313, episodes: 85, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.41466817602898, 400.0], time: 27.005
steps: 85313, episodes: 86, mean episode reward: 2100.0, agent episode reward: [300.0, 1800.0], [301.32551884510997, 1800.0], time: 27.777
steps: 86313, episodes: 87, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.47811440619145, 300.0], time: 27.639
steps: 87313, episodes: 88, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.5530691002804, 300.0], time: 28.182
steps: 88313, episodes: 89, mean episode reward: 2000.0, agent episode reward: [1700.0, 300.0], [1701.489828528798, 300.0], time: 28.271
steps: 89313, episodes: 90, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.63958738195248, 200.0], time: 28.873
steps: 90313, episodes: 91, mean episode reward: 600.0, agent episode reward: [600.0, 0.0], [601.3615900394404, 0.0], time: 29.22
steps: 91313, episodes: 92, mean episode reward: 400.0, agent episode reward: [0.0, 400.0], [1.379001175675465, 400.0], time: 29.523
steps: 92313, episodes: 93, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.30085867489532, 400.0], time: 29.797
steps: 93313, episodes: 94, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.5741461424354, 1600.0], time: 30.067
steps: 94313, episodes: 95, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.44350128127735, 300.0], time: 30.49
steps: 95313, episodes: 96, mean episode reward: 2600.0, agent episode reward: [100.0, 2500.0], [101.29595454374513, 2500.0], time: 30.861
steps: 96313, episodes: 97, mean episode reward: 200.0, agent episode reward: [100.0, 100.0], [101.22260814309399, 100.0], time: 31.244
steps: 97313, episodes: 98, mean episode reward: 2800.0, agent episode reward: [1200.0, 1600.0], [1201.4091497299098, 1600.0], time: 31.37
steps: 98313, episodes: 99, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.70507051727446, 300.0], time: 31.76
steps: 99313, episodes: 100, mean episode reward: 1300.0, agent episode reward: [200.0, 1100.0], [201.74306809767515, 1100.0], time: 32.116
...Finished total of 101 episodes.
2021-10-24 00:56:00.476726: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 00:56:00.481035: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 00:56:00.481282: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55df4af0ebc0 executing computations on platform Host. Devices:
2021-10-24 00:56:00.481302: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 800.0, agent episode reward: [800.0, 0.0], [801.9880640508617, 0.0], time: 5.751
steps: 2000, episodes: 2, mean episode reward: 700.0, agent episode reward: [200.0, 500.0], [201.17041412263148, 500.0], time: 8.426
steps: 3000, episodes: 3, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.4381393679225, 500.0], time: 8.733
steps: 4000, episodes: 4, mean episode reward: 1300.0, agent episode reward: [700.0, 600.0], [701.4000250710301, 600.0], time: 9.282
steps: 4887, episodes: 5, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.35121036885707, 300.0], time: 7.875
steps: 5887, episodes: 6, mean episode reward: 1600.0, agent episode reward: [100.0, 1500.0], [101.290735239043, 1500.0], time: 10.239
steps: 6887, episodes: 7, mean episode reward: 2300.0, agent episode reward: [1800.0, 500.0], [1801.6561440774335, 500.0], time: 9.936
steps: 7887, episodes: 8, mean episode reward: 1400.0, agent episode reward: [400.0, 1000.0], [401.4294975425045, 1000.0], time: 9.865
steps: 8887, episodes: 9, mean episode reward: 1400.0, agent episode reward: [200.0, 1200.0], [201.60324455197375, 1200.0], time: 9.751
steps: 9881, episodes: 10, mean episode reward: 1700.0, agent episode reward: [100.0, 1600.0], [101.46420049709134, 1600.0], time: 10.741
steps: 10881, episodes: 11, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.52487536773297, 500.0], time: 10.735
steps: 11881, episodes: 12, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.39129478861813, 200.0], time: 10.704
steps: 12881, episodes: 13, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.36520500226203, 200.0], time: 11.125
steps: 13881, episodes: 14, mean episode reward: 1300.0, agent episode reward: [200.0, 1100.0], [201.56410116016545, 1100.0], time: 11.242
steps: 14881, episodes: 15, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.5352802645962, 1200.0], time: 11.222
steps: 15881, episodes: 16, mean episode reward: 700.0, agent episode reward: [500.0, 200.0], [501.49901839410785, 200.0], time: 11.181
steps: 16881, episodes: 17, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.30364942717557, 100.0], time: 11.991
steps: 17881, episodes: 18, mean episode reward: 2500.0, agent episode reward: [2300.0, 200.0], [2301.535255217348, 200.0], time: 12.215
steps: 18881, episodes: 19, mean episode reward: 800.0, agent episode reward: [500.0, 300.0], [501.3889367094298, 300.0], time: 11.803
steps: 19881, episodes: 20, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.57697728730284, 700.0], time: 11.871
steps: 20881, episodes: 21, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.40810570050758, 500.0], time: 12.077
steps: 21854, episodes: 22, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.427569403, 300.0], time: 12.291
steps: 22854, episodes: 23, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.46006433572904, 200.0], time: 12.028
steps: 23812, episodes: 24, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.46058297397127, 200.0], time: 11.932
steps: 24720, episodes: 25, mean episode reward: 200.0, agent episode reward: [0.0, 200.0], [1.0403551740445416, 200.0], time: 11.321
steps: 25720, episodes: 26, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.52293724616672, 300.0], time: 13.116
steps: 26720, episodes: 27, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.32274756321888, 1600.0], time: 12.74
steps: 27720, episodes: 28, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.497531574489, 1200.0], time: 12.788
steps: 28720, episodes: 29, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.45727782492077, 400.0], time: 13.631
steps: 29720, episodes: 30, mean episode reward: 1300.0, agent episode reward: [800.0, 500.0], [801.3278283049757, 500.0], time: 13.107
steps: 30720, episodes: 31, mean episode reward: 1300.0, agent episode reward: [700.0, 600.0], [701.3968495778365, 600.0], time: 13.501
steps: 31503, episodes: 32, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.16562352055433, 300.0], time: 10.31
steps: 32503, episodes: 33, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.5666836548785, 200.0], time: 13.748
steps: 33503, episodes: 34, mean episode reward: 1300.0, agent episode reward: [100.0, 1200.0], [101.51735552593047, 1200.0], time: 13.734
steps: 34503, episodes: 35, mean episode reward: 1800.0, agent episode reward: [1300.0, 500.0], [1301.3708320412313, 500.0], time: 13.834
steps: 35503, episodes: 36, mean episode reward: 2800.0, agent episode reward: [200.0, 2600.0], [201.53318106069418, 2600.0], time: 14.514
steps: 36503, episodes: 37, mean episode reward: 800.0, agent episode reward: [500.0, 300.0], [501.42209318361415, 300.0], time: 14.583
steps: 37503, episodes: 38, mean episode reward: 4000.0, agent episode reward: [3500.0, 500.0], [3501.5037674611017, 500.0], time: 14.235
steps: 38503, episodes: 39, mean episode reward: 2600.0, agent episode reward: [1300.0, 1300.0], [1301.4030512194277, 1300.0], time: 14.515
steps: 39503, episodes: 40, mean episode reward: 2400.0, agent episode reward: [2100.0, 300.0], [2101.548143999818, 300.0], time: 14.721
steps: 40503, episodes: 41, mean episode reward: 1600.0, agent episode reward: [1200.0, 400.0], [1201.398125135687, 400.0], time: 15.652
steps: 41503, episodes: 42, mean episode reward: 2300.0, agent episode reward: [1900.0, 400.0], [1901.6223172577097, 400.0], time: 15.059
steps: 42503, episodes: 43, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.360350789381, 100.0], time: 15.341
steps: 43503, episodes: 44, mean episode reward: 2600.0, agent episode reward: [1200.0, 1400.0], [1201.41519308031, 1400.0], time: 15.668
steps: 44503, episodes: 45, mean episode reward: 1300.0, agent episode reward: [1000.0, 300.0], [1001.6172405315584, 300.0], time: 16.187
steps: 45503, episodes: 46, mean episode reward: 2300.0, agent episode reward: [0.0, 2300.0], [1.592632777046145, 2300.0], time: 15.957
steps: 46503, episodes: 47, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.63586964119705, 1400.0], time: 16.085
steps: 47503, episodes: 48, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.61148409609574, 400.0], time: 16.306
steps: 48503, episodes: 49, mean episode reward: 2300.0, agent episode reward: [200.0, 2100.0], [201.69814525324873, 2100.0], time: 16.91
steps: 49503, episodes: 50, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.74623592900105, 200.0], time: 16.751
steps: 50503, episodes: 51, mean episode reward: 900.0, agent episode reward: [900.0, 0.0], [901.7484542465953, 0.0], time: 17.114
steps: 51503, episodes: 52, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.61672321174552, 400.0], time: 17.214
steps: 52503, episodes: 53, mean episode reward: 1700.0, agent episode reward: [1500.0, 200.0], [1501.6983464807818, 200.0], time: 17.919
steps: 53503, episodes: 54, mean episode reward: 1400.0, agent episode reward: [200.0, 1200.0], [201.6428391993546, 1200.0], time: 17.804
steps: 54503, episodes: 55, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.40092192482354, 1500.0], time: 17.865
steps: 55503, episodes: 56, mean episode reward: 2500.0, agent episode reward: [2300.0, 200.0], [2301.7909008146553, 200.0], time: 18.813
steps: 56503, episodes: 57, mean episode reward: 2800.0, agent episode reward: [0.0, 2800.0], [1.1410282417434285, 2800.0], time: 19.258
steps: 57503, episodes: 58, mean episode reward: 1400.0, agent episode reward: [100.0, 1300.0], [101.48122562113922, 1300.0], time: 19.289
steps: 58503, episodes: 59, mean episode reward: 1300.0, agent episode reward: [600.0, 700.0], [601.4498908266922, 700.0], time: 18.886
steps: 59503, episodes: 60, mean episode reward: 600.0, agent episode reward: [0.0, 600.0], [1.5837925975396523, 600.0], time: 19.5/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 60235, episodes: 61, mean episode reward: 1500.0, agent episode reward: [1200.0, 300.0], [1201.040935775617, 300.0], time: 14.604
steps: 61235, episodes: 62, mean episode reward: 2400.0, agent episode reward: [300.0, 2100.0], [301.6754046397288, 2100.0], time: 20.3
steps: 62235, episodes: 63, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.4270115973069, 300.0], time: 20.038
steps: 63235, episodes: 64, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.62581101939901, 400.0], time: 20.759
steps: 64235, episodes: 65, mean episode reward: 4000.0, agent episode reward: [100.0, 3900.0], [101.25443676636283, 3900.0], time: 20.805
steps: 65235, episodes: 66, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.30435951276493, 600.0], time: 21.272
steps: 66235, episodes: 67, mean episode reward: 2500.0, agent episode reward: [1200.0, 1300.0], [1201.7215443034088, 1300.0], time: 21.847
steps: 67235, episodes: 68, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.67992082835903, 300.0], time: 21.507
steps: 68235, episodes: 69, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.43459407936936, 400.0], time: 21.998
steps: 69235, episodes: 70, mean episode reward: 2300.0, agent episode reward: [1100.0, 1200.0], [1101.413956189311, 1200.0], time: 22.427
steps: 70131, episodes: 71, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.15623491296007, 300.0], time: 20.088
steps: 70960, episodes: 72, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.2114478596634, 300.0], time: 18.71
steps: 71960, episodes: 73, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.87033208920177, 300.0], time: 23.352
steps: 72960, episodes: 74, mean episode reward: 1600.0, agent episode reward: [400.0, 1200.0], [401.45730559891723, 1200.0], time: 23.339
steps: 73960, episodes: 75, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.5125540041288, 300.0], time: 23.863
steps: 74960, episodes: 76, mean episode reward: 600.0, agent episode reward: [600.0, 0.0], [601.6990450774659, 0.0], time: 23.961
steps: 75960, episodes: 77, mean episode reward: 1400.0, agent episode reward: [800.0, 600.0], [801.5107214137093, 600.0], time: 24.667
steps: 76960, episodes: 78, mean episode reward: 1300.0, agent episode reward: [500.0, 800.0], [501.5906221042128, 800.0], time: 24.634
steps: 77960, episodes: 79, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.47955376565525, 600.0], time: 24.534
steps: 78960, episodes: 80, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.468869438625, 300.0], time: 25.026
steps: 79960, episodes: 81, mean episode reward: 1600.0, agent episode reward: [1200.0, 400.0], [1201.8144730400345, 400.0], time: 25.649
steps: 80960, episodes: 82, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.85650934174487, 200.0], time: 26.403
steps: 81960, episodes: 83, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.5874319827008, 600.0], time: 26.109
steps: 82960, episodes: 84, mean episode reward: 1300.0, agent episode reward: [700.0, 600.0], [701.4978647617133, 600.0], time: 26.315
steps: 83960, episodes: 85, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.6453432031851, 400.0], time: 27.004
steps: 84960, episodes: 86, mean episode reward: 1600.0, agent episode reward: [300.0, 1300.0], [301.3926833735912, 1300.0], time: 27.442
steps: 85960, episodes: 87, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.70199847768728, 400.0], time: 27.685
steps: 86960, episodes: 88, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.77216391636858, 300.0], time: 28.086
steps: 87960, episodes: 89, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.84501813556103, 400.0], time: 28.221
steps: 88845, episodes: 90, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.2891296725344, 200.0], time: 25.15
steps: 89845, episodes: 91, mean episode reward: 1500.0, agent episode reward: [1100.0, 400.0], [1101.8376319208157, 400.0], time: 28.749
steps: 90845, episodes: 92, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.51046996908204, 400.0], time: 28.903
steps: 91845, episodes: 93, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.52525424569484, 200.0], time: 29.126
steps: 92845, episodes: 94, mean episode reward: 1500.0, agent episode reward: [1300.0, 200.0], [1301.6014482445332, 200.0], time: 29.871
steps: 93845, episodes: 95, mean episode reward: 500.0, agent episode reward: [0.0, 500.0], [1.1718405569328043, 500.0], time: 31.119
steps: 94845, episodes: 96, mean episode reward: 1400.0, agent episode reward: [400.0, 1000.0], [401.5086385390554, 1000.0], time: 30.497
steps: 95845, episodes: 97, mean episode reward: 1300.0, agent episode reward: [0.0, 1300.0], [1.6200972575344534, 1300.0], time: 31.122
steps: 96845, episodes: 98, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.66237578594234, 200.0], time: 30.914
steps: 97845, episodes: 99, mean episode reward: 1500.0, agent episode reward: [200.0, 1300.0], [201.5303934177872, 1300.0], time: 31.168
steps: 98845, episodes: 100, mean episode reward: 3600.0, agent episode reward: [2200.0, 1400.0], [2201.5404673925254, 1400.0], time: 31.848
...Finished total of 101 episodes.
2021-10-24 01:26:24.047727: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 01:26:24.052027: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 01:26:24.052486: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55cbc01eb680 executing computations on platform Host. Devices:
2021-10-24 01:26:24.052509: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 2200.0, agent episode reward: [1800.0, 400.0], [1801.8433495574182, 400.0], time: 5.88
steps: 2000, episodes: 2, mean episode reward: 2300.0, agent episode reward: [1200.0, 1100.0], [1201.69048375638, 1100.0], time: 8.544
steps: 3000, episodes: 3, mean episode reward: 1800.0, agent episode reward: [1300.0, 500.0], [1301.6362234025594, 500.0], time: 8.603
steps: 3911, episodes: 4, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.45998738858924, 400.0], time: 7.88
steps: 4911, episodes: 5, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.5614732743754, 400.0], time: 9.738
steps: 5911, episodes: 6, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.7056284887244, 400.0], time: 10.315
steps: 6911, episodes: 7, mean episode reward: 1300.0, agent episode reward: [900.0, 400.0], [901.3410788560369, 400.0], time: 9.727
steps: 7911, episodes: 8, mean episode reward: 1300.0, agent episode reward: [500.0, 800.0], [501.42398809945047, 800.0], time: 10.152
steps: 8911, episodes: 9, mean episode reward: 1800.0, agent episode reward: [1400.0, 400.0], [1401.3880521874278, 400.0], time: 9.978
StopIteration()
steps: 9911, episodes: 10, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.30602209947729, 200.0], time: 11.057
steps: 10881, episodes: 11, mean episode reward: 1400.0, agent episode reward: [200.0, 1200.0], [201.8092962581657, 1200.0], time: 10.203
steps: 11881, episodes: 12, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.5202831973767, 200.0], time: 11.186
steps: 12881, episodes: 13, mean episode reward: 1100.0, agent episode reward: [1100.0, 0.0], [1101.394272176844, 0.0], time: 11.014
StopIteration()
steps: 13881, episodes: 14, mean episode reward: 100.0, agent episode reward: [100.0, 0.0], [101.11388933910725, 0.0], time: 11.052
steps: 14679, episodes: 15, mean episode reward: 300.0, agent episode reward: [200.0, 100.0], [201.08847145468755, 100.0], time: 8.971
steps: 15679, episodes: 16, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.72065983305342, 200.0], time: 11.843
steps: 16662, episodes: 17, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.3643666424863, 300.0], time: 11.387
steps: 17662, episodes: 18, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.33874965862789, 200.0], time: 11.731
steps: 18662, episodes: 19, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.5774640736807, 300.0], time: 11.758
StopIteration()
steps: 19662, episodes: 20, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.51915120059994, 200.0], time: 12.473
steps: 20662, episodes: 21, mean episode reward: 2500.0, agent episode reward: [1300.0, 1200.0], [1301.7605342531351, 1200.0], time: 11.988
steps: 21662, episodes: 22, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.84608530346645, 100.0], time: 12.246
steps: 22550, episodes: 23, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.22941472692133, 300.0], time: 10.849
steps: 23550, episodes: 24, mean episode reward: 1800.0, agent episode reward: [600.0, 1200.0], [601.6725645624934, 1200.0], time: 12.832
steps: 24550, episodes: 25, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.44642192503517, 100.0], time: 12.278
StopIteration()
steps: 25550, episodes: 26, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.27031211495876, 100.0], time: 12.778
steps: 26550, episodes: 27, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.27855799072307, 400.0], time: 13.271
steps: 27550, episodes: 28, mean episode reward: 700.0, agent episode reward: [200.0, 500.0], [201.56238154950682, 500.0], time: 13.274
steps: 28550, episodes: 29, mean episode reward: 2500.0, agent episode reward: [200.0, 2300.0], [201.5936832268259, 2300.0], time: 13.512
steps: 29502, episodes: 30, mean episode reward: 1000.0, agent episode reward: [100.0, 900.0], [101.23549804007939, 900.0], time: 12.384
steps: 30502, episodes: 31, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.63767136521983, 300.0], time: 13.898
steps: 31502, episodes: 32, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.7389613414895, 100.0], time: 13.691
steps: 32502, episodes: 33, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.33558102987139, 500.0], time: 13.625
steps: 33502, episodes: 34, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.9143376085596, 100.0], time: 13.631
steps: 34502, episodes: 35, mean episode reward: 1800.0, agent episode reward: [1200.0, 600.0], [1201.566343104244, 600.0], time: 14.268
steps: 35502, episodes: 36, mean episode reward: 3300.0, agent episode reward: [1200.0, 2100.0], [1201.3697404829802, 2100.0], time: 13.97
steps: 36502, episodes: 37, mean episode reward: 2500.0, agent episode reward: [2100.0, 400.0], [2101.5161550563676, 400.0], time: 14.34
steps: 37273, episodes: 38, mean episode reward: 200.0, agent episode reward: [100.0, 100.0], [101.26459122506834, 100.0], time: 11.458
steps: 38273, episodes: 39, mean episode reward: 1000.0, agent episode reward: [300.0, 700.0], [301.4407011828573, 700.0], time: 14.581
steps: 39273, episodes: 40, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.6536146681624, 1500.0], time: 15.168
steps: 40273, episodes: 41, mean episode reward: 1300.0, agent episode reward: [900.0, 400.0], [901.4488815533916, 400.0], time: 15.421
steps: 41273, episodes: 42, mean episode reward: 1000.0, agent episode reward: [900.0, 100.0], [901.6954118474829, 100.0], time: 15.471
steps: 42273, episodes: 43, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.24556140105595, 400.0], time: 15.433
steps: 43273, episodes: 44, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.4055117784314, 500.0], time: 15.617
steps: 44088, episodes: 45, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.3542988035471, 100.0], time: 13.166
steps: 45088, episodes: 46, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.35324325401004, 400.0], time: 16.133
steps: 46088, episodes: 47, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.48817534659776, 1600.0], time: 16.163
steps: 47088, episodes: 48, mean episode reward: 1000.0, agent episode reward: [900.0, 100.0], [901.651431228592, 100.0], time: 16.325
steps: 48088, episodes: 49, mean episode reward: 2600.0, agent episode reward: [2400.0, 200.0], [2401.538164326475, 200.0], time: 17.206
steps: 48998, episodes: 50, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [101.34654975409377, 1400.0], time: 15.461
steps: 49998, episodes: 51, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.6286290602538, 300.0], time: 17.128
steps: 50998, episodes: 52, mean episode reward: 800.0, agent episode reward: [500.0, 300.0], [501.6155814413275, 300.0], time: 17.557
steps: 51998, episodes: 53, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.48460623923827, 600.0], time: 18.012
steps: 52998, episodes: 54, mean episode reward: 2500.0, agent episode reward: [1300.0, 1200.0], [1301.5220939321675, 1200.0], time: 17.887
steps: 53998, episodes: 55, mean episode reward: 1800.0, agent episode reward: [1600.0, 200.0], [1601.4358624738973, 200.0], time: 18.06
steps: 54998, episodes: 56, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.6467926613423, 400.0], time: 18.483
steps: 55998, episodes: 57, mean episode reward: 1500.0, agent episode reward: [1300.0, 200.0], [1301.2136851496698, 200.0], time: 19.037
steps: 56998, episodes: 58, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.56741355649663, 300.0], time: 18.71
steps: 57998, episodes: 59, mean episode reward: 400.0, agent episode reward: [0.0, 400.0], [1.27906696173769, 400.0], time: 19.113
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 58998, episodes: 60, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.17205366367664, 700.0], time: 19.561
steps: 59998, episodes: 61, mean episode reward: 1700.0, agent episode reward: [1200.0, 500.0], [1201.534527143109, 500.0], time: 20.289
steps: 60998, episodes: 62, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.55584627810225, 300.0], time: 20.01
steps: 61998, episodes: 63, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.30846825394762, 400.0], time: 20.16
steps: 62998, episodes: 64, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.19593884180584, 500.0], time: 20.645
steps: 63998, episodes: 65, mean episode reward: 1300.0, agent episode reward: [1000.0, 300.0], [1001.3850938623061, 300.0], time: 21.178
steps: 64998, episodes: 66, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.3729470634705, 600.0], time: 21.251
steps: 65998, episodes: 67, mean episode reward: 1700.0, agent episode reward: [1400.0, 300.0], [1401.3641860078033, 300.0], time: 21.331
steps: 66998, episodes: 68, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.78771798210806, 300.0], time: 21.659
steps: 67998, episodes: 69, mean episode reward: 3800.0, agent episode reward: [1000.0, 2800.0], [1001.3002171945731, 2800.0], time: 21.846
steps: 68827, episodes: 70, mean episode reward: 2400.0, agent episode reward: [2200.0, 200.0], [2201.1440257954837, 200.0], time: 18.729
steps: 69827, episodes: 71, mean episode reward: 3600.0, agent episode reward: [1600.0, 2000.0], [1601.2871611611213, 2000.0], time: 22.64
steps: 70827, episodes: 72, mean episode reward: 3400.0, agent episode reward: [1600.0, 1800.0], [1601.3222701984193, 1800.0], time: 22.941
steps: 71827, episodes: 73, mean episode reward: 1800.0, agent episode reward: [1100.0, 700.0], [1101.4864477203564, 700.0], time: 23.0
steps: 72827, episodes: 74, mean episode reward: 3600.0, agent episode reward: [2200.0, 1400.0], [2201.1828832244814, 1400.0], time: 23.404
steps: 73827, episodes: 75, mean episode reward: 1600.0, agent episode reward: [0.0, 1600.0], [1.4540928406804527, 1600.0], time: 24.182
steps: 74827, episodes: 76, mean episode reward: 3800.0, agent episode reward: [2500.0, 1300.0], [2501.3717390913466, 1300.0], time: 24.026
steps: 75827, episodes: 77, mean episode reward: 1800.0, agent episode reward: [1200.0, 600.0], [1201.4087965862857, 600.0], time: 24.589
steps: 76827, episodes: 78, mean episode reward: 2300.0, agent episode reward: [100.0, 2200.0], [101.36400608740578, 2200.0], time: 24.937
steps: 77827, episodes: 79, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.6089802499481, 300.0], time: 25.168
steps: 78827, episodes: 80, mean episode reward: 1500.0, agent episode reward: [200.0, 1300.0], [201.3144839839818, 1300.0], time: 25.467
steps: 79534, episodes: 81, mean episode reward: 1400.0, agent episode reward: [1000.0, 400.0], [1000.9310733852699, 400.0], time: 17.972
steps: 80412, episodes: 82, mean episode reward: 4300.0, agent episode reward: [1400.0, 2900.0], [1401.1114294705362, 2900.0], time: 22.496
steps: 81412, episodes: 83, mean episode reward: 2600.0, agent episode reward: [1900.0, 700.0], [1901.4350429155322, 700.0], time: 26.093
steps: 82412, episodes: 84, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.18742604571526, 200.0], time: 26.63
steps: 83412, episodes: 85, mean episode reward: 1700.0, agent episode reward: [100.0, 1600.0], [101.3208616409733, 1600.0], time: 27.241
StopIteration()
steps: 84412, episodes: 86, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.4422465248366, 400.0], time: 27.421
steps: 85412, episodes: 87, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.3028924761036, 300.0], time: 27.542
steps: 86412, episodes: 88, mean episode reward: 2300.0, agent episode reward: [1300.0, 1000.0], [1301.3596059347842, 1000.0], time: 27.855
steps: 87159, episodes: 89, mean episode reward: 2400.0, agent episode reward: [1200.0, 1200.0], [1201.1571412473163, 1200.0], time: 20.929
steps: 88159, episodes: 90, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.27237481496553, 500.0], time: 28.263
steps: 89110, episodes: 91, mean episode reward: 1600.0, agent episode reward: [300.0, 1300.0], [301.2747167838653, 1300.0], time: 27.368
steps: 90110, episodes: 92, mean episode reward: 2200.0, agent episode reward: [900.0, 1300.0], [901.3625820433266, 1300.0], time: 28.84
steps: 91110, episodes: 93, mean episode reward: 2300.0, agent episode reward: [600.0, 1700.0], [601.4856823371324, 1700.0], time: 29.364
steps: 92110, episodes: 94, mean episode reward: 2400.0, agent episode reward: [200.0, 2200.0], [201.21375674544186, 2200.0], time: 29.642
steps: 93110, episodes: 95, mean episode reward: 3300.0, agent episode reward: [3000.0, 300.0], [3001.6974281546727, 300.0], time: 29.997
steps: 94110, episodes: 96, mean episode reward: 3800.0, agent episode reward: [1100.0, 2700.0], [1101.6022830419854, 2700.0], time: 30.294
steps: 95110, episodes: 97, mean episode reward: 2600.0, agent episode reward: [1300.0, 1300.0], [1301.2390157983289, 1300.0], time: 30.791
steps: 96110, episodes: 98, mean episode reward: 1500.0, agent episode reward: [400.0, 1100.0], [401.3123283437346, 1100.0], time: 31.023
steps: 97110, episodes: 99, mean episode reward: 1800.0, agent episode reward: [1200.0, 600.0], [1201.541722879267, 600.0], time: 31.502
steps: 98110, episodes: 100, mean episode reward: 1300.0, agent episode reward: [800.0, 500.0], [801.2334517611573, 500.0], time: 31.677
...Finished total of 101 episodes.
2021-10-24 01:56:35.396052: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 01:56:35.400201: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 01:56:35.400662: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x559c2d476860 executing computations on platform Host. Devices:
2021-10-24 01:56:35.400683: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
StopIteration()
steps: 1000, episodes: 1, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.7856185753372, 400.0], time: 5.527
steps: 2000, episodes: 2, mean episode reward: 1800.0, agent episode reward: [1200.0, 600.0], [1202.0004342283182, 600.0], time: 8.385
steps: 3000, episodes: 3, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.89059006730395, 300.0], time: 8.337
steps: 4000, episodes: 4, mean episode reward: 1300.0, agent episode reward: [700.0, 600.0], [701.6007715645678, 600.0], time: 8.618
steps: 5000, episodes: 5, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.79663927957253, 600.0], time: 8.955
steps: 5994, episodes: 6, mean episode reward: 700.0, agent episode reward: [200.0, 500.0], [201.48086206772564, 500.0], time: 9.26
steps: 6994, episodes: 7, mean episode reward: 1300.0, agent episode reward: [400.0, 900.0], [401.76482239797286, 900.0], time: 9.566
steps: 7994, episodes: 8, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.4334781626728, 200.0], time: 9.592
steps: 8994, episodes: 9, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.58201201536656, 500.0], time: 10.581
StopIteration()
steps: 9994, episodes: 10, mean episode reward: 3400.0, agent episode reward: [2100.0, 1300.0], [2101.558167017882, 1300.0], time: 10.49
steps: 10961, episodes: 11, mean episode reward: 1700.0, agent episode reward: [200.0, 1500.0], [201.2847170656709, 1500.0], time: 10.113
steps: 11961, episodes: 12, mean episode reward: 1500.0, agent episode reward: [200.0, 1300.0], [201.9540753263185, 1300.0], time: 11.105
steps: 12961, episodes: 13, mean episode reward: 700.0, agent episode reward: [500.0, 200.0], [501.5953629757285, 200.0], time: 11.246
steps: 13794, episodes: 14, mean episode reward: 400.0, agent episode reward: [0.0, 400.0], [1.1650271542595303, 400.0], time: 8.935
steps: 14675, episodes: 15, mean episode reward: 2600.0, agent episode reward: [1100.0, 1500.0], [1101.404525562996, 1500.0], time: 9.634
StopIteration()
steps: 15675, episodes: 16, mean episode reward: 700.0, agent episode reward: [200.0, 500.0], [201.64547270078063, 500.0], time: 11.257
steps: 16321, episodes: 17, mean episode reward: 1300.0, agent episode reward: [0.0, 1300.0], [1.1620933188086324, 1300.0], time: 8.079
steps: 17321, episodes: 18, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.21670154575105, 300.0], time: 11.305
steps: 18321, episodes: 19, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.4440744967339, 400.0], time: 11.613
steps: 19321, episodes: 20, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.56179263658174, 300.0], time: 11.468
steps: 19994, episodes: 21, mean episode reward: 200.0, agent episode reward: [200.0, 0.0], [201.03969229343096, 0.0], time: 8.046
steps: 20994, episodes: 22, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.75563315154994, 100.0], time: 11.679
steps: 21994, episodes: 23, mean episode reward: 1700.0, agent episode reward: [200.0, 1500.0], [201.65077584621716, 1500.0], time: 11.806
steps: 22994, episodes: 24, mean episode reward: 1000.0, agent episode reward: [700.0, 300.0], [701.6414042489781, 300.0], time: 12.265
steps: 23994, episodes: 25, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.6585884757547, 400.0], time: 12.409
steps: 24994, episodes: 26, mean episode reward: 3500.0, agent episode reward: [3300.0, 200.0], [3301.94336642624, 200.0], time: 12.198
steps: 25808, episodes: 27, mean episode reward: 2400.0, agent episode reward: [2100.0, 300.0], [2101.247036451874, 300.0], time: 10.101
steps: 26808, episodes: 28, mean episode reward: 400.0, agent episode reward: [0.0, 400.0], [1.3282525251634503, 400.0], time: 12.558
steps: 27808, episodes: 29, mean episode reward: 3400.0, agent episode reward: [3200.0, 200.0], [3201.6434296143293, 200.0], time: 13.648
StopIteration()
steps: 28808, episodes: 30, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.69484819148465, 200.0], time: 12.992
steps: 29808, episodes: 31, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.66636357695643, 1600.0], time: 12.954
StopIteration()
steps: 30808, episodes: 32, mean episode reward: 1500.0, agent episode reward: [1400.0, 100.0], [1401.6645944558977, 100.0], time: 13.145
StopIteration()
steps: 31808, episodes: 33, mean episode reward: 1600.0, agent episode reward: [1400.0, 200.0], [1401.5332798825145, 200.0], time: 13.921
steps: 32808, episodes: 34, mean episode reward: 3500.0, agent episode reward: [3400.0, 100.0], [3401.618711127831, 100.0], time: 13.598
steps: 33712, episodes: 35, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.5105070072105, 100.0], time: 12.512
steps: 34712, episodes: 36, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.7121471275138, 300.0], time: 14.347
steps: 35712, episodes: 37, mean episode reward: 600.0, agent episode reward: [0.0, 600.0], [1.2930921461358458, 600.0], time: 14.586
steps: 36589, episodes: 38, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.20267102363543, 300.0], time: 12.277
steps: 37513, episodes: 39, mean episode reward: 900.0, agent episode reward: [100.0, 800.0], [101.14706760187102, 800.0], time: 13.418
steps: 38513, episodes: 40, mean episode reward: 1600.0, agent episode reward: [400.0, 1200.0], [401.7117776895047, 1200.0], time: 14.471
steps: 39513, episodes: 41, mean episode reward: 2400.0, agent episode reward: [2200.0, 200.0], [2201.6295475208667, 200.0], time: 15.103
steps: 40513, episodes: 42, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.46921862551294, 500.0], time: 14.87
steps: 41513, episodes: 43, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.3862408354151, 200.0], time: 15.368
steps: 42513, episodes: 44, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.3765491812325, 500.0], time: 15.191
steps: 43513, episodes: 45, mean episode reward: 1500.0, agent episode reward: [400.0, 1100.0], [401.4940610435382, 1100.0], time: 16.119
StopIteration()
steps: 44513, episodes: 46, mean episode reward: 200.0, agent episode reward: [200.0, 0.0], [201.47201556113612, 0.0], time: 15.816
steps: 45513, episodes: 47, mean episode reward: 600.0, agent episode reward: [0.0, 600.0], [1.2684654170924254, 600.0], time: 16.055
steps: 46513, episodes: 48, mean episode reward: 1000.0, agent episode reward: [800.0, 200.0], [801.6080014428045, 200.0], time: 16.174
steps: 47513, episodes: 49, mean episode reward: 1800.0, agent episode reward: [100.0, 1700.0], [101.15001280835291, 1700.0], time: 16.811
StopIteration()
steps: 48513, episodes: 50, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.73060070047813, 100.0], time: 16.654
steps: 49513, episodes: 51, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.60738990785815, 500.0], time: 16.795
steps: 50513, episodes: 52, mean episode reward: 1600.0, agent episode reward: [400.0, 1200.0], [401.55618735360474, 1200.0], time: 16.951
steps: 51513, episodes: 53, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.6486576829333, 500.0], time: 17.629
steps: 52513, episodes: 54, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.6647217965714, 200.0], time: 17.766
steps: 53416, episodes: 55, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.01270536073572, 500.0], time: 15.918
steps: 54416, episodes: 56, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.42512234164818, 300.0], time: 18.226
steps: 55402, episodes: 57, mean episode reward: 500.0, agent episode reward: [500.0, 0.0], [501.4377371206068, 0.0], time: 18.019
steps: 56326, episodes: 58, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.10659303435614, 700.0], time: 17.217
steps: 57326, episodes: 59, mean episode reward: 400.0, agent episode reward: [400.0, 0.0], [401.43131415571963, 0.0], time: 18.693/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 58326, episodes: 60, mean episode reward: 2400.0, agent episode reward: [2200.0, 200.0], [2201.5809880156844, 200.0], time: 19.417
steps: 59326, episodes: 61, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.6207460835609, 600.0], time: 19.367
steps: 59957, episodes: 62, mean episode reward: 200.0, agent episode reward: [200.0, 0.0], [200.98848220631626, 0.0], time: 12.427
steps: 60957, episodes: 63, mean episode reward: 800.0, agent episode reward: [700.0, 100.0], [701.7418874113185, 100.0], time: 19.79
steps: 61957, episodes: 64, mean episode reward: 400.0, agent episode reward: [400.0, 0.0], [401.78438419452476, 0.0], time: 20.387
steps: 62957, episodes: 65, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [302.1068168425061, 200.0], time: 20.56
steps: 63957, episodes: 66, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.65811353612673, 400.0], time: 20.385
steps: 64957, episodes: 67, mean episode reward: 2200.0, agent episode reward: [1100.0, 1100.0], [1101.679174392574, 1100.0], time: 21.1
steps: 65957, episodes: 68, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [302.0603176411908, 200.0], time: 21.353
steps: 66957, episodes: 69, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.3348363488666, 200.0], time: 21.758
steps: 67957, episodes: 70, mean episode reward: 2400.0, agent episode reward: [1300.0, 1100.0], [1301.5906654486512, 1100.0], time: 21.604
steps: 68957, episodes: 71, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [102.01045367227785, 400.0], time: 22.197
steps: 69766, episodes: 72, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.02665311436553, 200.0], time: 18.143
steps: 70766, episodes: 73, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.4134049680599, 100.0], time: 22.707
steps: 71766, episodes: 74, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.44705438177783, 100.0], time: 22.768
steps: 72766, episodes: 75, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.33970355170885, 300.0], time: 23.407
steps: 73766, episodes: 76, mean episode reward: 2400.0, agent episode reward: [800.0, 1600.0], [801.4205012956165, 1600.0], time: 23.661
steps: 74766, episodes: 77, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.54934492581174, 1500.0], time: 24.168
steps: 75766, episodes: 78, mean episode reward: 3000.0, agent episode reward: [1300.0, 1700.0], [1301.293214652189, 1700.0], time: 24.39
steps: 76766, episodes: 79, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.4211888925244, 400.0], time: 24.549
steps: 77766, episodes: 80, mean episode reward: 800.0, agent episode reward: [800.0, 0.0], [801.6144089133799, 0.0], time: 24.909
steps: 78766, episodes: 81, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [302.2274605502202, 200.0], time: 25.267
steps: 79766, episodes: 82, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.6310382009261, 200.0], time: 25.372
steps: 80766, episodes: 83, mean episode reward: 1500.0, agent episode reward: [1300.0, 200.0], [1301.9596985005628, 200.0], time: 25.551
steps: 81766, episodes: 84, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.6771089056234, 200.0], time: 25.925
steps: 82766, episodes: 85, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.7238786822065, 500.0], time: 26.372
steps: 83766, episodes: 86, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.37677820497328, 300.0], time: 26.255
steps: 84766, episodes: 87, mean episode reward: 4500.0, agent episode reward: [1300.0, 3200.0], [1301.4796086706294, 3200.0], time: 26.843
steps: 85766, episodes: 88, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.80637584429049, 300.0], time: 27.521
steps: 86766, episodes: 89, mean episode reward: 2500.0, agent episode reward: [1200.0, 1300.0], [1201.6677327005689, 1300.0], time: 29.018
steps: 87766, episodes: 90, mean episode reward: 5400.0, agent episode reward: [4400.0, 1000.0], [4401.378009971672, 1000.0], time: 28.371
steps: 88757, episodes: 91, mean episode reward: 2400.0, agent episode reward: [200.0, 2200.0], [201.71037110431553, 2200.0], time: 28.364
steps: 89757, episodes: 92, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.91312300673894, 400.0], time: 28.926
steps: 90721, episodes: 93, mean episode reward: 300.0, agent episode reward: [300.0, 0.0], [301.6380818558726, 0.0], time: 28.421
steps: 91721, episodes: 94, mean episode reward: 2600.0, agent episode reward: [100.0, 2500.0], [101.86057179967032, 2500.0], time: 29.599
steps: 92721, episodes: 95, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.603702421586, 100.0], time: 30.184
steps: 93721, episodes: 96, mean episode reward: 900.0, agent episode reward: [800.0, 100.0], [801.7651793140245, 100.0], time: 30.29
steps: 94721, episodes: 97, mean episode reward: 1600.0, agent episode reward: [1100.0, 500.0], [1101.557123314623, 500.0], time: 30.461
steps: 95721, episodes: 98, mean episode reward: 1500.0, agent episode reward: [400.0, 1100.0], [401.2875613529152, 1100.0], time: 30.927
steps: 96721, episodes: 99, mean episode reward: 800.0, agent episode reward: [500.0, 300.0], [501.6205481807398, 300.0], time: 31.266
steps: 97721, episodes: 100, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.5449773237762, 200.0], time: 31.449
...Finished total of 101 episodes.
2021-10-24 02:26:16.645362: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 02:26:16.649422: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 02:26:16.649619: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x561b68cdabb0 executing computations on platform Host. Devices:
2021-10-24 02:26:16.649636: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.6193353537286, 500.0], time: 5.939
steps: 2000, episodes: 2, mean episode reward: 2300.0, agent episode reward: [2100.0, 200.0], [2101.311655525317, 200.0], time: 9.343
steps: 3000, episodes: 3, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [302.0118190701138, 500.0], time: 9.334
steps: 4000, episodes: 4, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.39688436520078, 400.0], time: 9.274
steps: 5000, episodes: 5, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.480741112158, 400.0], time: 9.326
steps: 6000, episodes: 6, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.48041857224257, 200.0], time: 10.288
steps: 7000, episodes: 7, mean episode reward: 1300.0, agent episode reward: [500.0, 800.0], [501.63097503622345, 800.0], time: 10.498
steps: 8000, episodes: 8, mean episode reward: 1500.0, agent episode reward: [1200.0, 300.0], [1201.4581179662227, 300.0], time: 10.458
steps: 9000, episodes: 9, mean episode reward: 1500.0, agent episode reward: [200.0, 1300.0], [201.40103227772175, 1300.0], time: 10.695
steps: 9617, episodes: 10, mean episode reward: 200.0, agent episode reward: [100.0, 100.0], [100.84249402478599, 100.0], time: 7.283
steps: 10617, episodes: 11, mean episode reward: 700.0, agent episode reward: [600.0, 100.0], [601.3633758842923, 100.0], time: 11.476
steps: 11617, episodes: 12, mean episode reward: 700.0, agent episode reward: [100.0, 600.0], [101.81508589428961, 600.0], time: 11.517
steps: 12448, episodes: 13, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.4269413285098, 200.0], time: 9.719
steps: 13448, episodes: 14, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [202.07269733721563, 600.0], time: 11.612
steps: 14448, episodes: 15, mean episode reward: 700.0, agent episode reward: [600.0, 100.0], [601.2622687783138, 100.0], time: 11.976
steps: 15448, episodes: 16, mean episode reward: 700.0, agent episode reward: [500.0, 200.0], [502.04501653770336, 200.0], time: 11.976
steps: 16448, episodes: 17, mean episode reward: 4700.0, agent episode reward: [4700.0, 0.0], [4701.2384953449355, 0.0], time: 12.217
steps: 17448, episodes: 18, mean episode reward: 3400.0, agent episode reward: [3100.0, 300.0], [3101.4796739607664, 300.0], time: 12.178
StopIteration()
steps: 18448, episodes: 19, mean episode reward: 3500.0, agent episode reward: [3300.0, 200.0], [3301.66019280951, 200.0], time: 12.76
steps: 19398, episodes: 20, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.6816550756687, 1200.0], time: 11.63
steps: 20028, episodes: 21, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [100.87528513382055, 200.0], time: 8.258
steps: 21028, episodes: 22, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.717051320004, 300.0], time: 12.454
StopIteration()
steps: 22028, episodes: 23, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.53845964977614, 300.0], time: 12.695
steps: 23028, episodes: 24, mean episode reward: 200.0, agent episode reward: [0.0, 200.0], [1.3205356294218784, 200.0], time: 12.918
steps: 24028, episodes: 25, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.27049590471452, 1600.0], time: 12.928
steps: 25028, episodes: 26, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.62153688067525, 400.0], time: 13.095
steps: 26028, episodes: 27, mean episode reward: 2700.0, agent episode reward: [2000.0, 700.0], [2001.618005978316, 700.0], time: 13.336
steps: 27028, episodes: 28, mean episode reward: 3600.0, agent episode reward: [300.0, 3300.0], [301.32800057044784, 3300.0], time: 13.616
steps: 28028, episodes: 29, mean episode reward: 1500.0, agent episode reward: [1000.0, 500.0], [1001.5073517218848, 500.0], time: 13.513
steps: 29028, episodes: 30, mean episode reward: 3500.0, agent episode reward: [2000.0, 1500.0], [2001.2861284328412, 1500.0], time: 13.546
steps: 30028, episodes: 31, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.45531135677231, 400.0], time: 13.877
steps: 31028, episodes: 32, mean episode reward: 3800.0, agent episode reward: [0.0, 3800.0], [1.2104763728394234, 3800.0], time: 13.963
steps: 32028, episodes: 33, mean episode reward: 3800.0, agent episode reward: [3600.0, 200.0], [3601.3268977020316, 200.0], time: 14.15
steps: 33028, episodes: 34, mean episode reward: 3300.0, agent episode reward: [1800.0, 1500.0], [1801.5747294489931, 1500.0], time: 14.264
steps: 34028, episodes: 35, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.8891104373568, 200.0], time: 14.367
steps: 35028, episodes: 36, mean episode reward: 900.0, agent episode reward: [100.0, 800.0], [101.26449930352788, 800.0], time: 14.831
steps: 36028, episodes: 37, mean episode reward: 1500.0, agent episode reward: [1000.0, 500.0], [1001.326404616863, 500.0], time: 14.945
steps: 37028, episodes: 38, mean episode reward: 2300.0, agent episode reward: [700.0, 1600.0], [701.6432684877475, 1600.0], time: 15.272
steps: 38028, episodes: 39, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.5188474120974, 200.0], time: 15.318
steps: 38962, episodes: 40, mean episode reward: 1800.0, agent episode reward: [1200.0, 600.0], [1201.578396328741, 600.0], time: 14.384
steps: 39962, episodes: 41, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.38275998567688, 300.0], time: 15.657
steps: 40962, episodes: 42, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.5092267533423, 300.0], time: 15.704
steps: 41777, episodes: 43, mean episode reward: 3800.0, agent episode reward: [3600.0, 200.0], [3601.109312945609, 200.0], time: 13.012
steps: 42559, episodes: 44, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.17546019718618, 200.0], time: 12.812
steps: 43559, episodes: 45, mean episode reward: 1600.0, agent episode reward: [1500.0, 100.0], [1501.704414768365, 100.0], time: 16.43
steps: 44559, episodes: 46, mean episode reward: 1700.0, agent episode reward: [1000.0, 700.0], [1001.1591554092245, 700.0], time: 16.305
steps: 45559, episodes: 47, mean episode reward: 3500.0, agent episode reward: [2200.0, 1300.0], [2201.4226083431095, 1300.0], time: 16.629
steps: 46559, episodes: 48, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [101.52686895145294, 1400.0], time: 16.945
steps: 47559, episodes: 49, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.53717390910714, 300.0], time: 17.46
steps: 48559, episodes: 50, mean episode reward: 1400.0, agent episode reward: [1100.0, 300.0], [1101.4510403908782, 300.0], time: 17.325
steps: 49465, episodes: 51, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.4452555628305, 100.0], time: 15.95
steps: 50465, episodes: 52, mean episode reward: 4500.0, agent episode reward: [2300.0, 2200.0], [2301.5143727703744, 2200.0], time: 17.828
steps: 51465, episodes: 53, mean episode reward: 3700.0, agent episode reward: [1900.0, 1800.0], [1901.443672638486, 1800.0], time: 18.013
steps: 52465, episodes: 54, mean episode reward: 1000.0, agent episode reward: [1000.0, 0.0], [1001.2682382805037, 0.0], time: 18.316
steps: 53465, episodes: 55, mean episode reward: 5800.0, agent episode reward: [4700.0, 1100.0], [4701.414795994741, 1100.0], time: 18.613
steps: 54465, episodes: 56, mean episode reward: 1600.0, agent episode reward: [500.0, 1100.0], [501.6183361852604, 1100.0], time: 18.812
steps: 55465, episodes: 57, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.7401097361874, 200.0], time: 19.007
steps: 56465, episodes: 58, mean episode reward: 3400.0, agent episode reward: [0.0, 3400.0], [1.2125818022310666, 3400.0], time: 19.342
steps: 57137, episodes: 59, mean episode reward: 4600.0, agent episode reward: [3400.0, 1200.0], [3401.019569067622, 1200.0], time: 13.05
steps: 58137, episodes: 60, mean episode reward: 1600.0, agent episode reward: [100.0, 1500.0], [101.3765167428446, 1500.0], time: 19.867/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 59137, episodes: 61, mean episode reward: 4600.0, agent episode reward: [3500.0, 1100.0], [3501.4238873739387, 1100.0], time: 19.849
steps: 60137, episodes: 62, mean episode reward: 1400.0, agent episode reward: [1200.0, 200.0], [1201.371514589765, 200.0], time: 20.38
steps: 61137, episodes: 63, mean episode reward: 1800.0, agent episode reward: [400.0, 1400.0], [401.40021584162656, 1400.0], time: 20.424
steps: 62137, episodes: 64, mean episode reward: 4800.0, agent episode reward: [3400.0, 1400.0], [3401.63478115199, 1400.0], time: 20.746
steps: 63137, episodes: 65, mean episode reward: 3600.0, agent episode reward: [3000.0, 600.0], [3001.705880634802, 600.0], time: 20.865
steps: 64137, episodes: 66, mean episode reward: 3300.0, agent episode reward: [3200.0, 100.0], [3201.668454584465, 100.0], time: 21.258
steps: 65137, episodes: 67, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.9735984402164, 300.0], time: 21.592
steps: 66137, episodes: 68, mean episode reward: 2300.0, agent episode reward: [1800.0, 500.0], [1801.6191645778442, 500.0], time: 22.007
steps: 66786, episodes: 69, mean episode reward: 2200.0, agent episode reward: [1000.0, 1200.0], [1000.8670163323005, 1200.0], time: 14.452
steps: 67786, episodes: 70, mean episode reward: 2700.0, agent episode reward: [2700.0, 0.0], [2701.755520091756, 0.0], time: 22.491
steps: 68786, episodes: 71, mean episode reward: 5200.0, agent episode reward: [5000.0, 200.0], [5001.432817173074, 200.0], time: 22.98
StopIteration()
steps: 69786, episodes: 72, mean episode reward: 300.0, agent episode reward: [300.0, 0.0], [301.3833240969753, 0.0], time: 23.099
steps: 70786, episodes: 73, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [102.08787968941323, 1400.0], time: 23.31
steps: 71786, episodes: 74, mean episode reward: 1700.0, agent episode reward: [500.0, 1200.0], [501.4080604600501, 1200.0], time: 23.81
steps: 72786, episodes: 75, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.99927688207988, 500.0], time: 23.953
steps: 73752, episodes: 76, mean episode reward: 3500.0, agent episode reward: [2300.0, 1200.0], [2301.466463164954, 1200.0], time: 23.352
steps: 74752, episodes: 77, mean episode reward: 900.0, agent episode reward: [800.0, 100.0], [801.6935432988387, 100.0], time: 24.315
steps: 75676, episodes: 78, mean episode reward: 1500.0, agent episode reward: [200.0, 1300.0], [201.28378507067126, 1300.0], time: 22.815
steps: 76676, episodes: 79, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.5974920610928, 500.0], time: 24.85
StopIteration()
steps: 77676, episodes: 80, mean episode reward: 5700.0, agent episode reward: [3300.0, 2400.0], [3301.6001580210323, 2400.0], time: 25.186
steps: 78676, episodes: 81, mean episode reward: 2500.0, agent episode reward: [2300.0, 200.0], [2301.3332039885026, 200.0], time: 25.498
steps: 79676, episodes: 82, mean episode reward: 5600.0, agent episode reward: [4400.0, 1200.0], [4401.747365880946, 1200.0], time: 25.925
StopIteration()
steps: 80676, episodes: 83, mean episode reward: 2600.0, agent episode reward: [2500.0, 100.0], [2501.4202266487778, 100.0], time: 26.753
steps: 81676, episodes: 84, mean episode reward: 4400.0, agent episode reward: [4200.0, 200.0], [4201.839127363167, 200.0], time: 26.751
steps: 82676, episodes: 85, mean episode reward: 1800.0, agent episode reward: [1400.0, 400.0], [1401.9542558839823, 400.0], time: 26.901
steps: 83676, episodes: 86, mean episode reward: 4500.0, agent episode reward: [4300.0, 200.0], [4301.514088511175, 200.0], time: 26.996
steps: 84676, episodes: 87, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.28959799568116, 500.0], time: 27.286
steps: 85676, episodes: 88, mean episode reward: 1500.0, agent episode reward: [1100.0, 400.0], [1101.6830512221495, 400.0], time: 27.764
steps: 86676, episodes: 89, mean episode reward: 3400.0, agent episode reward: [1400.0, 2000.0], [1401.4709374097938, 2000.0], time: 28.18
steps: 87676, episodes: 90, mean episode reward: 2500.0, agent episode reward: [100.0, 2400.0], [101.45234850868633, 2400.0], time: 28.474
steps: 88676, episodes: 91, mean episode reward: 5500.0, agent episode reward: [4100.0, 1400.0], [4101.449510244279, 1400.0], time: 28.833
steps: 89518, episodes: 92, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.24250366751352, 200.0], time: 24.257
steps: 90518, episodes: 93, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.469556759509, 300.0], time: 29.313
steps: 91518, episodes: 94, mean episode reward: 1600.0, agent episode reward: [1400.0, 200.0], [1402.0495469083976, 200.0], time: 29.6
steps: 92518, episodes: 95, mean episode reward: 4500.0, agent episode reward: [2100.0, 2400.0], [2101.7842389407037, 2400.0], time: 29.845
steps: 93518, episodes: 96, mean episode reward: 2500.0, agent episode reward: [1100.0, 1400.0], [1101.6799212890603, 1400.0], time: 30.314
steps: 94330, episodes: 97, mean episode reward: 1100.0, agent episode reward: [100.0, 1000.0], [101.17396047609098, 1000.0], time: 24.698
steps: 95330, episodes: 98, mean episode reward: 2200.0, agent episode reward: [1900.0, 300.0], [1901.6915469673227, 300.0], time: 30.828
steps: 96330, episodes: 99, mean episode reward: 1000.0, agent episode reward: [200.0, 800.0], [201.49364684251643, 800.0], time: 31.181
steps: 97330, episodes: 100, mean episode reward: 3600.0, agent episode reward: [100.0, 3500.0], [101.27656429871534, 3500.0], time: 31.504
...Finished total of 101 episodes.
2021-10-24 02:56:41.049299: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 02:56:41.053675: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 02:56:41.053847: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x561e8c3abd20 executing computations on platform Host. Devices:
2021-10-24 02:56:41.053869: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 1400.0, agent episode reward: [200.0, 1200.0], [201.92987224967686, 1200.0], time: 5.876
steps: 2000, episodes: 2, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.13746841249818, 300.0], time: 9.372
steps: 3000, episodes: 3, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.5863607445377, 300.0], time: 9.342
steps: 4000, episodes: 4, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.623752098896, 200.0], time: 9.382
steps: 5000, episodes: 5, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.4312356787374, 400.0], time: 9.417
steps: 6000, episodes: 6, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.57473536757465, 100.0], time: 10.11
steps: 7000, episodes: 7, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.75322870892936, 400.0], time: 10.624
steps: 8000, episodes: 8, mean episode reward: 1300.0, agent episode reward: [800.0, 500.0], [801.6074066343556, 500.0], time: 10.433
steps: 9000, episodes: 9, mean episode reward: 1300.0, agent episode reward: [400.0, 900.0], [401.6543400827892, 900.0], time: 10.585
steps: 9770, episodes: 10, mean episode reward: 2400.0, agent episode reward: [100.0, 2300.0], [101.25367802510598, 2300.0], time: 8.72
steps: 10770, episodes: 11, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.3054608821229, 400.0], time: 11.381
steps: 11770, episodes: 12, mean episode reward: 2700.0, agent episode reward: [200.0, 2500.0], [201.24026084933294, 2500.0], time: 11.312
steps: 12770, episodes: 13, mean episode reward: 600.0, agent episode reward: [0.0, 600.0], [1.3779540204709655, 600.0], time: 11.785
steps: 13770, episodes: 14, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.57487979461321, 500.0], time: 11.584
steps: 14770, episodes: 15, mean episode reward: 1600.0, agent episode reward: [1200.0, 400.0], [1201.794062728794, 400.0], time: 11.863
steps: 15743, episodes: 16, mean episode reward: 1400.0, agent episode reward: [1300.0, 100.0], [1301.1742649724383, 100.0], time: 11.511
steps: 16743, episodes: 17, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.83512816633544, 300.0], time: 11.955
steps: 17743, episodes: 18, mean episode reward: 2600.0, agent episode reward: [1400.0, 1200.0], [1401.6044367849865, 1200.0], time: 12.079
steps: 18743, episodes: 19, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.80601559823805, 200.0], time: 12.29
steps: 19572, episodes: 20, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.00920384746186, 200.0], time: 10.336
steps: 20572, episodes: 21, mean episode reward: 1600.0, agent episode reward: [500.0, 1100.0], [501.7690861832937, 1100.0], time: 12.364
steps: 21572, episodes: 22, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [202.09103673919518, 1400.0], time: 12.691
steps: 22572, episodes: 23, mean episode reward: 1800.0, agent episode reward: [1800.0, 0.0], [1801.5969831571115, 0.0], time: 12.841
steps: 23572, episodes: 24, mean episode reward: 1500.0, agent episode reward: [0.0, 1500.0], [1.4954926282363612, 1500.0], time: 13.33
steps: 24572, episodes: 25, mean episode reward: 3600.0, agent episode reward: [2100.0, 1500.0], [2101.830339259058, 1500.0], time: 13.098
steps: 25572, episodes: 26, mean episode reward: 1400.0, agent episode reward: [1200.0, 200.0], [1201.2591582985983, 200.0], time: 13.245
steps: 26572, episodes: 27, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.24338396257437, 300.0], time: 13.485
steps: 27572, episodes: 28, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.8173938877409, 300.0], time: 13.772
steps: 28572, episodes: 29, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.61158943176446, 500.0], time: 13.543
steps: 29572, episodes: 30, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [102.27471665758421, 1400.0], time: 13.692
steps: 30551, episodes: 31, mean episode reward: 200.0, agent episode reward: [0.0, 200.0], [1.557081159393764, 200.0], time: 13.552
steps: 31551, episodes: 32, mean episode reward: 2600.0, agent episode reward: [1700.0, 900.0], [1701.7140378453878, 900.0], time: 14.077
steps: 32551, episodes: 33, mean episode reward: 2600.0, agent episode reward: [200.0, 2400.0], [201.35152028900572, 2400.0], time: 14.308
steps: 33551, episodes: 34, mean episode reward: 1300.0, agent episode reward: [600.0, 700.0], [601.7882140304142, 700.0], time: 14.429
steps: 34551, episodes: 35, mean episode reward: 1900.0, agent episode reward: [0.0, 1900.0], [1.2332458014525876, 1900.0], time: 14.74
steps: 35476, episodes: 36, mean episode reward: 1600.0, agent episode reward: [100.0, 1500.0], [101.58050933695147, 1500.0], time: 13.504
steps: 36476, episodes: 37, mean episode reward: 2400.0, agent episode reward: [100.0, 2300.0], [101.52452614234168, 2300.0], time: 14.942
steps: 37476, episodes: 38, mean episode reward: 1700.0, agent episode reward: [300.0, 1400.0], [301.2793336188675, 1400.0], time: 15.137
steps: 38476, episodes: 39, mean episode reward: 1600.0, agent episode reward: [1400.0, 200.0], [1401.3877057507782, 200.0], time: 15.23
steps: 39476, episodes: 40, mean episode reward: 2000.0, agent episode reward: [1400.0, 600.0], [1401.827866510573, 600.0], time: 15.304
steps: 40368, episodes: 41, mean episode reward: 1400.0, agent episode reward: [200.0, 1200.0], [201.00558377579748, 1200.0], time: 13.979
steps: 41368, episodes: 42, mean episode reward: 2500.0, agent episode reward: [2500.0, 0.0], [2502.135721267649, 0.0], time: 15.796
steps: 42368, episodes: 43, mean episode reward: 600.0, agent episode reward: [0.0, 600.0], [2.2887676345516397, 600.0], time: 15.837
steps: 43368, episodes: 44, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [602.034450171902, 200.0], time: 16.172
steps: 44368, episodes: 45, mean episode reward: 3700.0, agent episode reward: [300.0, 3400.0], [301.1888071810015, 3400.0], time: 16.321
steps: 45368, episodes: 46, mean episode reward: 2400.0, agent episode reward: [1200.0, 1200.0], [1201.186332369193, 1200.0], time: 16.431
steps: 46368, episodes: 47, mean episode reward: 2600.0, agent episode reward: [300.0, 2300.0], [301.81052006072343, 2300.0], time: 16.876
steps: 47368, episodes: 48, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.4230651513839, 200.0], time: 17.268
steps: 48368, episodes: 49, mean episode reward: 1300.0, agent episode reward: [600.0, 700.0], [601.8319981206901, 700.0], time: 17.421
steps: 49368, episodes: 50, mean episode reward: 1000.0, agent episode reward: [800.0, 200.0], [801.7016540271798, 200.0], time: 17.424
steps: 50368, episodes: 51, mean episode reward: 2400.0, agent episode reward: [300.0, 2100.0], [301.45878647407096, 2100.0], time: 17.675
steps: 51368, episodes: 52, mean episode reward: 4800.0, agent episode reward: [2000.0, 2800.0], [2001.5719907111989, 2800.0], time: 18.008
steps: 52368, episodes: 53, mean episode reward: 1700.0, agent episode reward: [1200.0, 500.0], [1201.9767158024217, 500.0], time: 18.132
steps: 53368, episodes: 54, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.57423304467798, 300.0], time: 18.455
steps: 54368, episodes: 55, mean episode reward: 3600.0, agent episode reward: [3400.0, 200.0], [3401.219954064378, 200.0], time: 18.813
steps: 55368, episodes: 56, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.3902389471686, 200.0], time: 18.948
steps: 56368, episodes: 57, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.8016100424961, 300.0], time: 19.259
steps: 57368, episodes: 58, mean episode reward: 3600.0, agent episode reward: [300.0, 3300.0], [301.1988551918415, 3300.0], time: 19.368
steps: 58368, episodes: 59, mean episode reward: 1200.0, agent episode reward: [900.0, 300.0], [901.4272218354282, 300.0], time: 19.937
steps: 59368, episodes: 60, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.61927597224232, 400.0], time: 19.943/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 60368, episodes: 61, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.40746602239291, 300.0], time: 20.294
steps: 61368, episodes: 62, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.8218545585676, 300.0], time: 20.474
steps: 62368, episodes: 63, mean episode reward: 3300.0, agent episode reward: [2900.0, 400.0], [2901.7708782883587, 400.0], time: 20.803
steps: 63368, episodes: 64, mean episode reward: 3000.0, agent episode reward: [1200.0, 1800.0], [1201.8944820443144, 1800.0], time: 21.111
steps: 64368, episodes: 65, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.54622718680713, 200.0], time: 21.362
steps: 65204, episodes: 66, mean episode reward: 2800.0, agent episode reward: [1300.0, 1500.0], [1301.2515954270477, 1500.0], time: 18.103
steps: 66204, episodes: 67, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [101.47640177047954, 1400.0], time: 21.937
steps: 67204, episodes: 68, mean episode reward: 2500.0, agent episode reward: [1200.0, 1300.0], [1201.5314208392506, 1300.0], time: 22.308
StopIteration()
steps: 68204, episodes: 69, mean episode reward: 2400.0, agent episode reward: [2300.0, 100.0], [2301.7990631010844, 100.0], time: 22.525
steps: 69204, episodes: 70, mean episode reward: 1600.0, agent episode reward: [1100.0, 500.0], [1101.7581551630399, 500.0], time: 22.732
steps: 70204, episodes: 71, mean episode reward: 1300.0, agent episode reward: [300.0, 1000.0], [301.88263627111434, 1000.0], time: 23.128
steps: 71204, episodes: 72, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [302.00312810567624, 500.0], time: 23.441
steps: 72204, episodes: 73, mean episode reward: 1300.0, agent episode reward: [100.0, 1200.0], [101.59069472953364, 1200.0], time: 23.806
steps: 73204, episodes: 74, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.3433427165378, 300.0], time: 24.024
steps: 74204, episodes: 75, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [102.2184722741914, 300.0], time: 24.269
steps: 75204, episodes: 76, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.546020295415, 400.0], time: 24.643
steps: 76204, episodes: 77, mean episode reward: 500.0, agent episode reward: [0.0, 500.0], [1.2892065613100887, 500.0], time: 24.975
steps: 77204, episodes: 78, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.75864341371104, 400.0], time: 25.037
steps: 78204, episodes: 79, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.57091286584387, 500.0], time: 25.565
steps: 79204, episodes: 80, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.3377545339638, 300.0], time: 26.005
steps: 80204, episodes: 81, mean episode reward: 2600.0, agent episode reward: [100.0, 2500.0], [101.51038021012478, 2500.0], time: 26.41
steps: 81204, episodes: 82, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.5766563270081, 200.0], time: 26.771
steps: 82204, episodes: 83, mean episode reward: 1500.0, agent episode reward: [1400.0, 100.0], [1401.9912897036236, 100.0], time: 27.062
steps: 83152, episodes: 84, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.14566902426833, 200.0], time: 25.747
steps: 84152, episodes: 85, mean episode reward: 3400.0, agent episode reward: [2200.0, 1200.0], [2202.028212725885, 1200.0], time: 27.607
steps: 85152, episodes: 86, mean episode reward: 2500.0, agent episode reward: [0.0, 2500.0], [1.6462614453211122, 2500.0], time: 27.773
steps: 86152, episodes: 87, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.84805247403426, 200.0], time: 27.913
steps: 87152, episodes: 88, mean episode reward: 1400.0, agent episode reward: [1200.0, 200.0], [1201.7980144278329, 200.0], time: 28.325
steps: 88152, episodes: 89, mean episode reward: 3600.0, agent episode reward: [1400.0, 2200.0], [1401.3483383312782, 2200.0], time: 28.56
steps: 89152, episodes: 90, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.8866949172419, 300.0], time: 29.002
steps: 90152, episodes: 91, mean episode reward: 4500.0, agent episode reward: [4400.0, 100.0], [4401.317441232485, 100.0], time: 29.261
steps: 91152, episodes: 92, mean episode reward: 700.0, agent episode reward: [500.0, 200.0], [501.5022327028457, 200.0], time: 29.811
steps: 92152, episodes: 93, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.50495786426167, 700.0], time: 29.927
steps: 93152, episodes: 94, mean episode reward: 1800.0, agent episode reward: [0.0, 1800.0], [1.4385528636860516, 1800.0], time: 30.121
steps: 94152, episodes: 95, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.89852288927673, 400.0], time: 30.587
steps: 95152, episodes: 96, mean episode reward: 2500.0, agent episode reward: [200.0, 2300.0], [201.35337547453452, 2300.0], time: 30.984
steps: 96152, episodes: 97, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.4161193701637, 400.0], time: 31.406
steps: 97152, episodes: 98, mean episode reward: 1600.0, agent episode reward: [100.0, 1500.0], [101.54221963982843, 1500.0], time: 31.621
steps: 98152, episodes: 99, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [302.0792973784601, 200.0], time: 31.934
steps: 98948, episodes: 100, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [301.2265511350815, 100.0], time: 25.463
...Finished total of 101 episodes.
2021-10-24 03:27:56.689266: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 03:27:56.693337: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 03:27:56.693601: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x563eabfedb20 executing computations on platform Host. Devices:
2021-10-24 03:27:56.693622: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 1800.0, agent episode reward: [1500.0, 300.0], [1501.9391660878657, 300.0], time: 6.06
steps: 2000, episodes: 2, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.299672904447, 300.0], time: 9.218
steps: 2778, episodes: 3, mean episode reward: 1700.0, agent episode reward: [100.0, 1600.0], [101.09626918348411, 1600.0], time: 7.525
steps: 3778, episodes: 4, mean episode reward: 1800.0, agent episode reward: [0.0, 1800.0], [1.3004152171035037, 1800.0], time: 9.564
steps: 4778, episodes: 5, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.27836753021285, 300.0], time: 9.281
steps: 5778, episodes: 6, mean episode reward: 3300.0, agent episode reward: [1300.0, 2000.0], [1301.6660933156409, 2000.0], time: 10.656
steps: 6778, episodes: 7, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.4425756577582, 500.0], time: 10.529
steps: 7778, episodes: 8, mean episode reward: 2700.0, agent episode reward: [0.0, 2700.0], [1.405057761127686, 2700.0], time: 10.542
steps: 8778, episodes: 9, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.40799833293715, 200.0], time: 10.456
steps: 9778, episodes: 10, mean episode reward: 2300.0, agent episode reward: [800.0, 1500.0], [801.434977600615, 1500.0], time: 11.49
steps: 10778, episodes: 11, mean episode reward: 1400.0, agent episode reward: [1400.0, 0.0], [1401.4226425470192, 0.0], time: 11.426
steps: 11778, episodes: 12, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.3742534745442, 200.0], time: 11.681
steps: 12778, episodes: 13, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.39338184504396, 200.0], time: 11.841
steps: 13778, episodes: 14, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.4554016735232, 400.0], time: 11.781
steps: 14773, episodes: 15, mean episode reward: 600.0, agent episode reward: [600.0, 0.0], [601.2360555321959, 0.0], time: 11.653
steps: 15773, episodes: 16, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.55478860252344, 300.0], time: 12.318
steps: 16773, episodes: 17, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.29556335038336, 200.0], time: 12.085
steps: 17773, episodes: 18, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.25997515542076, 300.0], time: 12.453
steps: 18773, episodes: 19, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.2402685734724, 200.0], time: 12.392
steps: 19773, episodes: 20, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.49700522707826, 300.0], time: 12.824
steps: 20773, episodes: 21, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.28418133375436, 400.0], time: 12.627
steps: 21755, episodes: 22, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.14845219625124, 200.0], time: 12.836
steps: 22755, episodes: 23, mean episode reward: 300.0, agent episode reward: [200.0, 100.0], [201.3050868732386, 100.0], time: 13.182
steps: 23755, episodes: 24, mean episode reward: 2600.0, agent episode reward: [100.0, 2500.0], [101.50199061649832, 2500.0], time: 13.055
steps: 24755, episodes: 25, mean episode reward: 300.0, agent episode reward: [200.0, 100.0], [201.14388091754228, 100.0], time: 13.187
steps: 25755, episodes: 26, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.36779786546174, 200.0], time: 13.173
steps: 26568, episodes: 27, mean episode reward: 2400.0, agent episode reward: [200.0, 2200.0], [200.9163029979907, 2200.0], time: 11.067
steps: 27174, episodes: 28, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [300.88430151431027, 100.0], time: 8.68
steps: 28105, episodes: 29, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.13188501883094, 200.0], time: 12.784
steps: 29105, episodes: 30, mean episode reward: 1300.0, agent episode reward: [300.0, 1000.0], [301.6110365789077, 1000.0], time: 13.853
steps: 30105, episodes: 31, mean episode reward: 2300.0, agent episode reward: [100.0, 2200.0], [101.18965119179805, 2200.0], time: 14.061
steps: 31105, episodes: 32, mean episode reward: 700.0, agent episode reward: [700.0, 0.0], [701.152384570467, 0.0], time: 14.22
steps: 32105, episodes: 33, mean episode reward: 2100.0, agent episode reward: [1800.0, 300.0], [1801.4115307784523, 300.0], time: 14.403
steps: 33105, episodes: 34, mean episode reward: 1600.0, agent episode reward: [300.0, 1300.0], [301.19360456258414, 1300.0], time: 14.447
steps: 33722, episodes: 35, mean episode reward: 2200.0, agent episode reward: [1100.0, 1100.0], [1100.731086207636, 1100.0], time: 6.223
steps: 34722, episodes: 36, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.58852145482504, 400.0], time: 14.655
steps: 35722, episodes: 37, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.2822325367122, 300.0], time: 15.115
steps: 36652, episodes: 38, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.32743500353052, 400.0], time: 13.931
steps: 37652, episodes: 39, mean episode reward: 3800.0, agent episode reward: [3800.0, 0.0], [3801.256864573008, 0.0], time: 15.386
steps: 38652, episodes: 40, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.1809549322421, 100.0], time: 15.477
steps: 39652, episodes: 41, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.48071223988205, 400.0], time: 15.321
steps: 40652, episodes: 42, mean episode reward: 1500.0, agent episode reward: [1400.0, 100.0], [1401.5046449888528, 100.0], time: 15.575
steps: 41652, episodes: 43, mean episode reward: 2600.0, agent episode reward: [600.0, 2000.0], [601.412644231508, 2000.0], time: 15.922
steps: 42652, episodes: 44, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.51926340736844, 100.0], time: 16.037
steps: 43652, episodes: 45, mean episode reward: 2500.0, agent episode reward: [400.0, 2100.0], [401.55763373371553, 2100.0], time: 16.579
steps: 44652, episodes: 46, mean episode reward: 700.0, agent episode reward: [200.0, 500.0], [201.1131990583075, 500.0], time: 16.421
steps: 45652, episodes: 47, mean episode reward: 1700.0, agent episode reward: [500.0, 1200.0], [501.420475872197, 1200.0], time: 16.664
steps: 46583, episodes: 48, mean episode reward: 1400.0, agent episode reward: [400.0, 1000.0], [401.21419454862775, 1000.0], time: 15.682
steps: 47583, episodes: 49, mean episode reward: 2700.0, agent episode reward: [2600.0, 100.0], [2601.323277393081, 100.0], time: 17.349
steps: 48583, episodes: 50, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.47318324546904, 400.0], time: 17.472
steps: 49583, episodes: 51, mean episode reward: 2400.0, agent episode reward: [1000.0, 1400.0], [1001.511217031034, 1400.0], time: 17.837
steps: 50583, episodes: 52, mean episode reward: 200.0, agent episode reward: [100.0, 100.0], [101.12687511130078, 100.0], time: 18.002
steps: 51583, episodes: 53, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.5978952246556, 400.0], time: 18.051
steps: 52583, episodes: 54, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.52932624785376, 300.0], time: 18.406
steps: 53583, episodes: 55, mean episode reward: 3500.0, agent episode reward: [1600.0, 1900.0], [1601.508630750393, 1900.0], time: 18.592
steps: 54583, episodes: 56, mean episode reward: 700.0, agent episode reward: [400.0, 300.0], [401.1962837897156, 300.0], time: 18.798
steps: 55583, episodes: 57, mean episode reward: 2600.0, agent episode reward: [1200.0, 1400.0], [1201.6281443747832, 1400.0], time: 19.191
steps: 56583, episodes: 58, mean episode reward: 2800.0, agent episode reward: [1700.0, 1100.0], [1701.644660219283, 1100.0], time: 19.48
steps: 57394, episodes: 59, mean episode reward: 200.0, agent episode reward: [100.0, 100.0], [100.95613401646398, 100.0], time: 15.934
steps: 58394, episodes: 60, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.3504481151582, 200.0], time: 19.748/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 59394, episodes: 61, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.65852828118406, 300.0], time: 20.205
steps: 60394, episodes: 62, mean episode reward: 1700.0, agent episode reward: [1200.0, 500.0], [1201.1591665689175, 500.0], time: 20.456
steps: 61394, episodes: 63, mean episode reward: 2500.0, agent episode reward: [2400.0, 100.0], [2401.3990655862845, 100.0], time: 20.681
steps: 62394, episodes: 64, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.50615305795003, 200.0], time: 21.016
steps: 63394, episodes: 65, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.4385093677119, 400.0], time: 21.325
steps: 64394, episodes: 66, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.5545837302565, 500.0], time: 21.379
steps: 65394, episodes: 67, mean episode reward: 2400.0, agent episode reward: [1200.0, 1200.0], [1201.2420790897397, 1200.0], time: 22.269
steps: 66394, episodes: 68, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.23052920257155, 400.0], time: 22.058
steps: 67394, episodes: 69, mean episode reward: 1600.0, agent episode reward: [300.0, 1300.0], [301.44289172521303, 1300.0], time: 22.639
steps: 68394, episodes: 70, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.36356930071565, 300.0], time: 22.582
steps: 69394, episodes: 71, mean episode reward: 3700.0, agent episode reward: [2600.0, 1100.0], [2601.4638339272715, 1100.0], time: 22.88
steps: 70394, episodes: 72, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.5536798794463, 400.0], time: 23.142
steps: 71394, episodes: 73, mean episode reward: 1800.0, agent episode reward: [500.0, 1300.0], [501.3673526161705, 1300.0], time: 23.797
steps: 72297, episodes: 74, mean episode reward: 1500.0, agent episode reward: [1300.0, 200.0], [1301.2024024033528, 200.0], time: 21.447
steps: 73297, episodes: 75, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.14116474759558, 700.0], time: 24.011
steps: 74297, episodes: 76, mean episode reward: 1500.0, agent episode reward: [500.0, 1000.0], [501.31084437844135, 1000.0], time: 24.426
steps: 75297, episodes: 77, mean episode reward: 1500.0, agent episode reward: [400.0, 1100.0], [401.3377561421851, 1100.0], time: 24.655
steps: 76297, episodes: 78, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.68130549585933, 300.0], time: 24.993
steps: 77297, episodes: 79, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.39807988060988, 1400.0], time: 25.284
steps: 78297, episodes: 80, mean episode reward: 4700.0, agent episode reward: [4500.0, 200.0], [4501.444799833123, 200.0], time: 25.85
steps: 79297, episodes: 81, mean episode reward: 3800.0, agent episode reward: [1500.0, 2300.0], [1501.229971847401, 2300.0], time: 26.173
steps: 80297, episodes: 82, mean episode reward: 2800.0, agent episode reward: [400.0, 2400.0], [401.52429157585215, 2400.0], time: 26.29
steps: 81263, episodes: 83, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.2627647670194, 1200.0], time: 25.571
steps: 82263, episodes: 84, mean episode reward: 3600.0, agent episode reward: [2200.0, 1400.0], [2201.156112409617, 1400.0], time: 26.869
steps: 83263, episodes: 85, mean episode reward: 2400.0, agent episode reward: [100.0, 2300.0], [101.22443080677733, 2300.0], time: 27.5
steps: 84263, episodes: 86, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.22835912452788, 200.0], time: 27.428
steps: 85263, episodes: 87, mean episode reward: 2800.0, agent episode reward: [1200.0, 1600.0], [1201.5892994783621, 1600.0], time: 27.894
steps: 86263, episodes: 88, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.26039969898324, 200.0], time: 28.065
steps: 87263, episodes: 89, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.4693283003489, 300.0], time: 28.778
StopIteration()
steps: 88263, episodes: 90, mean episode reward: 700.0, agent episode reward: [700.0, 0.0], [701.210315919123, 0.0], time: 28.932
steps: 89263, episodes: 91, mean episode reward: 1400.0, agent episode reward: [100.0, 1300.0], [101.1225864121713, 1300.0], time: 29.016
steps: 90263, episodes: 92, mean episode reward: 1400.0, agent episode reward: [1300.0, 100.0], [1301.2843416354972, 100.0], time: 29.342
steps: 91263, episodes: 93, mean episode reward: 1000.0, agent episode reward: [100.0, 900.0], [101.18611951042931, 900.0], time: 29.924
steps: 92263, episodes: 94, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.5807511162363, 400.0], time: 30.005
steps: 93263, episodes: 95, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.2228836121671, 1400.0], time: 30.326
steps: 94162, episodes: 96, mean episode reward: 1400.0, agent episode reward: [100.0, 1300.0], [101.14893488794377, 1300.0], time: 27.659
steps: 95061, episodes: 97, mean episode reward: 1400.0, agent episode reward: [200.0, 1200.0], [201.2043119994723, 1200.0], time: 27.81
steps: 96061, episodes: 98, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.56290558052619, 400.0], time: 31.417
steps: 97061, episodes: 99, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.3213594191283, 200.0], time: 31.857
steps: 98061, episodes: 100, mean episode reward: 1400.0, agent episode reward: [1300.0, 100.0], [1301.5255536184877, 100.0], time: 32.457
...Finished total of 101 episodes.
2021-10-24 03:58:53.760987: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 03:58:53.765309: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 03:58:53.765467: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5558d48ac380 executing computations on platform Host. Devices:
2021-10-24 03:58:53.765492: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.41541314199304, 400.0], time: 5.878
steps: 2000, episodes: 2, mean episode reward: 2300.0, agent episode reward: [1500.0, 800.0], [1501.5014636275603, 800.0], time: 9.155
steps: 3000, episodes: 3, mean episode reward: 3000.0, agent episode reward: [2900.0, 100.0], [2901.5017478623986, 100.0], time: 9.108
steps: 4000, episodes: 4, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.3304964085724, 500.0], time: 9.621
steps: 5000, episodes: 5, mean episode reward: 600.0, agent episode reward: [100.0, 500.0], [101.31450390728266, 500.0], time: 9.356
steps: 6000, episodes: 6, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.48250453375266, 1200.0], time: 10.286
steps: 7000, episodes: 7, mean episode reward: 2600.0, agent episode reward: [2200.0, 400.0], [2201.4901655986246, 400.0], time: 10.347
steps: 8000, episodes: 8, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.4527112672085, 400.0], time: 10.43
steps: 9000, episodes: 9, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.3393792907819, 400.0], time: 10.457
steps: 10000, episodes: 10, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [402.14214204138545, 100.0], time: 11.377
steps: 11000, episodes: 11, mean episode reward: 800.0, agent episode reward: [700.0, 100.0], [701.5160219460167, 100.0], time: 11.584
StopIteration()
steps: 12000, episodes: 12, mean episode reward: 300.0, agent episode reward: [0.0, 300.0], [1.9312518303569663, 300.0], time: 11.372
steps: 13000, episodes: 13, mean episode reward: 700.0, agent episode reward: [400.0, 300.0], [401.3677298678984, 300.0], time: 11.438
steps: 13637, episodes: 14, mean episode reward: 1300.0, agent episode reward: [1000.0, 300.0], [1001.085881862479, 300.0], time: 7.905
steps: 14637, episodes: 15, mean episode reward: 1700.0, agent episode reward: [1500.0, 200.0], [1501.5215988735667, 200.0], time: 11.779
StopIteration()
steps: 15637, episodes: 16, mean episode reward: 2500.0, agent episode reward: [1100.0, 1400.0], [1101.8178245674183, 1400.0], time: 11.909
steps: 16637, episodes: 17, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.958465370431, 300.0], time: 12.248
steps: 17637, episodes: 18, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.4850083086334, 100.0], time: 9.015
steps: 18637, episodes: 19, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [102.00105339777626, 400.0], time: 12.264
steps: 19279, episodes: 20, mean episode reward: 2300.0, agent episode reward: [1000.0, 1300.0], [1000.9341579511636, 1300.0], time: 8.561
steps: 20279, episodes: 21, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.37426161247112, 200.0], time: 12.856
steps: 20973, episodes: 22, mean episode reward: 1400.0, agent episode reward: [200.0, 1200.0], [201.07197421455984, 1200.0], time: 8.905
steps: 21973, episodes: 23, mean episode reward: 400.0, agent episode reward: [0.0, 400.0], [1.7344844012940406, 400.0], time: 12.963
steps: 22973, episodes: 24, mean episode reward: 2800.0, agent episode reward: [1200.0, 1600.0], [1201.2709941116952, 1600.0], time: 12.816
steps: 23973, episodes: 25, mean episode reward: 1300.0, agent episode reward: [200.0, 1100.0], [201.36896294532085, 1100.0], time: 12.221
steps: 24841, episodes: 26, mean episode reward: 700.0, agent episode reward: [600.0, 100.0], [601.5824597099146, 100.0], time: 10.848
steps: 25841, episodes: 27, mean episode reward: 3200.0, agent episode reward: [100.0, 3100.0], [101.42203922712446, 3100.0], time: 12.689
steps: 26841, episodes: 28, mean episode reward: 1800.0, agent episode reward: [1600.0, 200.0], [1601.597072886834, 200.0], time: 12.829
steps: 27841, episodes: 29, mean episode reward: 2800.0, agent episode reward: [200.0, 2600.0], [201.265910312739, 2600.0], time: 12.832
steps: 28841, episodes: 30, mean episode reward: 4000.0, agent episode reward: [2500.0, 1500.0], [2501.40360631108, 1500.0], time: 13.14
steps: 29756, episodes: 31, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.1185175971869, 200.0], time: 11.84
steps: 30756, episodes: 32, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.6288771819237, 100.0], time: 13.304
steps: 31756, episodes: 33, mean episode reward: 300.0, agent episode reward: [0.0, 300.0], [1.8214597265494263, 300.0], time: 13.949
steps: 32484, episodes: 34, mean episode reward: 1600.0, agent episode reward: [100.0, 1500.0], [100.98621329658106, 1500.0], time: 9.694
steps: 33484, episodes: 35, mean episode reward: 2300.0, agent episode reward: [500.0, 1800.0], [501.2089209026652, 1800.0], time: 13.682
steps: 34291, episodes: 36, mean episode reward: 2100.0, agent episode reward: [0.0, 2100.0], [1.3196108510597557, 2100.0], time: 11.293
steps: 35261, episodes: 37, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.31324099156681, 300.0], time: 13.506
steps: 36261, episodes: 38, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.4326800186825, 400.0], time: 14.119
StopIteration()
steps: 37261, episodes: 39, mean episode reward: 400.0, agent episode reward: [100.0, 300.0], [101.41204640252866, 300.0], time: 14.318
steps: 38040, episodes: 40, mean episode reward: 200.0, agent episode reward: [100.0, 100.0], [101.03107822522207, 100.0], time: 11.491
steps: 39040, episodes: 41, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.2931405636631, 600.0], time: 14.598
steps: 40040, episodes: 42, mean episode reward: 3600.0, agent episode reward: [3100.0, 500.0], [3101.4017096745274, 500.0], time: 14.934
StopIteration()
steps: 41040, episodes: 43, mean episode reward: 3400.0, agent episode reward: [3100.0, 300.0], [3101.4615916116613, 300.0], time: 15.079
steps: 42040, episodes: 44, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.52786062276672, 600.0], time: 15.753
steps: 43040, episodes: 45, mean episode reward: 1600.0, agent episode reward: [1300.0, 300.0], [1301.2540180182693, 300.0], time: 15.335
steps: 44040, episodes: 46, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.33471224217243, 400.0], time: 15.778
StopIteration()
steps: 45040, episodes: 47, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.56041150731403, 300.0], time: 15.964
steps: 46040, episodes: 48, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.39929898040575, 400.0], time: 16.034
steps: 47040, episodes: 49, mean episode reward: 500.0, agent episode reward: [0.0, 500.0], [1.437626549514937, 500.0], time: 16.242
steps: 48040, episodes: 50, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.36899794388626, 1500.0], time: 16.397
steps: 49040, episodes: 51, mean episode reward: 2600.0, agent episode reward: [2200.0, 400.0], [2201.1972529250033, 400.0], time: 17.244
steps: 50040, episodes: 52, mean episode reward: 2000.0, agent episode reward: [1400.0, 600.0], [1401.2620048355952, 600.0], time: 17.026
steps: 50701, episodes: 53, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.05892059385882, 200.0], time: 11.326
steps: 51701, episodes: 54, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.19521025413577, 700.0], time: 17.753
steps: 52701, episodes: 55, mean episode reward: 600.0, agent episode reward: [600.0, 0.0], [601.7703507588143, 0.0], time: 17.53
steps: 53434, episodes: 56, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [200.8985928119823, 400.0], time: 12.846
steps: 54434, episodes: 57, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.2371149379087, 400.0], time: 18.043
steps: 55434, episodes: 58, mean episode reward: 1800.0, agent episode reward: [1300.0, 500.0], [1301.320550749405, 500.0], time: 18.918
steps: 56434, episodes: 59, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.9525971457454, 100.0], time: 18.504/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 57434, episodes: 60, mean episode reward: 800.0, agent episode reward: [700.0, 100.0], [701.3496332827128, 100.0], time: 18.985
steps: 58434, episodes: 61, mean episode reward: 600.0, agent episode reward: [600.0, 0.0], [601.6915542942724, 0.0], time: 19.142
steps: 59434, episodes: 62, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.24922739466206, 300.0], time: 19.599
steps: 60434, episodes: 63, mean episode reward: 900.0, agent episode reward: [900.0, 0.0], [901.4612372937916, 0.0], time: 20.156
steps: 61434, episodes: 64, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.42636426987804, 200.0], time: 19.869
steps: 62434, episodes: 65, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.32466442421975, 200.0], time: 20.955
steps: 63434, episodes: 66, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.9084569747096, 100.0], time: 20.596
steps: 64434, episodes: 67, mean episode reward: 1100.0, agent episode reward: [200.0, 900.0], [201.1514749494214, 900.0], time: 20.933
steps: 65434, episodes: 68, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.5345399481255, 300.0], time: 21.363
steps: 66434, episodes: 69, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.3857949796398, 200.0], time: 21.259
steps: 67434, episodes: 70, mean episode reward: 1000.0, agent episode reward: [900.0, 100.0], [901.4491230359189, 100.0], time: 21.777
steps: 68434, episodes: 71, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.28748408692576, 500.0], time: 22.13
steps: 69434, episodes: 72, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.47308990915437, 300.0], time: 22.593
steps: 70434, episodes: 73, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.72443494168337, 200.0], time: 22.874
steps: 71434, episodes: 74, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.21222968774325, 600.0], time: 22.862
steps: 72434, episodes: 75, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.3169404470311, 400.0], time: 23.471
steps: 73434, episodes: 76, mean episode reward: 1500.0, agent episode reward: [200.0, 1300.0], [201.26516050854968, 1300.0], time: 23.913
steps: 74434, episodes: 77, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.25525883987298, 700.0], time: 23.585
steps: 75434, episodes: 78, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.32578564042217, 300.0], time: 24.286
steps: 76434, episodes: 79, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.2591512648229, 400.0], time: 24.543
steps: 77434, episodes: 80, mean episode reward: 1300.0, agent episode reward: [600.0, 700.0], [601.2248418638529, 700.0], time: 25.246
steps: 78314, episodes: 81, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.33850839877584, 200.0], time: 22.21
steps: 79280, episodes: 82, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.3208261799309, 200.0], time: 24.415
steps: 80280, episodes: 83, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.25396493268244, 200.0], time: 25.826
steps: 81280, episodes: 84, mean episode reward: 800.0, agent episode reward: [200.0, 600.0], [201.25389018427236, 600.0], time: 26.19
steps: 82280, episodes: 85, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.26624698075125, 500.0], time: 26.692
steps: 83280, episodes: 86, mean episode reward: 500.0, agent episode reward: [500.0, 0.0], [501.6506346997272, 0.0], time: 26.934
steps: 84280, episodes: 87, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.69678373581996, 200.0], time: 24.928
steps: 85280, episodes: 88, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.22752427557754, 400.0], time: 27.41
steps: 86280, episodes: 89, mean episode reward: 200.0, agent episode reward: [100.0, 100.0], [101.35435277029384, 100.0], time: 27.836
steps: 87280, episodes: 90, mean episode reward: 700.0, agent episode reward: [500.0, 200.0], [501.48388158861565, 200.0], time: 28.209
steps: 88280, episodes: 91, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.44097291810581, 400.0], time: 28.39
steps: 89280, episodes: 92, mean episode reward: 2500.0, agent episode reward: [2200.0, 300.0], [2201.531514114872, 300.0], time: 28.817
steps: 90280, episodes: 93, mean episode reward: 1600.0, agent episode reward: [1100.0, 500.0], [1101.2791149903358, 500.0], time: 28.719
steps: 91280, episodes: 94, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.29154568960425, 400.0], time: 29.168
steps: 91913, episodes: 95, mean episode reward: 400.0, agent episode reward: [300.0, 100.0], [300.8845911597624, 100.0], time: 18.69
steps: 92913, episodes: 96, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.42910124423736, 100.0], time: 27.987
steps: 93913, episodes: 97, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.26044481531267, 400.0], time: 30.256
steps: 94913, episodes: 98, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.3628424458267, 400.0], time: 30.518
steps: 95913, episodes: 99, mean episode reward: 2500.0, agent episode reward: [500.0, 2000.0], [501.29846198061443, 2000.0], time: 30.809
steps: 96913, episodes: 100, mean episode reward: 700.0, agent episode reward: [500.0, 200.0], [501.22862211014655, 200.0], time: 31.259
...Finished total of 101 episodes.
2021-10-24 04:28:21.320852: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 04:28:21.324859: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 04:28:21.325115: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55aaf4f60cf0 executing computations on platform Host. Devices:
2021-10-24 04:28:21.325136: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 400.0, agent episode reward: [200.0, 200.0], [201.48569586168531, 200.0], time: 5.727
steps: 2000, episodes: 2, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.84746027238725, 200.0], time: 8.62
steps: 3000, episodes: 3, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [202.1878570036992, 300.0], time: 8.42
steps: 4000, episodes: 4, mean episode reward: 1300.0, agent episode reward: [300.0, 1000.0], [301.60522636108914, 1000.0], time: 8.529
steps: 5000, episodes: 5, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.5711601318965, 300.0], time: 9.483
steps: 6000, episodes: 6, mean episode reward: 800.0, agent episode reward: [100.0, 700.0], [101.19049256175697, 700.0], time: 9.761
steps: 7000, episodes: 7, mean episode reward: 2800.0, agent episode reward: [1200.0, 1600.0], [1201.56521381234, 1600.0], time: 9.892
steps: 8000, episodes: 8, mean episode reward: 2600.0, agent episode reward: [1300.0, 1300.0], [1301.9773640900762, 1300.0], time: 10.034
steps: 9000, episodes: 9, mean episode reward: 3500.0, agent episode reward: [1300.0, 2200.0], [1301.7175294148035, 2200.0], time: 10.171
steps: 9965, episodes: 10, mean episode reward: 700.0, agent episode reward: [100.0, 600.0], [101.33247092836325, 600.0], time: 10.055
steps: 10965, episodes: 11, mean episode reward: 300.0, agent episode reward: [0.0, 300.0], [1.9239156149165386, 300.0], time: 8.146
steps: 11965, episodes: 12, mean episode reward: 1600.0, agent episode reward: [300.0, 1300.0], [301.4701007095907, 1300.0], time: 11.347
StopIteration()
steps: 12965, episodes: 13, mean episode reward: 1700.0, agent episode reward: [100.0, 1600.0], [101.38208016092918, 1600.0], time: 10.815
steps: 13965, episodes: 14, mean episode reward: 300.0, agent episode reward: [100.0, 200.0], [101.5520617875435, 200.0], time: 11.144
steps: 14965, episodes: 15, mean episode reward: 1500.0, agent episode reward: [1200.0, 300.0], [1201.6482085603077, 300.0], time: 11.143
steps: 15965, episodes: 16, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.4000291466294, 300.0], time: 11.655
steps: 16965, episodes: 17, mean episode reward: 2500.0, agent episode reward: [200.0, 2300.0], [201.60948631373424, 2300.0], time: 11.401
steps: 17965, episodes: 18, mean episode reward: 3500.0, agent episode reward: [3100.0, 400.0], [3101.5028408697244, 400.0], time: 11.719
steps: 18965, episodes: 19, mean episode reward: 1500.0, agent episode reward: [1100.0, 400.0], [1101.8190775463645, 400.0], time: 11.683
steps: 19965, episodes: 20, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.6908556726688, 100.0], time: 12.527
steps: 20965, episodes: 21, mean episode reward: 1400.0, agent episode reward: [400.0, 1000.0], [401.7575862484627, 1000.0], time: 11.906
steps: 21959, episodes: 22, mean episode reward: 400.0, agent episode reward: [0.0, 400.0], [1.1845680324359973, 400.0], time: 11.978
steps: 22959, episodes: 23, mean episode reward: 2700.0, agent episode reward: [100.0, 2600.0], [101.04928027609341, 2600.0], time: 12.692
steps: 23959, episodes: 24, mean episode reward: 700.0, agent episode reward: [500.0, 200.0], [501.30953690756405, 200.0], time: 13.259
steps: 24959, episodes: 25, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.31962323331823, 300.0], time: 12.473
steps: 25959, episodes: 26, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.6669129523449, 200.0], time: 12.5
steps: 26959, episodes: 27, mean episode reward: 2500.0, agent episode reward: [200.0, 2300.0], [201.4812159398837, 2300.0], time: 12.803
steps: 27959, episodes: 28, mean episode reward: 3500.0, agent episode reward: [1300.0, 2200.0], [1301.7733541182554, 2200.0], time: 13.507
steps: 28959, episodes: 29, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.55698622827157, 400.0], time: 13.012
steps: 29959, episodes: 30, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.6835474822253, 200.0], time: 13.129
steps: 30959, episodes: 31, mean episode reward: 1600.0, agent episode reward: [300.0, 1300.0], [301.7303625819826, 1300.0], time: 13.393
steps: 31959, episodes: 32, mean episode reward: 2500.0, agent episode reward: [2200.0, 300.0], [2201.7424910287095, 300.0], time: 13.92
steps: 32959, episodes: 33, mean episode reward: 600.0, agent episode reward: [300.0, 300.0], [301.45792200179864, 300.0], time: 13.623
steps: 33926, episodes: 34, mean episode reward: 500.0, agent episode reward: [400.0, 100.0], [401.3432888171176, 100.0], time: 13.291
steps: 34926, episodes: 35, mean episode reward: 4100.0, agent episode reward: [2800.0, 1300.0], [2801.7508286505717, 1300.0], time: 14.124
steps: 35926, episodes: 36, mean episode reward: 1600.0, agent episode reward: [1200.0, 400.0], [1201.560259594011, 400.0], time: 14.961
steps: 36926, episodes: 37, mean episode reward: 2700.0, agent episode reward: [500.0, 2200.0], [501.5628106516059, 2200.0], time: 14.508
steps: 37926, episodes: 38, mean episode reward: 4600.0, agent episode reward: [2500.0, 2100.0], [2501.715083503983, 2100.0], time: 14.48
steps: 38926, episodes: 39, mean episode reward: 1800.0, agent episode reward: [1600.0, 200.0], [1601.3138958248492, 200.0], time: 14.627
steps: 39926, episodes: 40, mean episode reward: 1600.0, agent episode reward: [1200.0, 400.0], [1201.7043109500378, 400.0], time: 15.165
steps: 40926, episodes: 41, mean episode reward: 3600.0, agent episode reward: [3300.0, 300.0], [3301.821589722841, 300.0], time: 15.703
steps: 41926, episodes: 42, mean episode reward: 1800.0, agent episode reward: [100.0, 1700.0], [101.49120352544846, 1700.0], time: 15.388
steps: 42926, episodes: 43, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.6942439353396, 500.0], time: 15.461
steps: 43926, episodes: 44, mean episode reward: 3600.0, agent episode reward: [1200.0, 2400.0], [1201.3232058203487, 2400.0], time: 15.695
steps: 44926, episodes: 45, mean episode reward: 1700.0, agent episode reward: [200.0, 1500.0], [201.4877657739073, 1500.0], time: 16.213
steps: 45926, episodes: 46, mean episode reward: 1800.0, agent episode reward: [600.0, 1200.0], [601.3816805614316, 1200.0], time: 16.025
steps: 46926, episodes: 47, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.5782493349362, 200.0], time: 16.231
StopIteration()
steps: 47926, episodes: 48, mean episode reward: 3500.0, agent episode reward: [3200.0, 300.0], [3201.5405454639495, 300.0], time: 16.577
steps: 48926, episodes: 49, mean episode reward: 1000.0, agent episode reward: [200.0, 800.0], [201.48072026380237, 800.0], time: 17.122
steps: 49926, episodes: 50, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.45048987823935, 200.0], time: 17.082
steps: 50926, episodes: 51, mean episode reward: 800.0, agent episode reward: [600.0, 200.0], [601.6561173881688, 200.0], time: 17.489
steps: 51926, episodes: 52, mean episode reward: 2000.0, agent episode reward: [1700.0, 300.0], [1701.5171593113564, 300.0], time: 17.887
steps: 52926, episodes: 53, mean episode reward: 1300.0, agent episode reward: [200.0, 1100.0], [201.48357132183637, 1100.0], time: 17.672
steps: 53926, episodes: 54, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.2822666176993, 1400.0], time: 17.997
steps: 54580, episodes: 55, mean episode reward: 1400.0, agent episode reward: [100.0, 1300.0], [100.88257787720599, 1300.0], time: 11.835
steps: 55580, episodes: 56, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.5841457441978, 1400.0], time: 18.766
steps: 56580, episodes: 57, mean episode reward: 2600.0, agent episode reward: [1200.0, 1400.0], [1201.4038083860664, 1400.0], time: 18.777
steps: 57580, episodes: 58, mean episode reward: 600.0, agent episode reward: [400.0, 200.0], [401.5705480235474, 200.0], time: 18.983
steps: 58580, episodes: 59, mean episode reward: 4800.0, agent episode reward: [3400.0, 1400.0], [3401.8746954008625, 1400.0], time: 19.759/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 59580, episodes: 60, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [101.41033987773004, 1400.0], time: 19.969
steps: 60580, episodes: 61, mean episode reward: 500.0, agent episode reward: [100.0, 400.0], [101.45312787744885, 400.0], time: 19.886
steps: 61580, episodes: 62, mean episode reward: 1600.0, agent episode reward: [200.0, 1400.0], [201.3047225791791, 1400.0], time: 20.022
steps: 62580, episodes: 63, mean episode reward: 2300.0, agent episode reward: [500.0, 1800.0], [501.52398351458015, 1800.0], time: 20.287
steps: 63580, episodes: 64, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.38224413011383, 300.0], time: 21.136
steps: 64580, episodes: 65, mean episode reward: 3600.0, agent episode reward: [3400.0, 200.0], [3401.7059161810294, 200.0], time: 21.123
steps: 65580, episodes: 66, mean episode reward: 1500.0, agent episode reward: [300.0, 1200.0], [301.43759754838095, 1200.0], time: 21.293
steps: 66580, episodes: 67, mean episode reward: 1500.0, agent episode reward: [700.0, 800.0], [701.4500467109792, 800.0], time: 21.527
steps: 67580, episodes: 68, mean episode reward: 3200.0, agent episode reward: [1400.0, 1800.0], [1401.1585061218723, 1800.0], time: 22.254
steps: 68580, episodes: 69, mean episode reward: 1800.0, agent episode reward: [400.0, 1400.0], [401.7076140588265, 1400.0], time: 21.94
steps: 69580, episodes: 70, mean episode reward: 3800.0, agent episode reward: [3600.0, 200.0], [3601.3054296657724, 200.0], time: 22.661
steps: 70580, episodes: 71, mean episode reward: 1800.0, agent episode reward: [1200.0, 600.0], [1201.3713971121929, 600.0], time: 22.986
steps: 71580, episodes: 72, mean episode reward: 600.0, agent episode reward: [200.0, 400.0], [201.64088301753472, 400.0], time: 23.137
steps: 72580, episodes: 73, mean episode reward: 3800.0, agent episode reward: [2500.0, 1300.0], [2501.5024031760076, 1300.0], time: 23.551
steps: 73580, episodes: 74, mean episode reward: 2600.0, agent episode reward: [200.0, 2400.0], [201.1779719586529, 2400.0], time: 23.573
steps: 74580, episodes: 75, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.3703442657346, 1600.0], time: 24.232
steps: 75580, episodes: 76, mean episode reward: 600.0, agent episode reward: [500.0, 100.0], [501.5277556939814, 100.0], time: 24.575
steps: 76580, episodes: 77, mean episode reward: 2600.0, agent episode reward: [0.0, 2600.0], [1.3616985812211742, 2600.0], time: 24.721
steps: 77580, episodes: 78, mean episode reward: 2500.0, agent episode reward: [1300.0, 1200.0], [1301.3268414627955, 1200.0], time: 24.992
steps: 78288, episodes: 79, mean episode reward: 1600.0, agent episode reward: [300.0, 1300.0], [301.1933496939285, 1300.0], time: 17.581
steps: 79288, episodes: 80, mean episode reward: 1800.0, agent episode reward: [200.0, 1600.0], [201.21231351684517, 1600.0], time: 25.642
steps: 80288, episodes: 81, mean episode reward: 2800.0, agent episode reward: [2500.0, 300.0], [2501.552070081254, 300.0], time: 25.61
steps: 81288, episodes: 82, mean episode reward: 800.0, agent episode reward: [400.0, 400.0], [401.25670495329604, 400.0], time: 26.245
steps: 82288, episodes: 83, mean episode reward: 800.0, agent episode reward: [300.0, 500.0], [301.3955633940067, 500.0], time: 26.21
steps: 83288, episodes: 84, mean episode reward: 1800.0, agent episode reward: [100.0, 1700.0], [101.1749179372867, 1700.0], time: 27.067
steps: 84288, episodes: 85, mean episode reward: 2500.0, agent episode reward: [100.0, 2400.0], [101.23754224765781, 2400.0], time: 27.424
steps: 85288, episodes: 86, mean episode reward: 1600.0, agent episode reward: [1400.0, 200.0], [1402.0559944716267, 200.0], time: 27.206
steps: 86172, episodes: 87, mean episode reward: 2800.0, agent episode reward: [1400.0, 1400.0], [1401.201027785476, 1400.0], time: 24.435
steps: 87172, episodes: 88, mean episode reward: 1500.0, agent episode reward: [1100.0, 400.0], [1101.262714687847, 400.0], time: 27.717
steps: 88172, episodes: 89, mean episode reward: 3500.0, agent episode reward: [1400.0, 2100.0], [1401.259319084082, 2100.0], time: 28.446
steps: 89080, episodes: 90, mean episode reward: 500.0, agent episode reward: [200.0, 300.0], [201.29714766668386, 300.0], time: 23.945
steps: 90080, episodes: 91, mean episode reward: 2600.0, agent episode reward: [2300.0, 300.0], [2301.4406500158234, 300.0], time: 29.154
steps: 91080, episodes: 92, mean episode reward: 1800.0, agent episode reward: [400.0, 1400.0], [401.45410339365276, 1400.0], time: 29.193
steps: 92080, episodes: 93, mean episode reward: 1600.0, agent episode reward: [500.0, 1100.0], [501.52499768796775, 1100.0], time: 29.693
steps: 93080, episodes: 94, mean episode reward: 1800.0, agent episode reward: [1400.0, 400.0], [1401.3628110880459, 400.0], time: 29.688
steps: 94080, episodes: 95, mean episode reward: 500.0, agent episode reward: [300.0, 200.0], [301.33254613293116, 200.0], time: 30.674
steps: 95080, episodes: 96, mean episode reward: 2300.0, agent episode reward: [2000.0, 300.0], [2001.6479745213153, 300.0], time: 30.331
steps: 96080, episodes: 97, mean episode reward: 2600.0, agent episode reward: [2100.0, 500.0], [2101.690110703777, 500.0], time: 30.971
steps: 97080, episodes: 98, mean episode reward: 1800.0, agent episode reward: [300.0, 1500.0], [301.4487325584376, 1500.0], time: 31.282
steps: 98080, episodes: 99, mean episode reward: 700.0, agent episode reward: [300.0, 400.0], [301.21352837307455, 400.0], time: 31.496
steps: 99080, episodes: 100, mean episode reward: 3600.0, agent episode reward: [1400.0, 2200.0], [1401.4381302106267, 2200.0], time: 32.091
...Finished total of 101 episodes.
2021-10-24 04:58:53.044547: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 04:58:53.048556: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 04:58:53.048845: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5601ee71df90 executing computations on platform Host. Devices:
2021-10-24 04:58:53.048864: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 4450.0, agent episode reward: [250.0, 4200.0], [251.47901335659935, 4200.0], time: 8.674
steps: 2000, episodes: 2, mean episode reward: 7300.0, agent episode reward: [1200.0, 6100.0], [1201.8554656496153, 6100.0], time: 8.934
steps: 3000, episodes: 3, mean episode reward: 8050.0, agent episode reward: [3000.0, 5050.0], [3001.405289462374, 5050.0], time: 8.693
steps: 4000, episodes: 4, mean episode reward: 10700.0, agent episode reward: [6700.0, 4000.0], [6701.77008737639, 4000.0], time: 9.804
steps: 5000, episodes: 5, mean episode reward: 8550.0, agent episode reward: [1650.0, 6900.0], [1651.8540488509725, 6900.0], time: 8.785
steps: 6000, episodes: 6, mean episode reward: 19950.0, agent episode reward: [7500.0, 12450.0], [7501.504926157388, 12450.0], time: 9.865
steps: 7000, episodes: 7, mean episode reward: 13400.0, agent episode reward: [7450.0, 5950.0], [7451.819846333455, 5950.0], time: 10.353
steps: 8000, episodes: 8, mean episode reward: 7900.0, agent episode reward: [2250.0, 5650.0], [2251.4037526891348, 5650.0], time: 10.028
steps: 9000, episodes: 9, mean episode reward: 8450.0, agent episode reward: [3200.0, 5250.0], [3201.789426604728, 5250.0], time: 9.908
steps: 9820, episodes: 10, mean episode reward: 11050.0, agent episode reward: [4800.0, 6250.0], [4801.172936865923, 6250.0], time: 8.508
steps: 10820, episodes: 11, mean episode reward: 11750.0, agent episode reward: [8250.0, 3500.0], [8251.91530949383, 3500.0], time: 11.198
steps: 11820, episodes: 12, mean episode reward: 8200.0, agent episode reward: [7950.0, 250.0], [7951.495735114618, 250.0], time: 10.733
steps: 12820, episodes: 13, mean episode reward: 10200.0, agent episode reward: [6700.0, 3500.0], [6701.757048476786, 3500.0], time: 11.039
steps: 13820, episodes: 14, mean episode reward: 7200.0, agent episode reward: [250.0, 6950.0], [251.48795496745646, 6950.0], time: 11.108
steps: 14820, episodes: 15, mean episode reward: 3250.0, agent episode reward: [2100.0, 1150.0], [2101.4442106504102, 1150.0], time: 11.776
steps: 15820, episodes: 16, mean episode reward: 13800.0, agent episode reward: [3500.0, 10300.0], [3501.257359657584, 10300.0], time: 11.515
steps: 16820, episodes: 17, mean episode reward: 8350.0, agent episode reward: [4650.0, 3700.0], [4651.402746880562, 3700.0], time: 11.403
steps: 17820, episodes: 18, mean episode reward: 14900.0, agent episode reward: [5500.0, 9400.0], [5501.455893658031, 9400.0], time: 11.652
steps: 18820, episodes: 19, mean episode reward: 15100.0, agent episode reward: [12850.0, 2250.0], [12851.422634334138, 2250.0], time: 12.321
StopIteration()
steps: 19820, episodes: 20, mean episode reward: 13650.0, agent episode reward: [10650.0, 3000.0], [10651.352938032614, 3000.0], time: 11.98
steps: 20820, episodes: 21, mean episode reward: 12850.0, agent episode reward: [4700.0, 8150.0], [4701.502297086262, 8150.0], time: 11.975
steps: 21820, episodes: 22, mean episode reward: 6250.0, agent episode reward: [6000.0, 250.0], [6001.435132368512, 250.0], time: 12.05
steps: 22820, episodes: 23, mean episode reward: 5550.0, agent episode reward: [200.0, 5350.0], [201.33398767485096, 5350.0], time: 12.72
steps: 23820, episodes: 24, mean episode reward: 11150.0, agent episode reward: [3750.0, 7400.0], [3751.38198242439, 7400.0], time: 12.563
steps: 24820, episodes: 25, mean episode reward: 6850.0, agent episode reward: [2900.0, 3950.0], [2901.3257561454457, 3950.0], time: 12.553
steps: 25820, episodes: 26, mean episode reward: 9700.0, agent episode reward: [3350.0, 6350.0], [3351.392731123785, 6350.0], time: 12.905
steps: 26797, episodes: 27, mean episode reward: 13100.0, agent episode reward: [10850.0, 2250.0], [10851.453128459645, 2250.0], time: 12.865
steps: 27797, episodes: 28, mean episode reward: 13150.0, agent episode reward: [3250.0, 9900.0], [3251.3876810508186, 9900.0], time: 12.952
steps: 28797, episodes: 29, mean episode reward: 9200.0, agent episode reward: [5000.0, 4200.0], [5001.332428453176, 4200.0], time: 13.247
steps: 29797, episodes: 30, mean episode reward: 9050.0, agent episode reward: [3200.0, 5850.0], [3201.4341627623644, 5850.0], time: 13.439
steps: 30797, episodes: 31, mean episode reward: 7250.0, agent episode reward: [2850.0, 4400.0], [2851.583516420061, 4400.0], time: 13.765
steps: 31797, episodes: 32, mean episode reward: 5900.0, agent episode reward: [5650.0, 250.0], [5651.568488587651, 250.0], time: 13.254
steps: 32797, episodes: 33, mean episode reward: 12400.0, agent episode reward: [8400.0, 4000.0], [8401.33977424075, 4000.0], time: 13.958
steps: 33797, episodes: 34, mean episode reward: 6800.0, agent episode reward: [3800.0, 3000.0], [3801.7076487936197, 3000.0], time: 13.832
steps: 34797, episodes: 35, mean episode reward: 10400.0, agent episode reward: [9400.0, 1000.0], [9401.693331225248, 1000.0], time: 13.948
steps: 35762, episodes: 36, mean episode reward: 15050.0, agent episode reward: [9050.0, 6000.0], [9051.40366036344, 6000.0], time: 14.11
steps: 36762, episodes: 37, mean episode reward: 4500.0, agent episode reward: [1000.0, 3500.0], [1001.4650084670285, 3500.0], time: 14.857
steps: 37762, episodes: 38, mean episode reward: 4450.0, agent episode reward: [3000.0, 1450.0], [3001.2966638067846, 1450.0], time: 14.613
steps: 38762, episodes: 39, mean episode reward: 8900.0, agent episode reward: [7400.0, 1500.0], [7401.761491190569, 1500.0], time: 14.796
steps: 39762, episodes: 40, mean episode reward: 12400.0, agent episode reward: [4200.0, 8200.0], [4202.030242059429, 8200.0], time: 14.808
steps: 40762, episodes: 41, mean episode reward: 12800.0, agent episode reward: [7750.0, 5050.0], [7751.293974121397, 5050.0], time: 15.524
steps: 41762, episodes: 42, mean episode reward: 14450.0, agent episode reward: [6200.0, 8250.0], [6201.828847991045, 8250.0], time: 15.456
StopIteration()
steps: 42762, episodes: 43, mean episode reward: 5900.0, agent episode reward: [300.0, 5600.0], [301.4339764549708, 5600.0], time: 15.487
steps: 43762, episodes: 44, mean episode reward: 4650.0, agent episode reward: [2250.0, 2400.0], [2251.446110648077, 2400.0], time: 15.63
steps: 44762, episodes: 45, mean episode reward: 7350.0, agent episode reward: [5200.0, 2150.0], [5201.42077681895, 2150.0], time: 16.34
steps: 45762, episodes: 46, mean episode reward: 6350.0, agent episode reward: [5650.0, 700.0], [5651.807940675952, 700.0], time: 16.034
steps: 46762, episodes: 47, mean episode reward: 12000.0, agent episode reward: [5500.0, 6500.0], [5501.342784520919, 6500.0], time: 16.814
steps: 47644, episodes: 48, mean episode reward: 8800.0, agent episode reward: [4750.0, 4050.0], [4751.086147433583, 4050.0], time: 14.546
steps: 48644, episodes: 49, mean episode reward: 11650.0, agent episode reward: [4500.0, 7150.0], [4501.340164495458, 7150.0], time: 16.75
steps: 49644, episodes: 50, mean episode reward: 8900.0, agent episode reward: [2250.0, 6650.0], [2251.3830938139495, 6650.0], time: 17.277
steps: 50644, episodes: 51, mean episode reward: 5300.0, agent episode reward: [2600.0, 2700.0], [2601.5160458748087, 2700.0], time: 17.794
steps: 51644, episodes: 52, mean episode reward: 5050.0, agent episode reward: [250.0, 4800.0], [251.50713340537868, 4800.0], time: 17.549
steps: 52644, episodes: 53, mean episode reward: 12650.0, agent episode reward: [7400.0, 5250.0], [7401.725355121042, 5250.0], time: 17.749
steps: 53644, episodes: 54, mean episode reward: 6500.0, agent episode reward: [4800.0, 1700.0], [4801.406038415629, 1700.0], time: 17.936
steps: 54644, episodes: 55, mean episode reward: 7850.0, agent episode reward: [1700.0, 6150.0], [1701.5866890054401, 6150.0], time: 18.609
steps: 55644, episodes: 56, mean episode reward: 14150.0, agent episode reward: [4000.0, 10150.0], [4001.331818099924, 10150.0], time: 18.55
steps: 56644, episodes: 57, mean episode reward: 6900.0, agent episode reward: [4200.0, 2700.0], [4201.687814238357, 2700.0], time: 18.728
steps: 57644, episodes: 58, mean episode reward: 17250.0, agent episode reward: [9100.0, 8150.0], [9101.557356241172, 8150.0], time: 19.507/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 58644, episodes: 59, mean episode reward: 11700.0, agent episode reward: [5000.0, 6700.0], [5001.3347981636525, 6700.0], time: 19.204
steps: 59644, episodes: 60, mean episode reward: 7100.0, agent episode reward: [4100.0, 3000.0], [4101.3877527033355, 3000.0], time: 19.436
steps: 60644, episodes: 61, mean episode reward: 6750.0, agent episode reward: [1000.0, 5750.0], [1001.3628893155733, 5750.0], time: 19.707
steps: 61644, episodes: 62, mean episode reward: 14950.0, agent episode reward: [11450.0, 3500.0], [11451.489398110847, 3500.0], time: 20.411
steps: 62644, episodes: 63, mean episode reward: 9700.0, agent episode reward: [4200.0, 5500.0], [4201.583423829696, 5500.0], time: 20.381
steps: 63644, episodes: 64, mean episode reward: 8400.0, agent episode reward: [1250.0, 7150.0], [1251.448487624049, 7150.0], time: 20.954
steps: 64644, episodes: 65, mean episode reward: 6550.0, agent episode reward: [5300.0, 1250.0], [5301.612340442447, 1250.0], time: 21.155
steps: 65644, episodes: 66, mean episode reward: 5150.0, agent episode reward: [4900.0, 250.0], [4901.702863890631, 250.0], time: 20.982
steps: 66644, episodes: 67, mean episode reward: 9650.0, agent episode reward: [3000.0, 6650.0], [3001.361150737142, 6650.0], time: 21.565
steps: 67644, episodes: 68, mean episode reward: 10400.0, agent episode reward: [2750.0, 7650.0], [2751.4072970896495, 7650.0], time: 22.345
steps: 68644, episodes: 69, mean episode reward: 4200.0, agent episode reward: [2750.0, 1450.0], [2751.671294101151, 1450.0], time: 22.533
steps: 69644, episodes: 70, mean episode reward: 10050.0, agent episode reward: [6600.0, 3450.0], [6601.671399536689, 3450.0], time: 22.582
steps: 70644, episodes: 71, mean episode reward: 5700.0, agent episode reward: [5450.0, 250.0], [5452.144319165157, 250.0], time: 23.03
steps: 71644, episodes: 72, mean episode reward: 16550.0, agent episode reward: [6900.0, 9650.0], [6901.667059569964, 9650.0], time: 23.313
steps: 72644, episodes: 73, mean episode reward: 13200.0, agent episode reward: [2250.0, 10950.0], [2251.3721717372214, 10950.0], time: 23.401
steps: 73644, episodes: 74, mean episode reward: 6800.0, agent episode reward: [150.0, 6650.0], [151.44954288263466, 6650.0], time: 24.128
steps: 74644, episodes: 75, mean episode reward: 10900.0, agent episode reward: [8400.0, 2500.0], [8401.508883391261, 2500.0], time: 23.812
steps: 75644, episodes: 76, mean episode reward: 13250.0, agent episode reward: [6900.0, 6350.0], [6901.423924711292, 6350.0], time: 24.801
steps: 76644, episodes: 77, mean episode reward: 7950.0, agent episode reward: [3750.0, 4200.0], [3751.303099853008, 4200.0], time: 24.698
steps: 77644, episodes: 78, mean episode reward: 10850.0, agent episode reward: [6200.0, 4650.0], [6201.464673538999, 4650.0], time: 25.035
steps: 78476, episodes: 79, mean episode reward: 10800.0, agent episode reward: [5800.0, 5000.0], [5801.170675581425, 5000.0], time: 21.067
steps: 79476, episodes: 80, mean episode reward: 12200.0, agent episode reward: [5850.0, 6350.0], [5851.622856985055, 6350.0], time: 25.564
steps: 80476, episodes: 81, mean episode reward: 9500.0, agent episode reward: [6750.0, 2750.0], [6751.475904168857, 2750.0], time: 26.34
steps: 81476, episodes: 82, mean episode reward: 5750.0, agent episode reward: [2100.0, 3650.0], [2101.678406208315, 3650.0], time: 26.442
steps: 82476, episodes: 83, mean episode reward: 7700.0, agent episode reward: [5750.0, 1950.0], [5751.29840843618, 1950.0], time: 26.953
steps: 83476, episodes: 84, mean episode reward: 10300.0, agent episode reward: [3650.0, 6650.0], [3651.413522699539, 6650.0], time: 26.796
steps: 84476, episodes: 85, mean episode reward: 11800.0, agent episode reward: [11050.0, 750.0], [11051.620547511577, 750.0], time: 27.093
steps: 85476, episodes: 86, mean episode reward: 19500.0, agent episode reward: [7900.0, 11600.0], [7901.431578378337, 11600.0], time: 27.992
steps: 86387, episodes: 87, mean episode reward: 9250.0, agent episode reward: [3000.0, 6250.0], [3001.364561612833, 6250.0], time: 25.326
steps: 87387, episodes: 88, mean episode reward: 9050.0, agent episode reward: [6050.0, 3000.0], [6051.905110400652, 3000.0], time: 28.505
steps: 88321, episodes: 89, mean episode reward: 8350.0, agent episode reward: [5600.0, 2750.0], [5601.292692032876, 2750.0], time: 26.475
steps: 89321, episodes: 90, mean episode reward: 16850.0, agent episode reward: [9400.0, 7450.0], [9401.838707433762, 7450.0], time: 28.88
steps: 90233, episodes: 91, mean episode reward: 9800.0, agent episode reward: [7550.0, 2250.0], [7551.518434142792, 2250.0], time: 26.127
steps: 91233, episodes: 92, mean episode reward: 9500.0, agent episode reward: [3000.0, 6500.0], [3001.4398538815362, 6500.0], time: 29.226
steps: 92233, episodes: 93, mean episode reward: 9550.0, agent episode reward: [5150.0, 4400.0], [5151.436962225378, 4400.0], time: 29.976
steps: 93233, episodes: 94, mean episode reward: 6450.0, agent episode reward: [2250.0, 4200.0], [2251.6641058749265, 4200.0], time: 30.369
steps: 94233, episodes: 95, mean episode reward: 12900.0, agent episode reward: [9400.0, 3500.0], [9401.318990554148, 3500.0], time: 30.238
steps: 95233, episodes: 96, mean episode reward: 11650.0, agent episode reward: [3500.0, 8150.0], [3501.7642955029664, 8150.0], time: 30.833
steps: 96233, episodes: 97, mean episode reward: 15500.0, agent episode reward: [12250.0, 3250.0], [12251.411803249202, 3250.0], time: 31.223
steps: 97233, episodes: 98, mean episode reward: 6200.0, agent episode reward: [5950.0, 250.0], [5951.637672416913, 250.0], time: 31.452
steps: 98233, episodes: 99, mean episode reward: 10150.0, agent episode reward: [4700.0, 5450.0], [4701.501916835903, 5450.0], time: 31.966
steps: 99233, episodes: 100, mean episode reward: 9950.0, agent episode reward: [7500.0, 2450.0], [7501.599216745257, 2450.0], time: 32.288
...Finished total of 101 episodes.
2021-10-24 05:29:44.885146: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 05:29:44.889158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 05:29:44.889470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56092bfd24c0 executing computations on platform Host. Devices:
2021-10-24 05:29:44.889492: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 14200.0, agent episode reward: [10200.0, 4000.0], [10201.603871605472, 4000.0], time: 5.682
steps: 1987, episodes: 2, mean episode reward: 3850.0, agent episode reward: [250.0, 3600.0], [251.34194333900885, 3600.0], time: 8.098
steps: 2987, episodes: 3, mean episode reward: 9800.0, agent episode reward: [8050.0, 1750.0], [8051.581854115636, 1750.0], time: 8.492
steps: 3987, episodes: 4, mean episode reward: 9800.0, agent episode reward: [5700.0, 4100.0], [5701.376705461425, 4100.0], time: 8.892
steps: 4987, episodes: 5, mean episode reward: 7800.0, agent episode reward: [4900.0, 2900.0], [4901.298898886093, 2900.0], time: 5.945
steps: 5987, episodes: 6, mean episode reward: 7150.0, agent episode reward: [750.0, 6400.0], [751.487755926851, 6400.0], time: 9.961
steps: 6987, episodes: 7, mean episode reward: 9650.0, agent episode reward: [750.0, 8900.0], [751.1568219356312, 8900.0], time: 9.781
steps: 7987, episodes: 8, mean episode reward: 10200.0, agent episode reward: [1500.0, 8700.0], [1501.5816551311452, 8700.0], time: 9.869
steps: 8987, episodes: 9, mean episode reward: 10050.0, agent episode reward: [8300.0, 1750.0], [8301.531748472418, 1750.0], time: 9.831
steps: 9987, episodes: 10, mean episode reward: 11700.0, agent episode reward: [5000.0, 6700.0], [5001.423772199564, 6700.0], time: 11.414
steps: 10987, episodes: 11, mean episode reward: 10700.0, agent episode reward: [5700.0, 5000.0], [5701.263917362022, 5000.0], time: 10.68
steps: 11987, episodes: 12, mean episode reward: 12850.0, agent episode reward: [10150.0, 2700.0], [10151.326680846523, 2700.0], time: 10.821
steps: 12987, episodes: 13, mean episode reward: 6200.0, agent episode reward: [3950.0, 2250.0], [3951.7199106070734, 2250.0], time: 10.925
steps: 13987, episodes: 14, mean episode reward: 10350.0, agent episode reward: [3400.0, 6950.0], [3401.254834299481, 6950.0], time: 11.818
steps: 14752, episodes: 15, mean episode reward: 10250.0, agent episode reward: [6000.0, 4250.0], [6001.3019022272965, 4250.0], time: 8.396
steps: 15752, episodes: 16, mean episode reward: 6950.0, agent episode reward: [6200.0, 750.0], [6201.911403052613, 750.0], time: 11.273
steps: 16752, episodes: 17, mean episode reward: 11950.0, agent episode reward: [6700.0, 5250.0], [6701.4863822345205, 5250.0], time: 11.204
steps: 17752, episodes: 18, mean episode reward: 10750.0, agent episode reward: [5500.0, 5250.0], [5501.39905495034, 5250.0], time: 12.043
steps: 18752, episodes: 19, mean episode reward: 15350.0, agent episode reward: [2250.0, 13100.0], [2251.119977558629, 13100.0], time: 11.891
steps: 19752, episodes: 20, mean episode reward: 17450.0, agent episode reward: [17200.0, 250.0], [17201.58943041897, 250.0], time: 11.853
steps: 20752, episodes: 21, mean episode reward: 11150.0, agent episode reward: [3000.0, 8150.0], [3001.4047991151783, 8150.0], time: 12.026
steps: 21752, episodes: 22, mean episode reward: 15200.0, agent episode reward: [3500.0, 11700.0], [3501.4028803469555, 11700.0], time: 12.333
steps: 22752, episodes: 23, mean episode reward: 5650.0, agent episode reward: [2750.0, 2900.0], [2751.3878433460836, 2900.0], time: 12.084
steps: 23752, episodes: 24, mean episode reward: 12650.0, agent episode reward: [3250.0, 9400.0], [3251.3696598978513, 9400.0], time: 12.206
steps: 24752, episodes: 25, mean episode reward: 11700.0, agent episode reward: [4750.0, 6950.0], [4751.35638608691, 6950.0], time: 12.36
steps: 25752, episodes: 26, mean episode reward: 4250.0, agent episode reward: [1350.0, 2900.0], [1351.3186148619693, 2900.0], time: 13.002
steps: 26752, episodes: 27, mean episode reward: 18150.0, agent episode reward: [11700.0, 6450.0], [11701.336834526819, 6450.0], time: 12.739
steps: 27752, episodes: 28, mean episode reward: 6500.0, agent episode reward: [1000.0, 5500.0], [1001.1325109692636, 5500.0], time: 13.03
steps: 28743, episodes: 29, mean episode reward: 10750.0, agent episode reward: [5250.0, 5500.0], [5251.494537204661, 5500.0], time: 12.855
steps: 29743, episodes: 30, mean episode reward: 9250.0, agent episode reward: [2250.0, 7000.0], [2251.4625573054946, 7000.0], time: 13.426
steps: 30697, episodes: 31, mean episode reward: 15800.0, agent episode reward: [9550.0, 6250.0], [9551.444751806834, 6250.0], time: 12.714
steps: 31697, episodes: 32, mean episode reward: 9900.0, agent episode reward: [3750.0, 6150.0], [3751.1650141797477, 6150.0], time: 13.385
steps: 32697, episodes: 33, mean episode reward: 13850.0, agent episode reward: [9150.0, 4700.0], [9151.378597634292, 4700.0], time: 13.596
steps: 33697, episodes: 34, mean episode reward: 13300.0, agent episode reward: [10100.0, 3200.0], [10101.31372914247, 3200.0], time: 14.462
steps: 34697, episodes: 35, mean episode reward: 7750.0, agent episode reward: [5350.0, 2400.0], [5351.184028792029, 2400.0], time: 13.936
steps: 35697, episodes: 36, mean episode reward: 7350.0, agent episode reward: [6150.0, 1200.0], [6151.327174199264, 1200.0], time: 14.118
steps: 36697, episodes: 37, mean episode reward: 6550.0, agent episode reward: [250.0, 6300.0], [251.3283256662979, 6300.0], time: 14.675
steps: 37493, episodes: 38, mean episode reward: 4550.0, agent episode reward: [250.0, 4300.0], [251.2081931543303, 4300.0], time: 11.398
steps: 38493, episodes: 39, mean episode reward: 7700.0, agent episode reward: [6200.0, 1500.0], [6201.491495727113, 1500.0], time: 14.659
steps: 39493, episodes: 40, mean episode reward: 6400.0, agent episode reward: [2750.0, 3650.0], [2751.3936569813695, 3650.0], time: 14.721
steps: 40493, episodes: 41, mean episode reward: 11150.0, agent episode reward: [10650.0, 500.0], [10651.751735582679, 500.0], time: 15.398
steps: 41493, episodes: 42, mean episode reward: 7850.0, agent episode reward: [3850.0, 4000.0], [3851.4638286776208, 4000.0], time: 15.43
steps: 42493, episodes: 43, mean episode reward: 16150.0, agent episode reward: [13150.0, 3000.0], [13151.661694044145, 3000.0], time: 15.465
steps: 43493, episodes: 44, mean episode reward: 10350.0, agent episode reward: [3450.0, 6900.0], [3451.2794007339235, 6900.0], time: 15.404
steps: 44493, episodes: 45, mean episode reward: 11150.0, agent episode reward: [3450.0, 7700.0], [3451.3675217720997, 7700.0], time: 15.743
steps: 45493, episodes: 46, mean episode reward: 11950.0, agent episode reward: [8200.0, 3750.0], [8201.502339545683, 3750.0], time: 16.661
steps: 46493, episodes: 47, mean episode reward: 8000.0, agent episode reward: [4350.0, 3650.0], [4351.3309941437365, 3650.0], time: 16.546
steps: 47493, episodes: 48, mean episode reward: 11100.0, agent episode reward: [5350.0, 5750.0], [5351.678294627405, 5750.0], time: 16.605
steps: 48493, episodes: 49, mean episode reward: 7250.0, agent episode reward: [2250.0, 5000.0], [2251.423938884663, 5000.0], time: 16.919
StopIteration()
steps: 49493, episodes: 50, mean episode reward: 11200.0, agent episode reward: [1100.0, 10100.0], [1101.3055637582622, 10100.0], time: 17.194
steps: 50493, episodes: 51, mean episode reward: 3550.0, agent episode reward: [1250.0, 2300.0], [1251.268141575062, 2300.0], time: 17.123
steps: 51493, episodes: 52, mean episode reward: 9600.0, agent episode reward: [4950.0, 4650.0], [4951.319161400819, 4650.0], time: 17.436
steps: 52493, episodes: 53, mean episode reward: 6200.0, agent episode reward: [2700.0, 3500.0], [2701.598230393953, 3500.0], time: 18.083
steps: 53493, episodes: 54, mean episode reward: 6700.0, agent episode reward: [5000.0, 1700.0], [5001.429113547958, 1700.0], time: 17.731
steps: 54493, episodes: 55, mean episode reward: 6900.0, agent episode reward: [500.0, 6400.0], [501.33050222090196, 6400.0], time: 18.294
steps: 55493, episodes: 56, mean episode reward: 11950.0, agent episode reward: [2250.0, 9700.0], [2251.440849302938, 9700.0], time: 18.298
steps: 56493, episodes: 57, mean episode reward: 7750.0, agent episode reward: [7000.0, 750.0], [7001.743096357288, 750.0], time: 19.041
steps: 57493, episodes: 58, mean episode reward: 7450.0, agent episode reward: [250.0, 7200.0], [251.35133084574986, 7200.0], time: 18.933/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 58493, episodes: 59, mean episode reward: 4650.0, agent episode reward: [1000.0, 3650.0], [1001.4553147589421, 3650.0], time: 19.051
steps: 59493, episodes: 60, mean episode reward: 10650.0, agent episode reward: [8150.0, 2500.0], [8151.5173480133835, 2500.0], time: 19.576
steps: 60493, episodes: 61, mean episode reward: 9500.0, agent episode reward: [3000.0, 6500.0], [3001.3084440873527, 6500.0], time: 19.958
steps: 61493, episodes: 62, mean episode reward: 16150.0, agent episode reward: [13400.0, 2750.0], [13401.770132066473, 2750.0], time: 20.215
steps: 62493, episodes: 63, mean episode reward: 12200.0, agent episode reward: [5500.0, 6700.0], [5501.506694728218, 6700.0], time: 20.478
steps: 63443, episodes: 64, mean episode reward: 8750.0, agent episode reward: [4500.0, 4250.0], [4501.436492551096, 4250.0], time: 19.413
steps: 64443, episodes: 65, mean episode reward: 11300.0, agent episode reward: [5400.0, 5900.0], [5401.465564001269, 5900.0], time: 20.683
steps: 65443, episodes: 66, mean episode reward: 9150.0, agent episode reward: [5950.0, 3200.0], [5951.373272006545, 3200.0], time: 21.279
steps: 66443, episodes: 67, mean episode reward: 9900.0, agent episode reward: [1000.0, 8900.0], [1001.5391767870922, 8900.0], time: 21.225
steps: 67443, episodes: 68, mean episode reward: 11150.0, agent episode reward: [5000.0, 6150.0], [5001.551062170821, 6150.0], time: 21.631
steps: 68443, episodes: 69, mean episode reward: 7850.0, agent episode reward: [2200.0, 5650.0], [2201.368038602312, 5650.0], time: 22.434
steps: 69443, episodes: 70, mean episode reward: 10850.0, agent episode reward: [5700.0, 5150.0], [5701.427646268173, 5150.0], time: 22.449
steps: 70443, episodes: 71, mean episode reward: 6300.0, agent episode reward: [5350.0, 950.0], [5351.28244590136, 950.0], time: 22.424
steps: 71321, episodes: 72, mean episode reward: 6550.0, agent episode reward: [2300.0, 4250.0], [2301.8380196182784, 4250.0], time: 20.246
steps: 72321, episodes: 73, mean episode reward: 15150.0, agent episode reward: [10900.0, 4250.0], [10901.88931821393, 4250.0], time: 23.268
steps: 73321, episodes: 74, mean episode reward: 6850.0, agent episode reward: [2050.0, 4800.0], [2051.311657291655, 4800.0], time: 23.262
steps: 74321, episodes: 75, mean episode reward: 12700.0, agent episode reward: [3000.0, 9700.0], [3001.401752365113, 9700.0], time: 23.531
steps: 75321, episodes: 76, mean episode reward: 15700.0, agent episode reward: [10700.0, 5000.0], [10701.48729183277, 5000.0], time: 24.275
steps: 76278, episodes: 77, mean episode reward: 7850.0, agent episode reward: [4850.0, 3000.0], [4851.617179399214, 3000.0], time: 23.561
steps: 77278, episodes: 78, mean episode reward: 10350.0, agent episode reward: [9100.0, 1250.0], [9101.482297689792, 1250.0], time: 24.938
steps: 78278, episodes: 79, mean episode reward: 12600.0, agent episode reward: [6200.0, 6400.0], [6201.5912153321815, 6400.0], time: 25.265
steps: 79278, episodes: 80, mean episode reward: 7650.0, agent episode reward: [250.0, 7400.0], [251.36983945100684, 7400.0], time: 25.76
steps: 80278, episodes: 81, mean episode reward: 11350.0, agent episode reward: [8600.0, 2750.0], [8601.626309563366, 2750.0], time: 25.766
steps: 81256, episodes: 82, mean episode reward: 8550.0, agent episode reward: [3000.0, 5550.0], [3001.3973523657764, 5550.0], time: 25.459
steps: 82256, episodes: 83, mean episode reward: 5750.0, agent episode reward: [4500.0, 1250.0], [4502.037365173486, 1250.0], time: 26.428
steps: 83256, episodes: 84, mean episode reward: 19400.0, agent episode reward: [7700.0, 11700.0], [7701.689663987555, 11700.0], time: 26.526
steps: 84256, episodes: 85, mean episode reward: 8050.0, agent episode reward: [250.0, 7800.0], [251.54100886829235, 7800.0], time: 27.037
steps: 85256, episodes: 86, mean episode reward: 6900.0, agent episode reward: [250.0, 6650.0], [251.430810208478, 6650.0], time: 27.75
steps: 86256, episodes: 87, mean episode reward: 8550.0, agent episode reward: [250.0, 8300.0], [251.51872243990857, 8300.0], time: 28.2
steps: 87256, episodes: 88, mean episode reward: 5900.0, agent episode reward: [2750.0, 3150.0], [2751.6051793944926, 3150.0], time: 27.994
steps: 88256, episodes: 89, mean episode reward: 12400.0, agent episode reward: [5750.0, 6650.0], [5751.513116074894, 6650.0], time: 28.479
steps: 89256, episodes: 90, mean episode reward: 13950.0, agent episode reward: [250.0, 13700.0], [251.74120893555073, 13700.0], time: 28.859
steps: 90256, episodes: 91, mean episode reward: 5650.0, agent episode reward: [3000.0, 2650.0], [3001.7563069804787, 2650.0], time: 28.723
steps: 91256, episodes: 92, mean episode reward: 9400.0, agent episode reward: [1000.0, 8400.0], [1001.6022103290421, 8400.0], time: 29.36
steps: 92256, episodes: 93, mean episode reward: 8100.0, agent episode reward: [4750.0, 3350.0], [4751.694632356488, 3350.0], time: 29.859
steps: 93256, episodes: 94, mean episode reward: 10050.0, agent episode reward: [500.0, 9550.0], [501.5620661106693, 9550.0], time: 30.128
steps: 94256, episodes: 95, mean episode reward: 11250.0, agent episode reward: [1000.0, 10250.0], [1001.4260143668042, 10250.0], time: 30.42
steps: 95256, episodes: 96, mean episode reward: 7700.0, agent episode reward: [3250.0, 4450.0], [3251.7630623154514, 4450.0], time: 30.758
steps: 96256, episodes: 97, mean episode reward: 9600.0, agent episode reward: [1750.0, 7850.0], [1751.4772437514828, 7850.0], time: 31.141
steps: 97256, episodes: 98, mean episode reward: 5550.0, agent episode reward: [3400.0, 2150.0], [3401.545080275365, 2150.0], time: 31.349
steps: 98256, episodes: 99, mean episode reward: 10400.0, agent episode reward: [4750.0, 5650.0], [4751.8339276368615, 5650.0], time: 31.615
steps: 99256, episodes: 100, mean episode reward: 7450.0, agent episode reward: [4800.0, 2650.0], [4801.849124542512, 2650.0], time: 32.695
...Finished total of 101 episodes.
2021-10-24 06:00:20.716854: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 06:00:20.721108: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 06:00:20.721322: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x558f4ec6d750 executing computations on platform Host. Devices:
2021-10-24 06:00:20.721337: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 7400.0, agent episode reward: [4700.0, 2700.0], [4701.65372042578, 2700.0], time: 6.414
steps: 2000, episodes: 2, mean episode reward: 5900.0, agent episode reward: [5150.0, 750.0], [5151.7664559385275, 750.0], time: 8.854
steps: 3000, episodes: 3, mean episode reward: 11850.0, agent episode reward: [6150.0, 5700.0], [6151.784619988235, 5700.0], time: 8.927
steps: 4000, episodes: 4, mean episode reward: 9600.0, agent episode reward: [5600.0, 4000.0], [5601.844548902492, 4000.0], time: 8.739
steps: 5000, episodes: 5, mean episode reward: 10200.0, agent episode reward: [5250.0, 4950.0], [5251.3507163632685, 4950.0], time: 9.729
steps: 6000, episodes: 6, mean episode reward: 6450.0, agent episode reward: [1750.0, 4700.0], [1751.3899307484878, 4700.0], time: 9.769
steps: 7000, episodes: 7, mean episode reward: 17400.0, agent episode reward: [9150.0, 8250.0], [9151.585407004737, 8250.0], time: 10.069
steps: 8000, episodes: 8, mean episode reward: 12150.0, agent episode reward: [2250.0, 9900.0], [2251.460310808887, 9900.0], time: 10.32
steps: 9000, episodes: 9, mean episode reward: 5700.0, agent episode reward: [2950.0, 2750.0], [2951.484282916081, 2750.0], time: 10.022
steps: 10000, episodes: 10, mean episode reward: 12950.0, agent episode reward: [4000.0, 8950.0], [4001.4854924370106, 8950.0], time: 10.598
steps: 11000, episodes: 11, mean episode reward: 10600.0, agent episode reward: [10350.0, 250.0], [10351.395483054732, 250.0], time: 11.01
steps: 11890, episodes: 12, mean episode reward: 13050.0, agent episode reward: [10050.0, 3000.0], [10051.460590196413, 3000.0], time: 9.927
steps: 12890, episodes: 13, mean episode reward: 5150.0, agent episode reward: [2250.0, 2900.0], [2251.5013438977617, 2900.0], time: 10.964
steps: 13890, episodes: 14, mean episode reward: 8650.0, agent episode reward: [5650.0, 3000.0], [5651.75773847207, 3000.0], time: 11.235
steps: 14890, episodes: 15, mean episode reward: 5250.0, agent episode reward: [3000.0, 2250.0], [3001.924476780061, 2250.0], time: 11.224
steps: 15890, episodes: 16, mean episode reward: 7550.0, agent episode reward: [3400.0, 4150.0], [3401.609537310117, 4150.0], time: 11.739
steps: 16890, episodes: 17, mean episode reward: 10800.0, agent episode reward: [4000.0, 6800.0], [4001.3226834582815, 6800.0], time: 11.384
steps: 17890, episodes: 18, mean episode reward: 9900.0, agent episode reward: [6650.0, 3250.0], [6651.401531909965, 3250.0], time: 11.393
steps: 18890, episodes: 19, mean episode reward: 7300.0, agent episode reward: [250.0, 7050.0], [251.78343845594534, 7050.0], time: 11.845
steps: 19890, episodes: 20, mean episode reward: 5600.0, agent episode reward: [1000.0, 4600.0], [1001.6759155962428, 4600.0], time: 12.61
steps: 20890, episodes: 21, mean episode reward: 7100.0, agent episode reward: [5350.0, 1750.0], [5351.662285018954, 1750.0], time: 12.181
steps: 21890, episodes: 22, mean episode reward: 11500.0, agent episode reward: [3850.0, 7650.0], [3851.4235021544405, 7650.0], time: 12.128
steps: 22890, episodes: 23, mean episode reward: 2150.0, agent episode reward: [900.0, 1250.0], [901.4207406519344, 1250.0], time: 12.211
steps: 23890, episodes: 24, mean episode reward: 12200.0, agent episode reward: [3200.0, 9000.0], [3201.555461877257, 9000.0], time: 13.148
steps: 24890, episodes: 25, mean episode reward: 7300.0, agent episode reward: [3800.0, 3500.0], [3801.372535233665, 3500.0], time: 12.522
steps: 25890, episodes: 26, mean episode reward: 11650.0, agent episode reward: [6400.0, 5250.0], [6401.727678886315, 5250.0], time: 13.028
steps: 26890, episodes: 27, mean episode reward: 9800.0, agent episode reward: [4300.0, 5500.0], [4301.424824769604, 5500.0], time: 12.944
steps: 27890, episodes: 28, mean episode reward: 8300.0, agent episode reward: [3150.0, 5150.0], [3151.4982751369575, 5150.0], time: 13.705
steps: 28890, episodes: 29, mean episode reward: 4500.0, agent episode reward: [3100.0, 1400.0], [3101.9956093746596, 1400.0], time: 13.251
steps: 29890, episodes: 30, mean episode reward: 9350.0, agent episode reward: [4900.0, 4450.0], [4901.432628276846, 4450.0], time: 13.419
steps: 30890, episodes: 31, mean episode reward: 11850.0, agent episode reward: [7850.0, 4000.0], [7851.499847072954, 4000.0], time: 13.42
steps: 31890, episodes: 32, mean episode reward: 4700.0, agent episode reward: [1900.0, 2800.0], [1901.4393835041067, 2800.0], time: 13.986
steps: 32890, episodes: 33, mean episode reward: 5600.0, agent episode reward: [2650.0, 2950.0], [2651.465609734804, 2950.0], time: 13.784
steps: 33890, episodes: 34, mean episode reward: 7750.0, agent episode reward: [5600.0, 2150.0], [5601.372549619191, 2150.0], time: 14.065
steps: 34890, episodes: 35, mean episode reward: 15400.0, agent episode reward: [4750.0, 10650.0], [4751.352681160197, 10650.0], time: 14.275
steps: 35890, episodes: 36, mean episode reward: 10050.0, agent episode reward: [1250.0, 8800.0], [1251.8596170096216, 8800.0], time: 14.904
steps: 36890, episodes: 37, mean episode reward: 10700.0, agent episode reward: [6500.0, 4200.0], [6501.663629525541, 4200.0], time: 14.377
steps: 37890, episodes: 38, mean episode reward: 3500.0, agent episode reward: [2650.0, 850.0], [2651.4088577112066, 850.0], time: 15.09
steps: 38890, episodes: 39, mean episode reward: 6400.0, agent episode reward: [3800.0, 2600.0], [3801.4517051532216, 2600.0], time: 14.808
steps: 39890, episodes: 40, mean episode reward: 8600.0, agent episode reward: [1000.0, 7600.0], [1001.6282571599716, 7600.0], time: 15.336
steps: 40890, episodes: 41, mean episode reward: 5450.0, agent episode reward: [2650.0, 2800.0], [2651.4260471377365, 2800.0], time: 15.131
steps: 41870, episodes: 42, mean episode reward: 9800.0, agent episode reward: [6300.0, 3500.0], [6301.301670994289, 3500.0], time: 14.854
steps: 42870, episodes: 43, mean episode reward: 11650.0, agent episode reward: [5450.0, 6200.0], [5451.294799421629, 6200.0], time: 15.797
steps: 43870, episodes: 44, mean episode reward: 12700.0, agent episode reward: [9950.0, 2750.0], [9951.577975304857, 2750.0], time: 16.339
steps: 44870, episodes: 45, mean episode reward: 2950.0, agent episode reward: [2250.0, 700.0], [2251.235319196171, 700.0], time: 15.981
steps: 45870, episodes: 46, mean episode reward: 14300.0, agent episode reward: [10300.0, 4000.0], [10301.558957132554, 4000.0], time: 16.251
steps: 46744, episodes: 47, mean episode reward: 8550.0, agent episode reward: [2250.0, 6300.0], [2251.2367440175362, 6300.0], time: 14.399
steps: 47744, episodes: 48, mean episode reward: 8750.0, agent episode reward: [750.0, 8000.0], [751.2679003722827, 8000.0], time: 17.173
steps: 48744, episodes: 49, mean episode reward: 12900.0, agent episode reward: [4000.0, 8900.0], [4001.308034711268, 8900.0], time: 17.138
steps: 49744, episodes: 50, mean episode reward: 7350.0, agent episode reward: [550.0, 6800.0], [551.379104266182, 6800.0], time: 17.207
steps: 50744, episodes: 51, mean episode reward: 12650.0, agent episode reward: [4700.0, 7950.0], [4701.5901361360275, 7950.0], time: 17.471
steps: 51744, episodes: 52, mean episode reward: 5650.0, agent episode reward: [4400.0, 1250.0], [4401.535030993943, 1250.0], time: 18.273
steps: 52744, episodes: 53, mean episode reward: 7650.0, agent episode reward: [2950.0, 4700.0], [2951.626296111232, 4700.0], time: 18.02
steps: 53744, episodes: 54, mean episode reward: 5350.0, agent episode reward: [1200.0, 4150.0], [1201.3465683048578, 4150.0], time: 18.241
steps: 54744, episodes: 55, mean episode reward: 8400.0, agent episode reward: [3400.0, 5000.0], [3401.562552081898, 5000.0], time: 18.691
steps: 55744, episodes: 56, mean episode reward: 3000.0, agent episode reward: [450.0, 2550.0], [451.3815853629136, 2550.0], time: 18.564
steps: 56744, episodes: 57, mean episode reward: 9800.0, agent episode reward: [1000.0, 8800.0], [1001.5156374880391, 8800.0], time: 18.811
steps: 57744, episodes: 58, mean episode reward: 8150.0, agent episode reward: [2250.0, 5900.0], [2251.3970116398095, 5900.0], time: 19.497
steps: 58744, episodes: 59, mean episode reward: 7150.0, agent episode reward: [4250.0, 2900.0], [4251.4866135535285, 2900.0], time: 19.38/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 59744, episodes: 60, mean episode reward: 3850.0, agent episode reward: [3350.0, 500.0], [3351.4113630228153, 500.0], time: 19.848
steps: 60744, episodes: 61, mean episode reward: 5000.0, agent episode reward: [400.0, 4600.0], [401.4032383029995, 4600.0], time: 20.154
steps: 61744, episodes: 62, mean episode reward: 8300.0, agent episode reward: [4100.0, 4200.0], [4101.509850522052, 4200.0], time: 20.069
steps: 62744, episodes: 63, mean episode reward: 6950.0, agent episode reward: [700.0, 6250.0], [701.4890432668509, 6250.0], time: 20.327
steps: 63744, episodes: 64, mean episode reward: 14300.0, agent episode reward: [3500.0, 10800.0], [3501.612953552375, 10800.0], time: 20.746
steps: 64744, episodes: 65, mean episode reward: 10350.0, agent episode reward: [2250.0, 8100.0], [2251.5413960607857, 8100.0], time: 21.29
steps: 65744, episodes: 66, mean episode reward: 7650.0, agent episode reward: [2250.0, 5400.0], [2251.709471743144, 5400.0], time: 21.282
steps: 66744, episodes: 67, mean episode reward: 1500.0, agent episode reward: [650.0, 850.0], [651.5996792322529, 850.0], time: 21.814
steps: 67744, episodes: 68, mean episode reward: 550.0, agent episode reward: [300.0, 250.0], [301.33722981484567, 250.0], time: 21.767
steps: 68744, episodes: 69, mean episode reward: 9700.0, agent episode reward: [7450.0, 2250.0], [7451.760465681896, 2250.0], time: 22.166
steps: 69744, episodes: 70, mean episode reward: 3600.0, agent episode reward: [950.0, 2650.0], [951.4434658745536, 2650.0], time: 22.748
steps: 70744, episodes: 71, mean episode reward: 7500.0, agent episode reward: [4750.0, 2750.0], [4751.381129864715, 2750.0], time: 22.741
steps: 71744, episodes: 72, mean episode reward: 7150.0, agent episode reward: [5900.0, 1250.0], [5901.4204285548585, 1250.0], time: 23.776
steps: 72744, episodes: 73, mean episode reward: 17400.0, agent episode reward: [7450.0, 9950.0], [7451.467257452081, 9950.0], time: 23.322
steps: 73744, episodes: 74, mean episode reward: 5800.0, agent episode reward: [4150.0, 1650.0], [4151.481299415182, 1650.0], time: 23.948
steps: 74744, episodes: 75, mean episode reward: 6350.0, agent episode reward: [4100.0, 2250.0], [4101.865616721295, 2250.0], time: 24.433
steps: 75744, episodes: 76, mean episode reward: 8700.0, agent episode reward: [2750.0, 5950.0], [2751.388791116831, 5950.0], time: 24.227
steps: 76744, episodes: 77, mean episode reward: 13650.0, agent episode reward: [8650.0, 5000.0], [8651.533513830123, 5000.0], time: 24.842
steps: 77576, episodes: 78, mean episode reward: 8550.0, agent episode reward: [3800.0, 4750.0], [3801.2025318907367, 4750.0], time: 20.432
steps: 78576, episodes: 79, mean episode reward: 5400.0, agent episode reward: [4900.0, 500.0], [4901.356686754916, 500.0], time: 25.296
steps: 79576, episodes: 80, mean episode reward: 3100.0, agent episode reward: [2250.0, 850.0], [2251.4580503275006, 850.0], time: 26.024
steps: 80576, episodes: 81, mean episode reward: 13050.0, agent episode reward: [11300.0, 1750.0], [11301.44141538607, 1750.0], time: 25.855
steps: 81576, episodes: 82, mean episode reward: 15400.0, agent episode reward: [4250.0, 11150.0], [4251.416063331021, 11150.0], time: 26.134
steps: 82576, episodes: 83, mean episode reward: 5250.0, agent episode reward: [4750.0, 500.0], [4751.53728675678, 500.0], time: 26.201
steps: 83576, episodes: 84, mean episode reward: 7600.0, agent episode reward: [750.0, 6850.0], [751.8575597432006, 6850.0], time: 27.129
steps: 84576, episodes: 85, mean episode reward: 12500.0, agent episode reward: [3850.0, 8650.0], [3851.3925084360812, 8650.0], time: 27.165
steps: 85576, episodes: 86, mean episode reward: 12400.0, agent episode reward: [2750.0, 9650.0], [2751.222880338846, 9650.0], time: 27.462
steps: 86576, episodes: 87, mean episode reward: 1500.0, agent episode reward: [100.0, 1400.0], [101.54395792066053, 1400.0], time: 27.976
steps: 87576, episodes: 88, mean episode reward: 3900.0, agent episode reward: [2950.0, 950.0], [2951.5918667769515, 950.0], time: 28.187
steps: 88576, episodes: 89, mean episode reward: 3850.0, agent episode reward: [1350.0, 2500.0], [1351.2701710255694, 2500.0], time: 28.66
steps: 89576, episodes: 90, mean episode reward: 9150.0, agent episode reward: [8150.0, 1000.0], [8151.517287307659, 1000.0], time: 28.744
steps: 90576, episodes: 91, mean episode reward: 10700.0, agent episode reward: [6500.0, 4200.0], [6501.54847380427, 4200.0], time: 29.316
steps: 91576, episodes: 92, mean episode reward: 4100.0, agent episode reward: [500.0, 3600.0], [501.5668614072581, 3600.0], time: 29.438
steps: 92576, episodes: 93, mean episode reward: 11100.0, agent episode reward: [7350.0, 3750.0], [7351.532046081819, 3750.0], time: 29.971
steps: 93576, episodes: 94, mean episode reward: 9850.0, agent episode reward: [2750.0, 7100.0], [2751.39997865631, 7100.0], time: 30.179
steps: 94576, episodes: 95, mean episode reward: 5200.0, agent episode reward: [4250.0, 950.0], [4251.7422702186, 950.0], time: 30.457
steps: 95576, episodes: 96, mean episode reward: 7750.0, agent episode reward: [6300.0, 1450.0], [6301.383179350511, 1450.0], time: 30.812
steps: 96576, episodes: 97, mean episode reward: 10100.0, agent episode reward: [7600.0, 2500.0], [7601.551467354023, 2500.0], time: 31.289
steps: 97576, episodes: 98, mean episode reward: 18650.0, agent episode reward: [13150.0, 5500.0], [13151.584288372407, 5500.0], time: 31.267
steps: 98576, episodes: 99, mean episode reward: 13500.0, agent episode reward: [3000.0, 10500.0], [3001.4136360465836, 10500.0], time: 32.026
steps: 99482, episodes: 100, mean episode reward: 10550.0, agent episode reward: [2300.0, 8250.0], [2301.3043979916624, 8250.0], time: 28.861
...Finished total of 101 episodes.
2021-10-24 06:31:19.220312: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 06:31:19.224546: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 06:31:19.224958: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x561b62f74af0 executing computations on platform Host. Devices:
2021-10-24 06:31:19.224975: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 8100.0, agent episode reward: [7350.0, 750.0], [7351.55793753738, 750.0], time: 5.744
steps: 2000, episodes: 2, mean episode reward: 9800.0, agent episode reward: [1500.0, 8300.0], [1501.4274972530711, 8300.0], time: 8.28
steps: 3000, episodes: 3, mean episode reward: 8050.0, agent episode reward: [3150.0, 4900.0], [3151.7795782974276, 4900.0], time: 8.364
steps: 4000, episodes: 4, mean episode reward: 11150.0, agent episode reward: [6200.0, 4950.0], [6201.815722638118, 4950.0], time: 9.017
steps: 5000, episodes: 5, mean episode reward: 19350.0, agent episode reward: [5200.0, 14150.0], [5201.551486049135, 14150.0], time: 8.68
steps: 6000, episodes: 6, mean episode reward: 7500.0, agent episode reward: [2600.0, 4900.0], [2601.544836668839, 4900.0], time: 9.787
steps: 7000, episodes: 7, mean episode reward: 10650.0, agent episode reward: [9500.0, 1150.0], [9501.563385339501, 1150.0], time: 9.547
steps: 8000, episodes: 8, mean episode reward: 13450.0, agent episode reward: [8700.0, 4750.0], [8701.8477999016, 4750.0], time: 10.018
steps: 9000, episodes: 9, mean episode reward: 8500.0, agent episode reward: [7500.0, 1000.0], [7501.909910682074, 1000.0], time: 10.232
steps: 10000, episodes: 10, mean episode reward: 14700.0, agent episode reward: [8250.0, 6450.0], [8251.622142356839, 6450.0], time: 11.546
steps: 11000, episodes: 11, mean episode reward: 12200.0, agent episode reward: [7450.0, 4750.0], [7451.687065829908, 4750.0], time: 10.636
steps: 12000, episodes: 12, mean episode reward: 7500.0, agent episode reward: [3650.0, 3850.0], [3651.5907690229265, 3850.0], time: 10.804
StopIteration()
steps: 13000, episodes: 13, mean episode reward: 11900.0, agent episode reward: [5150.0, 6750.0], [5151.757836280251, 6750.0], time: 11.081
steps: 14000, episodes: 14, mean episode reward: 4950.0, agent episode reward: [250.0, 4700.0], [251.38242463154882, 4700.0], time: 11.765
steps: 15000, episodes: 15, mean episode reward: 10850.0, agent episode reward: [500.0, 10350.0], [501.3229419540812, 10350.0], time: 11.273
StopIteration()
steps: 16000, episodes: 16, mean episode reward: 8550.0, agent episode reward: [6050.0, 2500.0], [6051.717238756129, 2500.0], time: 11.273
steps: 17000, episodes: 17, mean episode reward: 11400.0, agent episode reward: [1500.0, 9900.0], [1501.2966526638972, 9900.0], time: 11.492
steps: 18000, episodes: 18, mean episode reward: 5050.0, agent episode reward: [4150.0, 900.0], [4151.525101128636, 900.0], time: 12.146
steps: 19000, episodes: 19, mean episode reward: 13000.0, agent episode reward: [6000.0, 7000.0], [6001.595639537062, 7000.0], time: 11.706
steps: 20000, episodes: 20, mean episode reward: 10000.0, agent episode reward: [250.0, 9750.0], [251.40464514017629, 9750.0], time: 11.954
steps: 21000, episodes: 21, mean episode reward: 14850.0, agent episode reward: [10150.0, 4700.0], [10151.509329668777, 4700.0], time: 11.958
steps: 22000, episodes: 22, mean episode reward: 11850.0, agent episode reward: [7100.0, 4750.0], [7101.50591780796, 4750.0], time: 12.97
steps: 23000, episodes: 23, mean episode reward: 12700.0, agent episode reward: [3250.0, 9450.0], [3251.346000599075, 9450.0], time: 12.034
steps: 24000, episodes: 24, mean episode reward: 7200.0, agent episode reward: [250.0, 6950.0], [251.47034505797458, 6950.0], time: 12.352
steps: 25000, episodes: 25, mean episode reward: 7650.0, agent episode reward: [2200.0, 5450.0], [2201.530208804303, 5450.0], time: 12.503
steps: 26000, episodes: 26, mean episode reward: 5850.0, agent episode reward: [1700.0, 4150.0], [1701.6331224225503, 4150.0], time: 12.987
steps: 27000, episodes: 27, mean episode reward: 10500.0, agent episode reward: [3950.0, 6550.0], [3951.640589196563, 6550.0], time: 12.711
steps: 28000, episodes: 28, mean episode reward: 6850.0, agent episode reward: [1750.0, 5100.0], [1751.638000884739, 5100.0], time: 12.906
steps: 29000, episodes: 29, mean episode reward: 10800.0, agent episode reward: [8550.0, 2250.0], [8551.51262978029, 2250.0], time: 12.918
steps: 30000, episodes: 30, mean episode reward: 5700.0, agent episode reward: [4700.0, 1000.0], [4701.354523131995, 1000.0], time: 13.372
steps: 31000, episodes: 31, mean episode reward: 5100.0, agent episode reward: [3950.0, 1150.0], [3951.638681303159, 1150.0], time: 13.306
StopIteration()
steps: 32000, episodes: 32, mean episode reward: 2850.0, agent episode reward: [500.0, 2350.0], [501.4279372157894, 2350.0], time: 13.695
steps: 33000, episodes: 33, mean episode reward: 11500.0, agent episode reward: [8000.0, 3500.0], [8001.450891855408, 3500.0], time: 13.652
steps: 34000, episodes: 34, mean episode reward: 7050.0, agent episode reward: [2750.0, 4300.0], [2751.342833623216, 4300.0], time: 14.025
steps: 35000, episodes: 35, mean episode reward: 12650.0, agent episode reward: [9400.0, 3250.0], [9401.61808782396, 3250.0], time: 13.839
steps: 36000, episodes: 36, mean episode reward: 12000.0, agent episode reward: [9350.0, 2650.0], [9351.614849952755, 2650.0], time: 14.185
steps: 37000, episodes: 37, mean episode reward: 5400.0, agent episode reward: [3700.0, 1700.0], [3701.7337649826954, 1700.0], time: 14.22
steps: 38000, episodes: 38, mean episode reward: 4350.0, agent episode reward: [4100.0, 250.0], [4101.518550667594, 250.0], time: 14.78
steps: 39000, episodes: 39, mean episode reward: 12900.0, agent episode reward: [8250.0, 4650.0], [8251.515690481001, 4650.0], time: 14.703
steps: 40000, episodes: 40, mean episode reward: 13400.0, agent episode reward: [8900.0, 4500.0], [8901.461028696038, 4500.0], time: 15.052
steps: 41000, episodes: 41, mean episode reward: 14800.0, agent episode reward: [7500.0, 7300.0], [7501.441911243129, 7300.0], time: 14.942
steps: 42000, episodes: 42, mean episode reward: 10400.0, agent episode reward: [1500.0, 8900.0], [1501.379185136665, 8900.0], time: 15.839
steps: 43000, episodes: 43, mean episode reward: 5850.0, agent episode reward: [750.0, 5100.0], [751.480042035463, 5100.0], time: 15.535
StopIteration()
steps: 44000, episodes: 44, mean episode reward: 10100.0, agent episode reward: [4000.0, 6100.0], [4001.353430594002, 6100.0], time: 15.834
steps: 45000, episodes: 45, mean episode reward: 5800.0, agent episode reward: [250.0, 5550.0], [251.4077430793316, 5550.0], time: 13.831
StopIteration()
steps: 46000, episodes: 46, mean episode reward: 13050.0, agent episode reward: [7750.0, 5300.0], [7751.563389881735, 5300.0], time: 16.499
steps: 47000, episodes: 47, mean episode reward: 11750.0, agent episode reward: [4750.0, 7000.0], [4751.256537381466, 7000.0], time: 16.298
steps: 48000, episodes: 48, mean episode reward: 14700.0, agent episode reward: [8200.0, 6500.0], [8201.317537882669, 6500.0], time: 16.974
steps: 49000, episodes: 49, mean episode reward: 4500.0, agent episode reward: [2500.0, 2000.0], [2501.4312917840225, 2000.0], time: 16.891
steps: 50000, episodes: 50, mean episode reward: 16250.0, agent episode reward: [8500.0, 7750.0], [8501.67259759447, 7750.0], time: 17.51
StopIteration()
steps: 51000, episodes: 51, mean episode reward: 5350.0, agent episode reward: [250.0, 5100.0], [251.414955563958, 5100.0], time: 17.433
steps: 52000, episodes: 52, mean episode reward: 14750.0, agent episode reward: [8500.0, 6250.0], [8501.668583406225, 6250.0], time: 17.574
steps: 53000, episodes: 53, mean episode reward: 11700.0, agent episode reward: [8700.0, 3000.0], [8701.470206806734, 3000.0], time: 18.095
steps: 54000, episodes: 54, mean episode reward: 13400.0, agent episode reward: [13150.0, 250.0], [13151.482545254734, 250.0], time: 18.031
steps: 55000, episodes: 55, mean episode reward: 7650.0, agent episode reward: [7150.0, 500.0], [7151.396487127543, 500.0], time: 18.74
steps: 55761, episodes: 56, mean episode reward: 5000.0, agent episode reward: [2250.0, 2750.0], [2251.146330509521, 2750.0], time: 14.194
steps: 56761, episodes: 57, mean episode reward: 9700.0, agent episode reward: [500.0, 9200.0], [501.2716225999006, 9200.0], time: 18.784
steps: 57761, episodes: 58, mean episode reward: 16900.0, agent episode reward: [7200.0, 9700.0], [7201.383439248232, 9700.0], time: 18.974/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 58761, episodes: 59, mean episode reward: 11850.0, agent episode reward: [5150.0, 6700.0], [5151.400230517853, 6700.0], time: 19.244
steps: 59761, episodes: 60, mean episode reward: 10950.0, agent episode reward: [4700.0, 6250.0], [4701.474270628067, 6250.0], time: 20.041
steps: 60761, episodes: 61, mean episode reward: 9050.0, agent episode reward: [7300.0, 1750.0], [7301.436544464729, 1750.0], time: 19.766
steps: 61761, episodes: 62, mean episode reward: 8350.0, agent episode reward: [4000.0, 4350.0], [4001.4038135446635, 4350.0], time: 20.109
steps: 62761, episodes: 63, mean episode reward: 8000.0, agent episode reward: [6350.0, 1650.0], [6351.569335267147, 1650.0], time: 20.833
steps: 63761, episodes: 64, mean episode reward: 8450.0, agent episode reward: [6200.0, 2250.0], [6201.520396439405, 2250.0], time: 20.908
steps: 64761, episodes: 65, mean episode reward: 8550.0, agent episode reward: [3700.0, 4850.0], [3701.616810672888, 4850.0], time: 20.978
steps: 65761, episodes: 66, mean episode reward: 12300.0, agent episode reward: [5450.0, 6850.0], [5451.363309813545, 6850.0], time: 20.955
steps: 66761, episodes: 67, mean episode reward: 5050.0, agent episode reward: [2800.0, 2250.0], [2801.5534443647316, 2250.0], time: 21.985
steps: 67761, episodes: 68, mean episode reward: 5250.0, agent episode reward: [3450.0, 1800.0], [3451.501855208027, 1800.0], time: 22.088
steps: 68761, episodes: 69, mean episode reward: 12900.0, agent episode reward: [12150.0, 750.0], [12151.7175641789, 750.0], time: 21.986
steps: 69761, episodes: 70, mean episode reward: 10950.0, agent episode reward: [9950.0, 1000.0], [9951.414954283167, 1000.0], time: 22.269
steps: 70761, episodes: 71, mean episode reward: 10300.0, agent episode reward: [7350.0, 2950.0], [7351.470562317504, 2950.0], time: 22.5
steps: 71761, episodes: 72, mean episode reward: 8200.0, agent episode reward: [3700.0, 4500.0], [3701.4179394075504, 4500.0], time: 23.409
steps: 72761, episodes: 73, mean episode reward: 6500.0, agent episode reward: [5250.0, 1250.0], [5251.545784238856, 1250.0], time: 23.547
steps: 73761, episodes: 74, mean episode reward: 7400.0, agent episode reward: [7150.0, 250.0], [7151.601944815752, 250.0], time: 23.868
steps: 74761, episodes: 75, mean episode reward: 12400.0, agent episode reward: [3500.0, 8900.0], [3501.5191841376386, 8900.0], time: 23.959
steps: 75761, episodes: 76, mean episode reward: 7600.0, agent episode reward: [4700.0, 2900.0], [4701.195463001206, 2900.0], time: 24.498
steps: 76761, episodes: 77, mean episode reward: 7450.0, agent episode reward: [1950.0, 5500.0], [1951.3243137357076, 5500.0], time: 24.643
steps: 77761, episodes: 78, mean episode reward: 9850.0, agent episode reward: [4350.0, 5500.0], [4351.369502699086, 5500.0], time: 24.926
steps: 78629, episodes: 79, mean episode reward: 9000.0, agent episode reward: [5500.0, 3500.0], [5501.240972382345, 3500.0], time: 22.073
steps: 79629, episodes: 80, mean episode reward: 19400.0, agent episode reward: [8900.0, 10500.0], [8901.492140588993, 10500.0], time: 25.431
steps: 80629, episodes: 81, mean episode reward: 8950.0, agent episode reward: [2000.0, 6950.0], [2001.4073464493788, 6950.0], time: 26.059
steps: 81629, episodes: 82, mean episode reward: 6650.0, agent episode reward: [3250.0, 3400.0], [3251.3587730343693, 3400.0], time: 26.267
steps: 82629, episodes: 83, mean episode reward: 11850.0, agent episode reward: [9100.0, 2750.0], [9101.62632467567, 2750.0], time: 26.545
steps: 83629, episodes: 84, mean episode reward: 11400.0, agent episode reward: [10900.0, 500.0], [10901.497240790643, 500.0], time: 26.821
steps: 84629, episodes: 85, mean episode reward: 6350.0, agent episode reward: [5600.0, 750.0], [5601.4646764843965, 750.0], time: 27.481
steps: 85629, episodes: 86, mean episode reward: 14400.0, agent episode reward: [12650.0, 1750.0], [12651.356989471891, 1750.0], time: 27.54
steps: 86629, episodes: 87, mean episode reward: 9700.0, agent episode reward: [6700.0, 3000.0], [6701.500239105368, 3000.0], time: 27.943
steps: 87629, episodes: 88, mean episode reward: 8400.0, agent episode reward: [2650.0, 5750.0], [2651.457835368389, 5750.0], time: 27.932
steps: 88629, episodes: 89, mean episode reward: 3750.0, agent episode reward: [600.0, 3150.0], [601.4903874695417, 3150.0], time: 28.728
steps: 89629, episodes: 90, mean episode reward: 15150.0, agent episode reward: [5750.0, 9400.0], [5751.39970135977, 9400.0], time: 28.492
steps: 90629, episodes: 91, mean episode reward: 5300.0, agent episode reward: [2300.0, 3000.0], [2301.4393839735194, 3000.0], time: 29.278
steps: 91629, episodes: 92, mean episode reward: 22250.0, agent episode reward: [8500.0, 13750.0], [8501.434840122902, 13750.0], time: 29.135
steps: 92629, episodes: 93, mean episode reward: 12400.0, agent episode reward: [3250.0, 9150.0], [3251.4245449713817, 9150.0], time: 29.733
steps: 93629, episodes: 94, mean episode reward: 8200.0, agent episode reward: [4250.0, 3950.0], [4251.517735373135, 3950.0], time: 30.407
steps: 94629, episodes: 95, mean episode reward: 7150.0, agent episode reward: [5800.0, 1350.0], [5801.448359863707, 1350.0], time: 30.832
steps: 95629, episodes: 96, mean episode reward: 9250.0, agent episode reward: [6300.0, 2950.0], [6301.48390055995, 2950.0], time: 30.982
steps: 96629, episodes: 97, mean episode reward: 8650.0, agent episode reward: [8150.0, 500.0], [8151.586680957826, 500.0], time: 31.419
steps: 97629, episodes: 98, mean episode reward: 10600.0, agent episode reward: [5400.0, 5200.0], [5401.377050777983, 5200.0], time: 31.555
steps: 98629, episodes: 99, mean episode reward: 10500.0, agent episode reward: [4500.0, 6000.0], [4501.447863064498, 6000.0], time: 31.555
steps: 99629, episodes: 100, mean episode reward: 10150.0, agent episode reward: [5900.0, 4250.0], [5901.571818301667, 4250.0], time: 31.793
...Finished total of 101 episodes.
2021-10-24 07:02:08.675843: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 07:02:08.679908: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 07:02:08.680232: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x563b95969db0 executing computations on platform Host. Devices:
2021-10-24 07:02:08.680253: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 11550.0, agent episode reward: [11050.0, 500.0], [11051.551113468735, 500.0], time: 5.73
steps: 2000, episodes: 2, mean episode reward: 9650.0, agent episode reward: [2750.0, 6900.0], [2751.3976187043168, 6900.0], time: 8.296
steps: 3000, episodes: 3, mean episode reward: 8150.0, agent episode reward: [4700.0, 3450.0], [4701.464363342336, 3450.0], time: 8.609
steps: 4000, episodes: 4, mean episode reward: 16950.0, agent episode reward: [15700.0, 1250.0], [15701.41047962947, 1250.0], time: 8.639
steps: 5000, episodes: 5, mean episode reward: 3600.0, agent episode reward: [1000.0, 2600.0], [1001.4630518531103, 2600.0], time: 9.332
steps: 6000, episodes: 6, mean episode reward: 5200.0, agent episode reward: [1000.0, 4200.0], [1001.3112775460552, 4200.0], time: 9.486
steps: 7000, episodes: 7, mean episode reward: 14100.0, agent episode reward: [4950.0, 9150.0], [4951.468538850261, 9150.0], time: 9.626
steps: 8000, episodes: 8, mean episode reward: 9350.0, agent episode reward: [6650.0, 2700.0], [6651.327411153501, 2700.0], time: 9.875
steps: 8993, episodes: 9, mean episode reward: 5850.0, agent episode reward: [1250.0, 4600.0], [1251.1746771754765, 4600.0], time: 10.445
steps: 9993, episodes: 10, mean episode reward: 11800.0, agent episode reward: [10550.0, 1250.0], [10551.473949353895, 1250.0], time: 10.453
steps: 10993, episodes: 11, mean episode reward: 9150.0, agent episode reward: [2250.0, 6900.0], [2251.4069716730287, 6900.0], time: 10.777
steps: 11993, episodes: 12, mean episode reward: 12650.0, agent episode reward: [5700.0, 6950.0], [5701.479005336909, 6950.0], time: 10.72
steps: 12993, episodes: 13, mean episode reward: 5500.0, agent episode reward: [3000.0, 2500.0], [3001.4182395886896, 2500.0], time: 11.227
steps: 13993, episodes: 14, mean episode reward: 8550.0, agent episode reward: [2800.0, 5750.0], [2801.573779573597, 5750.0], time: 11.012
steps: 14993, episodes: 15, mean episode reward: 5550.0, agent episode reward: [4050.0, 1500.0], [4051.3466144357203, 1500.0], time: 11.156
steps: 15993, episodes: 16, mean episode reward: 3800.0, agent episode reward: [3600.0, 200.0], [3601.468766076413, 200.0], time: 11.29
steps: 16993, episodes: 17, mean episode reward: 3150.0, agent episode reward: [2650.0, 500.0], [2651.36034406552, 500.0], time: 11.771
steps: 17993, episodes: 18, mean episode reward: 14900.0, agent episode reward: [2250.0, 12650.0], [2251.242346799247, 12650.0], time: 11.422
steps: 18993, episodes: 19, mean episode reward: 6200.0, agent episode reward: [1000.0, 5200.0], [1001.3393221999127, 5200.0], time: 11.555
steps: 19993, episodes: 20, mean episode reward: 21200.0, agent episode reward: [11150.0, 10050.0], [11151.820003108412, 10050.0], time: 12.206
steps: 20993, episodes: 21, mean episode reward: 11050.0, agent episode reward: [7400.0, 3650.0], [7401.433130777089, 3650.0], time: 12.52
steps: 21968, episodes: 22, mean episode reward: 16100.0, agent episode reward: [7850.0, 8250.0], [7851.605404232378, 8250.0], time: 11.87
steps: 22968, episodes: 23, mean episode reward: 7650.0, agent episode reward: [500.0, 7150.0], [501.27621613585234, 7150.0], time: 12.493
steps: 23968, episodes: 24, mean episode reward: 14450.0, agent episode reward: [13700.0, 750.0], [13701.639046984756, 750.0], time: 12.274
steps: 24968, episodes: 25, mean episode reward: 14800.0, agent episode reward: [10850.0, 3950.0], [10851.782384486522, 3950.0], time: 12.784
steps: 25968, episodes: 26, mean episode reward: 9600.0, agent episode reward: [250.0, 9350.0], [251.35770051867007, 9350.0], time: 12.718
StopIteration()
steps: 26968, episodes: 27, mean episode reward: 9700.0, agent episode reward: [6450.0, 3250.0], [6451.437355683628, 3250.0], time: 12.767
steps: 27968, episodes: 28, mean episode reward: 9250.0, agent episode reward: [6250.0, 3000.0], [6251.464835693575, 3000.0], time: 12.899
steps: 28856, episodes: 29, mean episode reward: 5800.0, agent episode reward: [2250.0, 3550.0], [2251.1638847667327, 3550.0], time: 12.04
steps: 29856, episodes: 30, mean episode reward: 9100.0, agent episode reward: [4200.0, 4900.0], [4201.522067503142, 4900.0], time: 13.096
steps: 30683, episodes: 31, mean episode reward: 7300.0, agent episode reward: [4800.0, 2500.0], [4801.16744903402, 2500.0], time: 11.527
steps: 31683, episodes: 32, mean episode reward: 12300.0, agent episode reward: [5400.0, 6900.0], [5401.498306124491, 6900.0], time: 13.609
steps: 32683, episodes: 33, mean episode reward: 8250.0, agent episode reward: [900.0, 7350.0], [901.4789216947852, 7350.0], time: 13.608
steps: 33683, episodes: 34, mean episode reward: 14150.0, agent episode reward: [5250.0, 8900.0], [5251.45410412878, 8900.0], time: 14.163
steps: 34683, episodes: 35, mean episode reward: 9200.0, agent episode reward: [3750.0, 5450.0], [3751.4516135109316, 5450.0], time: 13.87
steps: 35683, episodes: 36, mean episode reward: 9550.0, agent episode reward: [3150.0, 6400.0], [3151.586073116109, 6400.0], time: 14.18
steps: 36683, episodes: 37, mean episode reward: 12350.0, agent episode reward: [9600.0, 2750.0], [9601.635527072538, 2750.0], time: 14.823
steps: 37683, episodes: 38, mean episode reward: 7950.0, agent episode reward: [4950.0, 3000.0], [4951.487003911534, 3000.0], time: 14.361
steps: 38683, episodes: 39, mean episode reward: 8850.0, agent episode reward: [2250.0, 6600.0], [2251.828794015419, 6600.0], time: 14.546
steps: 39683, episodes: 40, mean episode reward: 10650.0, agent episode reward: [4200.0, 6450.0], [4201.6763482334045, 6450.0], time: 14.724
steps: 40683, episodes: 41, mean episode reward: 5250.0, agent episode reward: [1000.0, 4250.0], [1001.3051088319355, 4250.0], time: 12.755
steps: 41683, episodes: 42, mean episode reward: 7950.0, agent episode reward: [2800.0, 5150.0], [2801.578513479049, 5150.0], time: 15.765
steps: 42683, episodes: 43, mean episode reward: 10100.0, agent episode reward: [8600.0, 1500.0], [8601.789665720089, 1500.0], time: 15.475
steps: 43683, episodes: 44, mean episode reward: 14400.0, agent episode reward: [7700.0, 6700.0], [7701.667610332311, 6700.0], time: 15.512
steps: 44683, episodes: 45, mean episode reward: 21100.0, agent episode reward: [17100.0, 4000.0], [17101.64784424166, 4000.0], time: 15.893
steps: 45683, episodes: 46, mean episode reward: 11100.0, agent episode reward: [5650.0, 5450.0], [5651.685154514467, 5450.0], time: 16.492
steps: 46683, episodes: 47, mean episode reward: 8300.0, agent episode reward: [5950.0, 2350.0], [5951.5489843281475, 2350.0], time: 16.645
steps: 47683, episodes: 48, mean episode reward: 14600.0, agent episode reward: [10850.0, 3750.0], [10851.700826453229, 3750.0], time: 16.791
steps: 48683, episodes: 49, mean episode reward: 5950.0, agent episode reward: [2250.0, 3700.0], [2251.369002903328, 3700.0], time: 16.857
steps: 49683, episodes: 50, mean episode reward: 13150.0, agent episode reward: [7000.0, 6150.0], [7001.549803473795, 6150.0], time: 17.024
steps: 50683, episodes: 51, mean episode reward: 12850.0, agent episode reward: [7350.0, 5500.0], [7351.561780262449, 5500.0], time: 17.693
steps: 51683, episodes: 52, mean episode reward: 3100.0, agent episode reward: [950.0, 2150.0], [951.5292206384308, 2150.0], time: 17.426
steps: 52683, episodes: 53, mean episode reward: 10400.0, agent episode reward: [250.0, 10150.0], [251.35760948782888, 10150.0], time: 17.652
steps: 53683, episodes: 54, mean episode reward: 14900.0, agent episode reward: [9450.0, 5450.0], [9451.665759088113, 5450.0], time: 18.372
steps: 54683, episodes: 55, mean episode reward: 5500.0, agent episode reward: [2650.0, 2850.0], [2651.7440696586737, 2850.0], time: 18.251
steps: 55683, episodes: 56, mean episode reward: 9700.0, agent episode reward: [6200.0, 3500.0], [6201.813268471494, 3500.0], time: 18.777
steps: 56523, episodes: 57, mean episode reward: 5050.0, agent episode reward: [4800.0, 250.0], [4801.236437885116, 250.0], time: 15.779
steps: 57475, episodes: 58, mean episode reward: 10050.0, agent episode reward: [7300.0, 2750.0], [7301.558013825485, 2750.0], time: 18.24
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 58475, episodes: 59, mean episode reward: 7200.0, agent episode reward: [3000.0, 4200.0], [3001.627983596216, 4200.0], time: 19.845
steps: 59475, episodes: 60, mean episode reward: 9350.0, agent episode reward: [6350.0, 3000.0], [6351.530517126296, 3000.0], time: 19.792
steps: 60475, episodes: 61, mean episode reward: 14050.0, agent episode reward: [12050.0, 2000.0], [12051.872990058368, 2000.0], time: 19.888
steps: 61475, episodes: 62, mean episode reward: 7450.0, agent episode reward: [5950.0, 1500.0], [5951.731015060805, 1500.0], time: 20.177
steps: 62475, episodes: 63, mean episode reward: 16800.0, agent episode reward: [6500.0, 10300.0], [6501.650815504892, 10300.0], time: 20.535
steps: 63186, episodes: 64, mean episode reward: 4550.0, agent episode reward: [2300.0, 2250.0], [2301.037078337411, 2250.0], time: 15.166
steps: 64186, episodes: 65, mean episode reward: 8250.0, agent episode reward: [6000.0, 2250.0], [6001.5345857282955, 2250.0], time: 21.048
steps: 65186, episodes: 66, mean episode reward: 10050.0, agent episode reward: [750.0, 9300.0], [751.5772306248566, 9300.0], time: 21.077
steps: 66186, episodes: 67, mean episode reward: 7250.0, agent episode reward: [4750.0, 2500.0], [4751.610462263948, 2500.0], time: 21.968
steps: 67159, episodes: 68, mean episode reward: 10900.0, agent episode reward: [8650.0, 2250.0], [8651.544917907855, 2250.0], time: 21.537
steps: 68159, episodes: 69, mean episode reward: 9250.0, agent episode reward: [8250.0, 1000.0], [8251.711997278235, 1000.0], time: 21.997
steps: 69159, episodes: 70, mean episode reward: 10750.0, agent episode reward: [1250.0, 9500.0], [1251.6159318670302, 9500.0], time: 22.541
steps: 70159, episodes: 71, mean episode reward: 5950.0, agent episode reward: [4100.0, 1850.0], [4101.863398603659, 1850.0], time: 22.711
steps: 71159, episodes: 72, mean episode reward: 8100.0, agent episode reward: [3900.0, 4200.0], [3901.9900762349716, 4200.0], time: 22.749
steps: 72159, episodes: 73, mean episode reward: 11950.0, agent episode reward: [6950.0, 5000.0], [6951.957120119053, 5000.0], time: 23.498
steps: 73159, episodes: 74, mean episode reward: 7000.0, agent episode reward: [1000.0, 6000.0], [1001.6354518533245, 6000.0], time: 23.811
steps: 74159, episodes: 75, mean episode reward: 16150.0, agent episode reward: [10200.0, 5950.0], [10202.30503675703, 5950.0], time: 23.825
steps: 75091, episodes: 76, mean episode reward: 10550.0, agent episode reward: [5500.0, 5050.0], [5501.816889559327, 5050.0], time: 22.236
steps: 76091, episodes: 77, mean episode reward: 8250.0, agent episode reward: [3350.0, 4900.0], [3352.1712808732364, 4900.0], time: 24.687
steps: 77091, episodes: 78, mean episode reward: 10500.0, agent episode reward: [8050.0, 2450.0], [8052.160901923046, 2450.0], time: 25.154
steps: 78091, episodes: 79, mean episode reward: 9300.0, agent episode reward: [4200.0, 5100.0], [4202.038476132101, 5100.0], time: 24.894
steps: 79091, episodes: 80, mean episode reward: 11150.0, agent episode reward: [6250.0, 4900.0], [6251.911994239432, 4900.0], time: 25.432
steps: 80091, episodes: 81, mean episode reward: 6450.0, agent episode reward: [2950.0, 3500.0], [2951.6515104308764, 3500.0], time: 25.82
steps: 81091, episodes: 82, mean episode reward: 12300.0, agent episode reward: [4750.0, 7550.0], [4751.877773060635, 7550.0], time: 25.844
steps: 82091, episodes: 83, mean episode reward: 11300.0, agent episode reward: [4500.0, 6800.0], [4501.828833715687, 6800.0], time: 26.145
steps: 83091, episodes: 84, mean episode reward: 12100.0, agent episode reward: [5500.0, 6600.0], [5501.730974376743, 6600.0], time: 26.89
steps: 84091, episodes: 85, mean episode reward: 16850.0, agent episode reward: [8450.0, 8400.0], [8452.02472397897, 8400.0], time: 27.026
steps: 85091, episodes: 86, mean episode reward: 8350.0, agent episode reward: [5450.0, 2900.0], [5451.968735758534, 2900.0], time: 27.093
steps: 86091, episodes: 87, mean episode reward: 4800.0, agent episode reward: [1650.0, 3150.0], [1651.8719824566138, 3150.0], time: 27.749
steps: 87091, episodes: 88, mean episode reward: 9550.0, agent episode reward: [7050.0, 2500.0], [7051.850677755051, 2500.0], time: 28.244
steps: 88091, episodes: 89, mean episode reward: 7650.0, agent episode reward: [2900.0, 4750.0], [2901.9758696588597, 4750.0], time: 28.408
steps: 89091, episodes: 90, mean episode reward: 8300.0, agent episode reward: [5650.0, 2650.0], [5652.207602754996, 2650.0], time: 28.599
steps: 90091, episodes: 91, mean episode reward: 5550.0, agent episode reward: [5450.0, 100.0], [5451.711401838204, 100.0], time: 28.708
steps: 91091, episodes: 92, mean episode reward: 9300.0, agent episode reward: [250.0, 9050.0], [251.75852570762135, 9050.0], time: 29.023
StopIteration()
steps: 92091, episodes: 93, mean episode reward: 11350.0, agent episode reward: [4550.0, 6800.0], [4551.845123430379, 6800.0], time: 29.716
steps: 93091, episodes: 94, mean episode reward: 11100.0, agent episode reward: [4000.0, 7100.0], [4001.9320701981687, 7100.0], time: 29.97
steps: 94091, episodes: 95, mean episode reward: 1400.0, agent episode reward: [250.0, 1150.0], [251.42803736032153, 1150.0], time: 30.393
steps: 95091, episodes: 96, mean episode reward: 7150.0, agent episode reward: [2250.0, 4900.0], [2251.710433993686, 4900.0], time: 30.66
steps: 96091, episodes: 97, mean episode reward: 10850.0, agent episode reward: [6600.0, 4250.0], [6602.130340023887, 4250.0], time: 31.011
steps: 97091, episodes: 98, mean episode reward: 14450.0, agent episode reward: [5750.0, 8700.0], [5752.02608733601, 8700.0], time: 31.465
steps: 98091, episodes: 99, mean episode reward: 6650.0, agent episode reward: [4900.0, 1750.0], [4901.670249488119, 1750.0], time: 31.875
StopIteration()
steps: 99091, episodes: 100, mean episode reward: 15200.0, agent episode reward: [9100.0, 6100.0], [9101.94957602243, 6100.0], time: 31.682
...Finished total of 101 episodes.
2021-10-24 07:32:42.952130: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 07:32:42.956595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 07:32:42.957336: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x555b4f061770 executing computations on platform Host. Devices:
2021-10-24 07:32:42.957358: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 5200.0, agent episode reward: [1000.0, 4200.0], [1001.2682815113578, 4200.0], time: 5.878
steps: 2000, episodes: 2, mean episode reward: 5250.0, agent episode reward: [1000.0, 4250.0], [1001.7459377684033, 4250.0], time: 8.514
steps: 3000, episodes: 3, mean episode reward: 7000.0, agent episode reward: [3800.0, 3200.0], [3801.966519563891, 3200.0], time: 8.522
steps: 4000, episodes: 4, mean episode reward: 10550.0, agent episode reward: [2950.0, 7600.0], [2951.650078555305, 7600.0], time: 9.138
steps: 4808, episodes: 5, mean episode reward: 8800.0, agent episode reward: [4750.0, 4050.0], [4751.153587768901, 4050.0], time: 6.954
steps: 5808, episodes: 6, mean episode reward: 17100.0, agent episode reward: [12450.0, 4650.0], [12451.542681952147, 4650.0], time: 10.491
steps: 6749, episodes: 7, mean episode reward: 3050.0, agent episode reward: [800.0, 2250.0], [801.3592249504949, 2250.0], time: 9.035
steps: 7749, episodes: 8, mean episode reward: 9750.0, agent episode reward: [4650.0, 5100.0], [4651.585937908351, 5100.0], time: 9.906
steps: 8749, episodes: 9, mean episode reward: 7650.0, agent episode reward: [2750.0, 4900.0], [2751.598619036798, 4900.0], time: 10.026
steps: 9749, episodes: 10, mean episode reward: 10350.0, agent episode reward: [2950.0, 7400.0], [2951.4310712289002, 7400.0], time: 10.998
steps: 10597, episodes: 11, mean episode reward: 4600.0, agent episode reward: [2250.0, 2350.0], [2251.1476854668363, 2350.0], time: 8.953
steps: 11597, episodes: 12, mean episode reward: 7300.0, agent episode reward: [3650.0, 3650.0], [3651.4407268309633, 3650.0], time: 10.747
steps: 12551, episodes: 13, mean episode reward: 10550.0, agent episode reward: [5800.0, 4750.0], [5801.445543294398, 4750.0], time: 10.39
steps: 13551, episodes: 14, mean episode reward: 6650.0, agent episode reward: [1750.0, 4900.0], [1751.370222151706, 4900.0], time: 11.653
StopIteration()
steps: 14551, episodes: 15, mean episode reward: 5350.0, agent episode reward: [1750.0, 3600.0], [1751.590948293599, 3600.0], time: 11.084
steps: 15551, episodes: 16, mean episode reward: 5200.0, agent episode reward: [4700.0, 500.0], [4701.614175303746, 500.0], time: 11.232
steps: 16551, episodes: 17, mean episode reward: 4400.0, agent episode reward: [3450.0, 950.0], [3451.4264730948926, 950.0], time: 11.331
steps: 17464, episodes: 18, mean episode reward: 7600.0, agent episode reward: [4250.0, 3350.0], [4251.034456161214, 3350.0], time: 10.97
steps: 18464, episodes: 19, mean episode reward: 6900.0, agent episode reward: [2900.0, 4000.0], [2901.541785205184, 4000.0], time: 11.955
steps: 19464, episodes: 20, mean episode reward: 6700.0, agent episode reward: [5750.0, 950.0], [5751.555630292593, 950.0], time: 11.886
steps: 20464, episodes: 21, mean episode reward: 11500.0, agent episode reward: [2250.0, 9250.0], [2251.357279758959, 9250.0], time: 11.949
steps: 21103, episodes: 22, mean episode reward: 2550.0, agent episode reward: [250.0, 2300.0], [250.8059763942586, 2300.0], time: 8.449
steps: 22103, episodes: 23, mean episode reward: 13400.0, agent episode reward: [6250.0, 7150.0], [6251.338250385194, 7150.0], time: 12.2
steps: 23103, episodes: 24, mean episode reward: 5700.0, agent episode reward: [5000.0, 700.0], [5001.317249586393, 700.0], time: 12.315
steps: 24103, episodes: 25, mean episode reward: 5700.0, agent episode reward: [5500.0, 200.0], [5501.442998210124, 200.0], time: 12.282
steps: 25103, episodes: 26, mean episode reward: 6900.0, agent episode reward: [2750.0, 4150.0], [2751.5540982627026, 4150.0], time: 13.012
steps: 25960, episodes: 27, mean episode reward: 9050.0, agent episode reward: [2750.0, 6300.0], [2751.0283792275904, 6300.0], time: 10.809
steps: 26960, episodes: 28, mean episode reward: 5300.0, agent episode reward: [1000.0, 4300.0], [1001.20828307939, 4300.0], time: 12.81
steps: 27960, episodes: 29, mean episode reward: 9300.0, agent episode reward: [6800.0, 2500.0], [6801.3277020045825, 2500.0], time: 13.048
steps: 28960, episodes: 30, mean episode reward: 8850.0, agent episode reward: [1450.0, 7400.0], [1451.4760129444055, 7400.0], time: 13.621
steps: 29960, episodes: 31, mean episode reward: 8850.0, agent episode reward: [750.0, 8100.0], [751.3967953380202, 8100.0], time: 13.387
steps: 30960, episodes: 32, mean episode reward: 11700.0, agent episode reward: [6250.0, 5450.0], [6251.450329094026, 5450.0], time: 13.57
steps: 31960, episodes: 33, mean episode reward: 13350.0, agent episode reward: [1500.0, 11850.0], [1501.5748516222202, 11850.0], time: 13.956
steps: 32829, episodes: 34, mean episode reward: 2250.0, agent episode reward: [1500.0, 750.0], [1501.3463575354813, 750.0], time: 11.904
steps: 33829, episodes: 35, mean episode reward: 11050.0, agent episode reward: [5400.0, 5650.0], [5401.5007560857475, 5650.0], time: 13.832
steps: 34829, episodes: 36, mean episode reward: 6850.0, agent episode reward: [250.0, 6600.0], [251.21837317311875, 6600.0], time: 13.905
steps: 35829, episodes: 37, mean episode reward: 11200.0, agent episode reward: [3250.0, 7950.0], [3251.3792809728043, 7950.0], time: 14.623
steps: 36829, episodes: 38, mean episode reward: 5000.0, agent episode reward: [1500.0, 3500.0], [1501.394318548663, 3500.0], time: 14.345
steps: 37829, episodes: 39, mean episode reward: 16850.0, agent episode reward: [11100.0, 5750.0], [11101.606868807929, 5750.0], time: 14.371
steps: 38829, episodes: 40, mean episode reward: 11900.0, agent episode reward: [7650.0, 4250.0], [7651.53618379029, 4250.0], time: 14.748
steps: 39829, episodes: 41, mean episode reward: 13400.0, agent episode reward: [5450.0, 7950.0], [5451.578004714405, 7950.0], time: 15.452
steps: 40829, episodes: 42, mean episode reward: 11150.0, agent episode reward: [10900.0, 250.0], [10901.432986376552, 250.0], time: 15.027
steps: 41829, episodes: 43, mean episode reward: 11350.0, agent episode reward: [2500.0, 8850.0], [2501.4427943560527, 8850.0], time: 15.285
steps: 42829, episodes: 44, mean episode reward: 11350.0, agent episode reward: [6850.0, 4500.0], [6851.35406569109, 4500.0], time: 15.452
steps: 43829, episodes: 45, mean episode reward: 5650.0, agent episode reward: [1000.0, 4650.0], [1001.5231317556628, 4650.0], time: 16.12
steps: 44829, episodes: 46, mean episode reward: 5300.0, agent episode reward: [250.0, 5050.0], [251.20692138868802, 5050.0], time: 16.254
steps: 45829, episodes: 47, mean episode reward: 15350.0, agent episode reward: [5250.0, 10100.0], [5251.56492720313, 10100.0], time: 16.107
steps: 46829, episodes: 48, mean episode reward: 23450.0, agent episode reward: [8250.0, 15200.0], [8251.419355649832, 15200.0], time: 16.18
steps: 47829, episodes: 49, mean episode reward: 12750.0, agent episode reward: [5000.0, 7750.0], [5001.434190408183, 7750.0], time: 16.541
steps: 48829, episodes: 50, mean episode reward: 9200.0, agent episode reward: [4250.0, 4950.0], [4251.361576827483, 4950.0], time: 17.326
steps: 49829, episodes: 51, mean episode reward: 14550.0, agent episode reward: [11550.0, 3000.0], [11551.68318309782, 3000.0], time: 17.095
steps: 50829, episodes: 52, mean episode reward: 15500.0, agent episode reward: [1500.0, 14000.0], [1501.4956819628928, 14000.0], time: 17.039
steps: 51829, episodes: 53, mean episode reward: 9900.0, agent episode reward: [1500.0, 8400.0], [1501.3899600691268, 8400.0], time: 17.783
steps: 52829, episodes: 54, mean episode reward: 14450.0, agent episode reward: [12200.0, 2250.0], [12201.65295446766, 2250.0], time: 18.053
steps: 53829, episodes: 55, mean episode reward: 12100.0, agent episode reward: [750.0, 11350.0], [751.2118590894502, 11350.0], time: 18.221
steps: 54829, episodes: 56, mean episode reward: 12450.0, agent episode reward: [7000.0, 5450.0], [7001.546787776068, 5450.0], time: 18.234
steps: 55813, episodes: 57, mean episode reward: 5850.0, agent episode reward: [1500.0, 4350.0], [1501.291823679902, 4350.0], time: 18.558
steps: 56813, episodes: 58, mean episode reward: 8350.0, agent episode reward: [3750.0, 4600.0], [3751.3633083633913, 4600.0], time: 18.563
steps: 57813, episodes: 59, mean episode reward: 11600.0, agent episode reward: [4900.0, 6700.0], [4901.53460094028, 6700.0], time: 19.106/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 58813, episodes: 60, mean episode reward: 11600.0, agent episode reward: [1500.0, 10100.0], [1501.5873179386294, 10100.0], time: 19.202
steps: 59813, episodes: 61, mean episode reward: 7850.0, agent episode reward: [2500.0, 5350.0], [2501.4368881479086, 5350.0], time: 19.939
steps: 60813, episodes: 62, mean episode reward: 12150.0, agent episode reward: [5500.0, 6650.0], [5501.424381085726, 6650.0], time: 20.166
steps: 61630, episodes: 63, mean episode reward: 9100.0, agent episode reward: [2000.0, 7100.0], [2001.1375932890714, 7100.0], time: 16.42
steps: 62630, episodes: 64, mean episode reward: 7050.0, agent episode reward: [250.0, 6800.0], [251.37121190136372, 6800.0], time: 18.261
steps: 63630, episodes: 65, mean episode reward: 10250.0, agent episode reward: [4100.0, 6150.0], [4101.704054653044, 6150.0], time: 21.029
steps: 64630, episodes: 66, mean episode reward: 8150.0, agent episode reward: [7900.0, 250.0], [7901.464287993004, 250.0], time: 20.919
steps: 65630, episodes: 67, mean episode reward: 11000.0, agent episode reward: [2750.0, 8250.0], [2751.609843373607, 8250.0], time: 21.535
StopIteration()
steps: 66630, episodes: 68, mean episode reward: 10850.0, agent episode reward: [7800.0, 3050.0], [7801.452661702872, 3050.0], time: 21.464
steps: 67630, episodes: 69, mean episode reward: 8450.0, agent episode reward: [4350.0, 4100.0], [4351.651390760116, 4100.0], time: 22.208
steps: 68630, episodes: 70, mean episode reward: 9900.0, agent episode reward: [3000.0, 6900.0], [3001.5212565845154, 6900.0], time: 22.062
steps: 69630, episodes: 71, mean episode reward: 3200.0, agent episode reward: [250.0, 2950.0], [251.30715645666623, 2950.0], time: 22.661
steps: 70630, episodes: 72, mean episode reward: 5600.0, agent episode reward: [1500.0, 4100.0], [1501.4904051796466, 4100.0], time: 23.258
steps: 71584, episodes: 73, mean episode reward: 6050.0, agent episode reward: [2500.0, 3550.0], [2501.4752816707182, 3550.0], time: 22.218
steps: 72531, episodes: 74, mean episode reward: 5300.0, agent episode reward: [4800.0, 500.0], [4801.339903128873, 500.0], time: 22.761
steps: 73517, episodes: 75, mean episode reward: 6500.0, agent episode reward: [3000.0, 3500.0], [3001.463272410201, 3500.0], time: 23.372
steps: 74517, episodes: 76, mean episode reward: 7700.0, agent episode reward: [4250.0, 3450.0], [4251.432106319342, 3450.0], time: 23.924
steps: 75517, episodes: 77, mean episode reward: 7900.0, agent episode reward: [4550.0, 3350.0], [4551.855536880508, 3350.0], time: 24.553
steps: 76510, episodes: 78, mean episode reward: 6950.0, agent episode reward: [500.0, 6450.0], [501.24998238474774, 6450.0], time: 24.646
steps: 77510, episodes: 79, mean episode reward: 13200.0, agent episode reward: [2250.0, 10950.0], [2251.701512940873, 10950.0], time: 25.052
steps: 78510, episodes: 80, mean episode reward: 11350.0, agent episode reward: [6150.0, 5200.0], [6151.598728621715, 5200.0], time: 25.469
steps: 79510, episodes: 81, mean episode reward: 8150.0, agent episode reward: [7400.0, 750.0], [7401.563965162156, 750.0], time: 25.444
steps: 80322, episodes: 82, mean episode reward: 10550.0, agent episode reward: [4300.0, 6250.0], [4301.246704790021, 6250.0], time: 20.655
steps: 81322, episodes: 83, mean episode reward: 5000.0, agent episode reward: [250.0, 4750.0], [251.2284428399815, 4750.0], time: 26.603
steps: 82322, episodes: 84, mean episode reward: 12400.0, agent episode reward: [1750.0, 10650.0], [1751.4581008525201, 10650.0], time: 26.256
steps: 83322, episodes: 85, mean episode reward: 6750.0, agent episode reward: [2250.0, 4500.0], [2251.4241137982604, 4500.0], time: 26.802
steps: 84322, episodes: 86, mean episode reward: 10950.0, agent episode reward: [8700.0, 2250.0], [8701.528977834923, 2250.0], time: 27.301
steps: 85322, episodes: 87, mean episode reward: 8450.0, agent episode reward: [3750.0, 4700.0], [3751.5548063021597, 4700.0], time: 27.693
steps: 86322, episodes: 88, mean episode reward: 13100.0, agent episode reward: [6450.0, 6650.0], [6451.5995529001875, 6650.0], time: 27.615
steps: 87170, episodes: 89, mean episode reward: 7000.0, agent episode reward: [4250.0, 2750.0], [4251.309402512844, 2750.0], time: 23.815
steps: 88170, episodes: 90, mean episode reward: 11950.0, agent episode reward: [6200.0, 5750.0], [6201.510182036997, 5750.0], time: 28.295
steps: 89170, episodes: 91, mean episode reward: 6450.0, agent episode reward: [2950.0, 3500.0], [2951.3721710378845, 3500.0], time: 28.714
steps: 90170, episodes: 92, mean episode reward: 9200.0, agent episode reward: [4950.0, 4250.0], [4951.521076535212, 4250.0], time: 29.156
steps: 91170, episodes: 93, mean episode reward: 4250.0, agent episode reward: [4100.0, 150.0], [4101.515950386679, 150.0], time: 29.454
steps: 92170, episodes: 94, mean episode reward: 12150.0, agent episode reward: [7450.0, 4700.0], [7451.791973438302, 4700.0], time: 29.403
steps: 93170, episodes: 95, mean episode reward: 4850.0, agent episode reward: [2750.0, 2100.0], [2751.451612714522, 2100.0], time: 30.147
steps: 94170, episodes: 96, mean episode reward: 13800.0, agent episode reward: [9850.0, 3950.0], [9851.531617352846, 3950.0], time: 30.454
steps: 95170, episodes: 97, mean episode reward: 9450.0, agent episode reward: [3500.0, 5950.0], [3501.5785551026315, 5950.0], time: 30.742
steps: 96170, episodes: 98, mean episode reward: 9200.0, agent episode reward: [1450.0, 7750.0], [1451.4237472861082, 7750.0], time: 31.003
steps: 97170, episodes: 99, mean episode reward: 5800.0, agent episode reward: [3900.0, 1900.0], [3901.5550223874966, 1900.0], time: 31.327
steps: 98170, episodes: 100, mean episode reward: 9800.0, agent episode reward: [3450.0, 6350.0], [3451.498132697764, 6350.0], time: 31.939
...Finished total of 101 episodes.
2021-10-24 08:02:51.632746: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 08:02:51.636911: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 08:02:51.637108: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c37b1da480 executing computations on platform Host. Devices:
2021-10-24 08:02:51.637124: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 8600.0, agent episode reward: [4600.0, 4000.0], [4601.417718133832, 4000.0], time: 5.751
steps: 2000, episodes: 2, mean episode reward: 3000.0, agent episode reward: [150.0, 2850.0], [151.96770616725135, 2850.0], time: 8.205
steps: 3000, episodes: 3, mean episode reward: 6950.0, agent episode reward: [1000.0, 5950.0], [1001.326862563326, 5950.0], time: 8.533
steps: 4000, episodes: 4, mean episode reward: 6300.0, agent episode reward: [3450.0, 2850.0], [3451.400681206704, 2850.0], time: 8.782
steps: 5000, episodes: 5, mean episode reward: 8600.0, agent episode reward: [3900.0, 4700.0], [3901.4399470017506, 4700.0], time: 9.695
steps: 6000, episodes: 6, mean episode reward: 5650.0, agent episode reward: [4900.0, 750.0], [4901.358215859577, 750.0], time: 9.834
steps: 7000, episodes: 7, mean episode reward: 4350.0, agent episode reward: [1350.0, 3000.0], [1351.8278967545602, 3000.0], time: 9.819
steps: 8000, episodes: 8, mean episode reward: 10750.0, agent episode reward: [9750.0, 1000.0], [9751.858771375877, 1000.0], time: 10.442
steps: 9000, episodes: 9, mean episode reward: 11150.0, agent episode reward: [2500.0, 8650.0], [2501.3222304064248, 8650.0], time: 10.032
steps: 10000, episodes: 10, mean episode reward: 10100.0, agent episode reward: [5850.0, 4250.0], [5851.620713378162, 4250.0], time: 10.555
steps: 11000, episodes: 11, mean episode reward: 9300.0, agent episode reward: [250.0, 9050.0], [251.1706904869662, 9050.0], time: 10.652
steps: 12000, episodes: 12, mean episode reward: 16850.0, agent episode reward: [11600.0, 5250.0], [11601.502393666688, 5250.0], time: 11.258
steps: 13000, episodes: 13, mean episode reward: 9750.0, agent episode reward: [2250.0, 7500.0], [2251.3714889717944, 7500.0], time: 10.851
steps: 13858, episodes: 14, mean episode reward: 4750.0, agent episode reward: [3750.0, 1000.0], [3751.1827975119186, 1000.0], time: 9.475
steps: 14858, episodes: 15, mean episode reward: 8450.0, agent episode reward: [3000.0, 5450.0], [3001.3576180047044, 5450.0], time: 11.181
steps: 15858, episodes: 16, mean episode reward: 11050.0, agent episode reward: [7550.0, 3500.0], [7551.536933831444, 3500.0], time: 11.973
steps: 16770, episodes: 17, mean episode reward: 8300.0, agent episode reward: [5300.0, 3000.0], [5301.389567576263, 3000.0], time: 10.294
steps: 17770, episodes: 18, mean episode reward: 6600.0, agent episode reward: [1700.0, 4900.0], [1701.6887299410396, 4900.0], time: 9.152
steps: 18604, episodes: 19, mean episode reward: 13250.0, agent episode reward: [8500.0, 4750.0], [8501.255466736355, 4750.0], time: 9.624
steps: 19592, episodes: 20, mean episode reward: 17500.0, agent episode reward: [5000.0, 12500.0], [5001.398119197558, 12500.0], time: 11.775
steps: 20592, episodes: 21, mean episode reward: 9600.0, agent episode reward: [8850.0, 750.0], [8851.536091444392, 750.0], time: 12.392
steps: 21592, episodes: 22, mean episode reward: 10650.0, agent episode reward: [7650.0, 3000.0], [7651.413800339814, 3000.0], time: 12.052
steps: 22592, episodes: 23, mean episode reward: 9500.0, agent episode reward: [5150.0, 4350.0], [5151.429576159066, 4350.0], time: 12.237
steps: 23592, episodes: 24, mean episode reward: 14050.0, agent episode reward: [9100.0, 4950.0], [9101.47165134837, 4950.0], time: 12.321
steps: 24592, episodes: 25, mean episode reward: 6900.0, agent episode reward: [1700.0, 5200.0], [1701.5957129141889, 5200.0], time: 12.988
steps: 25592, episodes: 26, mean episode reward: 5650.0, agent episode reward: [500.0, 5150.0], [501.71463923998004, 5150.0], time: 12.478
StopIteration()
steps: 26592, episodes: 27, mean episode reward: 8850.0, agent episode reward: [3550.0, 5300.0], [3551.5104982221933, 5300.0], time: 12.641
steps: 27592, episodes: 28, mean episode reward: 10800.0, agent episode reward: [7800.0, 3000.0], [7801.512612709808, 3000.0], time: 13.008
steps: 28592, episodes: 29, mean episode reward: 11400.0, agent episode reward: [9900.0, 1500.0], [9901.434963155283, 1500.0], time: 13.508
steps: 29592, episodes: 30, mean episode reward: 5150.0, agent episode reward: [4900.0, 250.0], [4901.788945931743, 250.0], time: 13.11
steps: 30592, episodes: 31, mean episode reward: 7550.0, agent episode reward: [4650.0, 2900.0], [4651.400207935051, 2900.0], time: 13.262
steps: 31480, episodes: 32, mean episode reward: 14750.0, agent episode reward: [5750.0, 9000.0], [5751.186247698809, 9000.0], time: 11.972
steps: 32480, episodes: 33, mean episode reward: 8000.0, agent episode reward: [3900.0, 4100.0], [3901.5026789616613, 4100.0], time: 14.219
steps: 33480, episodes: 34, mean episode reward: 8450.0, agent episode reward: [250.0, 8200.0], [251.46798006952082, 8200.0], time: 13.73
steps: 34480, episodes: 35, mean episode reward: 5600.0, agent episode reward: [250.0, 5350.0], [251.4236033508378, 5350.0], time: 13.999
steps: 35480, episodes: 36, mean episode reward: 7650.0, agent episode reward: [4250.0, 3400.0], [4251.646195609809, 3400.0], time: 14.105
steps: 36480, episodes: 37, mean episode reward: 7100.0, agent episode reward: [1000.0, 6100.0], [1001.3503417739303, 6100.0], time: 14.944
steps: 37480, episodes: 38, mean episode reward: 14000.0, agent episode reward: [8500.0, 5500.0], [8501.719010613964, 5500.0], time: 14.376
steps: 38480, episodes: 39, mean episode reward: 3100.0, agent episode reward: [1000.0, 2100.0], [1001.4874512347088, 2100.0], time: 14.683
StopIteration()
steps: 39480, episodes: 40, mean episode reward: 12000.0, agent episode reward: [4050.0, 7950.0], [4051.438072266068, 7950.0], time: 14.801
steps: 40480, episodes: 41, mean episode reward: 9000.0, agent episode reward: [6750.0, 2250.0], [6751.578752625502, 2250.0], time: 15.461
steps: 41480, episodes: 42, mean episode reward: 10050.0, agent episode reward: [5750.0, 4300.0], [5751.480260355958, 4300.0], time: 15.152
steps: 42480, episodes: 43, mean episode reward: 5250.0, agent episode reward: [4250.0, 1000.0], [4251.873749682739, 1000.0], time: 15.332
steps: 43480, episodes: 44, mean episode reward: 13900.0, agent episode reward: [6450.0, 7450.0], [6451.372059601178, 7450.0], time: 15.681
steps: 44480, episodes: 45, mean episode reward: 8850.0, agent episode reward: [500.0, 8350.0], [501.55761986249433, 8350.0], time: 16.375
steps: 45480, episodes: 46, mean episode reward: 14350.0, agent episode reward: [9400.0, 4950.0], [9401.488102642943, 4950.0], time: 15.987
steps: 46480, episodes: 47, mean episode reward: 9050.0, agent episode reward: [7050.0, 2000.0], [7051.417434453311, 2000.0], time: 16.143
steps: 47480, episodes: 48, mean episode reward: 7650.0, agent episode reward: [3650.0, 4000.0], [3651.632728061035, 4000.0], time: 16.859
steps: 48480, episodes: 49, mean episode reward: 9350.0, agent episode reward: [3150.0, 6200.0], [3151.660392257207, 6200.0], time: 17.288
steps: 49480, episodes: 50, mean episode reward: 13200.0, agent episode reward: [4250.0, 8950.0], [4251.444608414935, 8950.0], time: 16.944
steps: 50480, episodes: 51, mean episode reward: 9600.0, agent episode reward: [4650.0, 4950.0], [4651.50676646789, 4950.0], time: 16.996
steps: 51480, episodes: 52, mean episode reward: 12450.0, agent episode reward: [750.0, 11700.0], [751.2168827299352, 11700.0], time: 17.623
steps: 52480, episodes: 53, mean episode reward: 8200.0, agent episode reward: [2200.0, 6000.0], [2201.6206195711206, 6000.0], time: 17.578
steps: 53480, episodes: 54, mean episode reward: 4350.0, agent episode reward: [3600.0, 750.0], [3601.438521735852, 750.0], time: 17.737
steps: 54480, episodes: 55, mean episode reward: 8300.0, agent episode reward: [2950.0, 5350.0], [2951.665695315173, 5350.0], time: 18.234
steps: 55480, episodes: 56, mean episode reward: 6300.0, agent episode reward: [5600.0, 700.0], [5601.744460251309, 700.0], time: 18.929
steps: 56480, episodes: 57, mean episode reward: 9800.0, agent episode reward: [5250.0, 4550.0], [5251.6601247344, 4550.0], time: 19.165
steps: 57480, episodes: 58, mean episode reward: 7100.0, agent episode reward: [2750.0, 4350.0], [2751.4941294966848, 4350.0], time: 18.975
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 58480, episodes: 59, mean episode reward: 6400.0, agent episode reward: [700.0, 5700.0], [701.7073579562808, 5700.0], time: 19.416
steps: 59480, episodes: 60, mean episode reward: 14350.0, agent episode reward: [8850.0, 5500.0], [8851.586222569274, 5500.0], time: 19.599
steps: 60480, episodes: 61, mean episode reward: 11450.0, agent episode reward: [3500.0, 7950.0], [3501.448324497683, 7950.0], time: 20.388
steps: 61480, episodes: 62, mean episode reward: 6300.0, agent episode reward: [4200.0, 2100.0], [4201.808516573571, 2100.0], time: 19.976
steps: 62480, episodes: 63, mean episode reward: 7350.0, agent episode reward: [4600.0, 2750.0], [4601.481363682197, 2750.0], time: 20.407
steps: 63480, episodes: 64, mean episode reward: 12400.0, agent episode reward: [4500.0, 7900.0], [4501.411170376309, 7900.0], time: 21.068
steps: 64480, episodes: 65, mean episode reward: 10700.0, agent episode reward: [9950.0, 750.0], [9951.517525152556, 750.0], time: 21.243
steps: 65480, episodes: 66, mean episode reward: 12850.0, agent episode reward: [7200.0, 5650.0], [7201.697052225598, 5650.0], time: 21.249
steps: 66480, episodes: 67, mean episode reward: 7500.0, agent episode reward: [5250.0, 2250.0], [5251.510700761842, 2250.0], time: 21.785
steps: 67480, episodes: 68, mean episode reward: 8250.0, agent episode reward: [6750.0, 1500.0], [6751.515122204842, 1500.0], time: 21.972
steps: 68480, episodes: 69, mean episode reward: 11150.0, agent episode reward: [8900.0, 2250.0], [8901.764576347192, 2250.0], time: 22.655
steps: 69480, episodes: 70, mean episode reward: 5600.0, agent episode reward: [2150.0, 3450.0], [2151.5283118808616, 3450.0], time: 22.763
steps: 70480, episodes: 71, mean episode reward: 14250.0, agent episode reward: [5250.0, 9000.0], [5251.350546659025, 9000.0], time: 22.637
steps: 71480, episodes: 72, mean episode reward: 10600.0, agent episode reward: [2250.0, 8350.0], [2251.6317857314148, 8350.0], time: 23.239
steps: 72361, episodes: 73, mean episode reward: 11000.0, agent episode reward: [6000.0, 5000.0], [6001.479395076818, 5000.0], time: 20.304
steps: 73361, episodes: 74, mean episode reward: 7650.0, agent episode reward: [3500.0, 4150.0], [3501.6252650872757, 4150.0], time: 23.808
steps: 74361, episodes: 75, mean episode reward: 7550.0, agent episode reward: [4350.0, 3200.0], [4351.478211994172, 3200.0], time: 23.676
steps: 75361, episodes: 76, mean episode reward: 11400.0, agent episode reward: [2750.0, 8650.0], [2751.465639944754, 8650.0], time: 24.244
steps: 76361, episodes: 77, mean episode reward: 7450.0, agent episode reward: [2100.0, 5350.0], [2101.8768175947134, 5350.0], time: 24.902
steps: 77361, episodes: 78, mean episode reward: 7400.0, agent episode reward: [4900.0, 2500.0], [4901.6980477273155, 2500.0], time: 24.632
steps: 78361, episodes: 79, mean episode reward: 7650.0, agent episode reward: [1250.0, 6400.0], [1251.5029913535775, 6400.0], time: 25.304
steps: 79361, episodes: 80, mean episode reward: 15550.0, agent episode reward: [2750.0, 12800.0], [2751.4233830779517, 12800.0], time: 25.319
steps: 80361, episodes: 81, mean episode reward: 8300.0, agent episode reward: [2900.0, 5400.0], [2901.6348598279797, 5400.0], time: 25.901
steps: 81361, episodes: 82, mean episode reward: 9350.0, agent episode reward: [5450.0, 3900.0], [5451.736536294807, 3900.0], time: 26.128
steps: 82361, episodes: 83, mean episode reward: 10750.0, agent episode reward: [10600.0, 150.0], [10601.564841337504, 150.0], time: 26.775
steps: 83361, episodes: 84, mean episode reward: 14550.0, agent episode reward: [8250.0, 6300.0], [8251.614485925986, 6300.0], time: 26.649
steps: 84361, episodes: 85, mean episode reward: 10750.0, agent episode reward: [5400.0, 5350.0], [5401.745433275978, 5350.0], time: 26.902
steps: 85361, episodes: 86, mean episode reward: 11900.0, agent episode reward: [10150.0, 1750.0], [10151.67897036129, 1750.0], time: 27.774
steps: 86361, episodes: 87, mean episode reward: 9150.0, agent episode reward: [4650.0, 4500.0], [4652.021870047729, 4500.0], time: 27.678
steps: 87361, episodes: 88, mean episode reward: 12350.0, agent episode reward: [9600.0, 2750.0], [9601.594291191037, 2750.0], time: 28.004
steps: 88361, episodes: 89, mean episode reward: 5200.0, agent episode reward: [4200.0, 1000.0], [4202.109744655376, 1000.0], time: 28.307
steps: 89361, episodes: 90, mean episode reward: 9600.0, agent episode reward: [2500.0, 7100.0], [2501.816626561813, 7100.0], time: 28.626
steps: 90361, episodes: 91, mean episode reward: 8550.0, agent episode reward: [4150.0, 4400.0], [4151.565198192107, 4400.0], time: 29.156
steps: 91361, episodes: 92, mean episode reward: 10450.0, agent episode reward: [8200.0, 2250.0], [8202.172618034007, 2250.0], time: 29.417
steps: 92361, episodes: 93, mean episode reward: 8200.0, agent episode reward: [5950.0, 2250.0], [5951.6418269848045, 2250.0], time: 29.702
steps: 93361, episodes: 94, mean episode reward: 15850.0, agent episode reward: [6250.0, 9600.0], [6251.632688611119, 9600.0], time: 29.92
steps: 94361, episodes: 95, mean episode reward: 10050.0, agent episode reward: [3200.0, 6850.0], [3201.665471865476, 6850.0], time: 30.243
steps: 95361, episodes: 96, mean episode reward: 11100.0, agent episode reward: [7400.0, 3700.0], [7401.591089830619, 3700.0], time: 31.227
steps: 96327, episodes: 97, mean episode reward: 7550.0, agent episode reward: [2250.0, 5300.0], [2251.522085360655, 5300.0], time: 30.573
steps: 97327, episodes: 98, mean episode reward: 11900.0, agent episode reward: [4500.0, 7400.0], [4501.561061070379, 7400.0], time: 31.689
steps: 98327, episodes: 99, mean episode reward: 5150.0, agent episode reward: [4900.0, 250.0], [4901.658287625057, 250.0], time: 31.923
steps: 99219, episodes: 100, mean episode reward: 6100.0, agent episode reward: [5850.0, 250.0], [5852.005758445965, 250.0], time: 28.67
...Finished total of 101 episodes.
2021-10-24 08:33:32.166332: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 08:33:32.170408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 08:33:32.170695: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5621e3ea44a0 executing computations on platform Host. Devices:
2021-10-24 08:33:32.170718: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 7150.0, agent episode reward: [3450.0, 3700.0], [3451.6311389145576, 3700.0], time: 5.803
steps: 2000, episodes: 2, mean episode reward: 9000.0, agent episode reward: [250.0, 8750.0], [251.20459116105062, 8750.0], time: 8.318
steps: 3000, episodes: 3, mean episode reward: 13150.0, agent episode reward: [9400.0, 3750.0], [9401.433656205869, 3750.0], time: 8.694
steps: 4000, episodes: 4, mean episode reward: 13950.0, agent episode reward: [5850.0, 8100.0], [5851.394346549473, 8100.0], time: 8.958
steps: 5000, episodes: 5, mean episode reward: 9250.0, agent episode reward: [250.0, 9000.0], [251.27578311705778, 9000.0], time: 9.695
steps: 5979, episodes: 6, mean episode reward: 2350.0, agent episode reward: [1350.0, 1000.0], [1351.275864669132, 1000.0], time: 9.339
steps: 6979, episodes: 7, mean episode reward: 8600.0, agent episode reward: [6950.0, 1650.0], [6951.394385002631, 1650.0], time: 9.768
steps: 7760, episodes: 8, mean episode reward: 6550.0, agent episode reward: [4250.0, 2300.0], [4251.074199613085, 2300.0], time: 8.046
steps: 8760, episodes: 9, mean episode reward: 11650.0, agent episode reward: [2700.0, 8950.0], [2701.4195162662563, 8950.0], time: 10.005
steps: 9760, episodes: 10, mean episode reward: 9750.0, agent episode reward: [9500.0, 250.0], [9501.365784590684, 250.0], time: 10.654
steps: 10760, episodes: 11, mean episode reward: 6800.0, agent episode reward: [1000.0, 5800.0], [1001.3465059440272, 5800.0], time: 10.589
steps: 11760, episodes: 12, mean episode reward: 13350.0, agent episode reward: [5150.0, 8200.0], [5151.378330580201, 8200.0], time: 11.255
steps: 12651, episodes: 13, mean episode reward: 3800.0, agent episode reward: [250.0, 3550.0], [251.13805279833554, 3550.0], time: 9.749
steps: 13651, episodes: 14, mean episode reward: 7700.0, agent episode reward: [6750.0, 950.0], [6751.363771914055, 950.0], time: 11.433
steps: 14651, episodes: 15, mean episode reward: 5650.0, agent episode reward: [3400.0, 2250.0], [3401.55115798786, 2250.0], time: 11.105
steps: 15651, episodes: 16, mean episode reward: 10100.0, agent episode reward: [750.0, 9350.0], [751.3978431612492, 9350.0], time: 11.824
steps: 16651, episodes: 17, mean episode reward: 9350.0, agent episode reward: [5900.0, 3450.0], [5901.402930895502, 3450.0], time: 11.397
steps: 17651, episodes: 18, mean episode reward: 5100.0, agent episode reward: [450.0, 4650.0], [451.50326791333737, 4650.0], time: 11.602
steps: 18651, episodes: 19, mean episode reward: 14350.0, agent episode reward: [6600.0, 7750.0], [6601.750699010877, 7750.0], time: 11.775
steps: 19651, episodes: 20, mean episode reward: 9850.0, agent episode reward: [1750.0, 8100.0], [1751.4652441797236, 8100.0], time: 12.336
steps: 20651, episodes: 21, mean episode reward: 12900.0, agent episode reward: [2000.0, 10900.0], [2001.3986602183409, 10900.0], time: 11.86
steps: 21651, episodes: 22, mean episode reward: 8100.0, agent episode reward: [1750.0, 6350.0], [1751.3472129054674, 6350.0], time: 12.135
steps: 22651, episodes: 23, mean episode reward: 5250.0, agent episode reward: [4250.0, 1000.0], [4251.371689448538, 1000.0], time: 12.18
steps: 23651, episodes: 24, mean episode reward: 7000.0, agent episode reward: [4500.0, 2500.0], [4501.489266927768, 2500.0], time: 12.842
steps: 24559, episodes: 25, mean episode reward: 10550.0, agent episode reward: [1250.0, 9300.0], [1251.2285640621403, 9300.0], time: 11.173
steps: 25559, episodes: 26, mean episode reward: 16550.0, agent episode reward: [7650.0, 8900.0], [7651.420477461815, 8900.0], time: 12.632
steps: 26559, episodes: 27, mean episode reward: 7300.0, agent episode reward: [6500.0, 800.0], [6501.391570373127, 800.0], time: 12.626
steps: 27559, episodes: 28, mean episode reward: 1550.0, agent episode reward: [600.0, 950.0], [601.3507937322074, 950.0], time: 13.334
steps: 28559, episodes: 29, mean episode reward: 15100.0, agent episode reward: [6900.0, 8200.0], [6901.442411100912, 8200.0], time: 13.117
steps: 29559, episodes: 30, mean episode reward: 8000.0, agent episode reward: [2400.0, 5600.0], [2401.5193650185024, 5600.0], time: 13.247
steps: 30559, episodes: 31, mean episode reward: 6350.0, agent episode reward: [1500.0, 4850.0], [1501.2861413914768, 4850.0], time: 13.546
steps: 31559, episodes: 32, mean episode reward: 8450.0, agent episode reward: [6950.0, 1500.0], [6951.3829628469375, 1500.0], time: 13.612
steps: 32559, episodes: 33, mean episode reward: 15250.0, agent episode reward: [8400.0, 6850.0], [8401.322194027282, 6850.0], time: 13.899
steps: 33559, episodes: 34, mean episode reward: 7700.0, agent episode reward: [4950.0, 2750.0], [4951.310388804875, 2750.0], time: 13.861
steps: 34559, episodes: 35, mean episode reward: 3250.0, agent episode reward: [3000.0, 250.0], [3001.312921594391, 250.0], time: 13.993
steps: 35559, episodes: 36, mean episode reward: 7600.0, agent episode reward: [3000.0, 4600.0], [3001.3523454396695, 4600.0], time: 14.735
steps: 36559, episodes: 37, mean episode reward: 8600.0, agent episode reward: [2450.0, 6150.0], [2451.406460962772, 6150.0], time: 14.38
steps: 37559, episodes: 38, mean episode reward: 5950.0, agent episode reward: [250.0, 5700.0], [251.2642290060267, 5700.0], time: 14.513
steps: 38559, episodes: 39, mean episode reward: 12450.0, agent episode reward: [4900.0, 7550.0], [4901.4224589130245, 7550.0], time: 14.639
steps: 39559, episodes: 40, mean episode reward: 5600.0, agent episode reward: [4650.0, 950.0], [4651.339072040213, 950.0], time: 15.309
steps: 40559, episodes: 41, mean episode reward: 5750.0, agent episode reward: [5050.0, 700.0], [5051.412327354326, 700.0], time: 15.044
steps: 41559, episodes: 42, mean episode reward: 18800.0, agent episode reward: [16300.0, 2500.0], [16301.403334003238, 2500.0], time: 15.429
steps: 42559, episodes: 43, mean episode reward: 13050.0, agent episode reward: [9800.0, 3250.0], [9801.479046009357, 3250.0], time: 15.445
steps: 43505, episodes: 44, mean episode reward: 6250.0, agent episode reward: [6000.0, 250.0], [6001.259904291123, 250.0], time: 15.257
steps: 44505, episodes: 45, mean episode reward: 6800.0, agent episode reward: [3000.0, 3800.0], [3001.339634805293, 3800.0], time: 15.784
steps: 45505, episodes: 46, mean episode reward: 13600.0, agent episode reward: [6200.0, 7400.0], [6201.372876291357, 7400.0], time: 16.134
steps: 46505, episodes: 47, mean episode reward: 12150.0, agent episode reward: [4750.0, 7400.0], [4751.47241649321, 7400.0], time: 16.837
steps: 47505, episodes: 48, mean episode reward: 22150.0, agent episode reward: [8250.0, 13900.0], [8251.430673419556, 13900.0], time: 16.512
steps: 48505, episodes: 49, mean episode reward: 11050.0, agent episode reward: [2000.0, 9050.0], [2001.3693972858136, 9050.0], time: 16.828
steps: 49224, episodes: 50, mean episode reward: 2750.0, agent episode reward: [1250.0, 1500.0], [1251.0407797688943, 1500.0], time: 12.031
steps: 50224, episodes: 51, mean episode reward: 10000.0, agent episode reward: [4750.0, 5250.0], [4751.2710195653835, 5250.0], time: 17.696
steps: 51224, episodes: 52, mean episode reward: 5200.0, agent episode reward: [4700.0, 500.0], [4701.291239003911, 500.0], time: 17.469
steps: 52224, episodes: 53, mean episode reward: 6800.0, agent episode reward: [4050.0, 2750.0], [4051.3646453699635, 2750.0], time: 17.709
steps: 53224, episodes: 54, mean episode reward: 5900.0, agent episode reward: [5150.0, 750.0], [5151.374588957171, 750.0], time: 18.088
steps: 54224, episodes: 55, mean episode reward: 13100.0, agent episode reward: [5500.0, 7600.0], [5501.336078198552, 7600.0], time: 18.106
steps: 55224, episodes: 56, mean episode reward: 12100.0, agent episode reward: [5200.0, 6900.0], [5201.312870784822, 6900.0], time: 18.493
steps: 56224, episodes: 57, mean episode reward: 15900.0, agent episode reward: [10150.0, 5750.0], [10151.685422779337, 5750.0], time: 19.186
steps: 57224, episodes: 58, mean episode reward: 7500.0, agent episode reward: [2650.0, 4850.0], [2651.6549733913052, 4850.0], time: 18.739
steps: 58224, episodes: 59, mean episode reward: 10500.0, agent episode reward: [2750.0, 7750.0], [2751.384857689315, 7750.0], time: 19.134/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 59224, episodes: 60, mean episode reward: 6050.0, agent episode reward: [2200.0, 3850.0], [2201.3884397251836, 3850.0], time: 19.95
steps: 60224, episodes: 61, mean episode reward: 9500.0, agent episode reward: [1750.0, 7750.0], [1751.3685982440368, 7750.0], time: 19.742
steps: 61224, episodes: 62, mean episode reward: 9850.0, agent episode reward: [1500.0, 8350.0], [1501.4109073276686, 8350.0], time: 20.052
steps: 62224, episodes: 63, mean episode reward: 11650.0, agent episode reward: [5950.0, 5700.0], [5951.394491495612, 5700.0], time: 20.559
steps: 63224, episodes: 64, mean episode reward: 5150.0, agent episode reward: [250.0, 4900.0], [251.3128364224013, 4900.0], time: 21.069
steps: 64224, episodes: 65, mean episode reward: 14100.0, agent episode reward: [4250.0, 9850.0], [4251.382862403111, 9850.0], time: 20.667
steps: 65224, episodes: 66, mean episode reward: 11450.0, agent episode reward: [11000.0, 450.0], [11001.444117909707, 450.0], time: 21.18
steps: 66224, episodes: 67, mean episode reward: 7900.0, agent episode reward: [2000.0, 5900.0], [2001.4192415725304, 5900.0], time: 21.692
steps: 67224, episodes: 68, mean episode reward: 9000.0, agent episode reward: [1500.0, 7500.0], [1501.305015037594, 7500.0], time: 22.252
steps: 68079, episodes: 69, mean episode reward: 8500.0, agent episode reward: [7750.0, 750.0], [7751.242905905978, 750.0], time: 18.728
steps: 69079, episodes: 70, mean episode reward: 5950.0, agent episode reward: [3850.0, 2100.0], [3851.404921255428, 2100.0], time: 22.274
steps: 70079, episodes: 71, mean episode reward: 2500.0, agent episode reward: [150.0, 2350.0], [151.33244492442626, 2350.0], time: 22.873
steps: 71079, episodes: 72, mean episode reward: 8050.0, agent episode reward: [4100.0, 3950.0], [4101.528277639003, 3950.0], time: 22.791
steps: 72079, episodes: 73, mean episode reward: 12250.0, agent episode reward: [8100.0, 4150.0], [8101.254268719854, 4150.0], time: 23.047
steps: 73079, episodes: 74, mean episode reward: 9750.0, agent episode reward: [750.0, 9000.0], [751.3465501096128, 9000.0], time: 23.472
steps: 74079, episodes: 75, mean episode reward: 7700.0, agent episode reward: [4250.0, 3450.0], [4251.249731087301, 3450.0], time: 24.176
steps: 75079, episodes: 76, mean episode reward: 10350.0, agent episode reward: [7850.0, 2500.0], [7851.407951814313, 2500.0], time: 24.24
steps: 76079, episodes: 77, mean episode reward: 8700.0, agent episode reward: [4200.0, 4500.0], [4201.359302634576, 4500.0], time: 24.753
steps: 77079, episodes: 78, mean episode reward: 5150.0, agent episode reward: [1850.0, 3300.0], [1851.452007533202, 3300.0], time: 24.767
steps: 78079, episodes: 79, mean episode reward: 16100.0, agent episode reward: [3000.0, 13100.0], [3001.3635816853216, 13100.0], time: 25.148
steps: 79016, episodes: 80, mean episode reward: 8050.0, agent episode reward: [7800.0, 250.0], [7801.204716372317, 250.0], time: 23.908
steps: 80016, episodes: 81, mean episode reward: 8750.0, agent episode reward: [5650.0, 3100.0], [5651.24322723995, 3100.0], time: 25.801
steps: 81016, episodes: 82, mean episode reward: 5850.0, agent episode reward: [2250.0, 3600.0], [2251.267264802396, 3600.0], time: 26.064
StopIteration()
steps: 82016, episodes: 83, mean episode reward: 13100.0, agent episode reward: [2350.0, 10750.0], [2351.36964165684, 10750.0], time: 26.59
steps: 83016, episodes: 84, mean episode reward: 5000.0, agent episode reward: [4750.0, 250.0], [4751.349924829971, 250.0], time: 26.89
steps: 83854, episodes: 85, mean episode reward: 3050.0, agent episode reward: [2300.0, 750.0], [2301.2302129919894, 750.0], time: 22.487
steps: 84854, episodes: 86, mean episode reward: 12250.0, agent episode reward: [7800.0, 4450.0], [7801.4144257309745, 4450.0], time: 26.974
steps: 85854, episodes: 87, mean episode reward: 7750.0, agent episode reward: [6050.0, 1700.0], [6051.3720847239465, 1700.0], time: 27.673
steps: 86854, episodes: 88, mean episode reward: 8400.0, agent episode reward: [3750.0, 4650.0], [3751.406432512193, 4650.0], time: 28.019
steps: 87854, episodes: 89, mean episode reward: 12800.0, agent episode reward: [1450.0, 11350.0], [1451.4903206862457, 11350.0], time: 28.286
steps: 88854, episodes: 90, mean episode reward: 5550.0, agent episode reward: [700.0, 4850.0], [701.3336491354239, 4850.0], time: 28.445
steps: 89854, episodes: 91, mean episode reward: 5300.0, agent episode reward: [2650.0, 2650.0], [2651.2078212350425, 2650.0], time: 28.661
steps: 90854, episodes: 92, mean episode reward: 8500.0, agent episode reward: [7550.0, 950.0], [7551.330231180385, 950.0], time: 29.618
steps: 91854, episodes: 93, mean episode reward: 4200.0, agent episode reward: [100.0, 4100.0], [101.36528768197111, 4100.0], time: 30.616
steps: 92854, episodes: 94, mean episode reward: 7800.0, agent episode reward: [2750.0, 5050.0], [2751.4515764382, 5050.0], time: 30.182
steps: 93854, episodes: 95, mean episode reward: 13800.0, agent episode reward: [13050.0, 750.0], [13051.30347421285, 750.0], time: 30.364
steps: 94854, episodes: 96, mean episode reward: 6600.0, agent episode reward: [4100.0, 2500.0], [4101.361446664261, 2500.0], time: 30.686
steps: 95854, episodes: 97, mean episode reward: 4700.0, agent episode reward: [1500.0, 3200.0], [1501.386469396786, 3200.0], time: 31.194
steps: 96854, episodes: 98, mean episode reward: 7200.0, agent episode reward: [150.0, 7050.0], [151.41791677334578, 7050.0], time: 31.333
steps: 97854, episodes: 99, mean episode reward: 16600.0, agent episode reward: [4900.0, 11700.0], [4901.469728499442, 11700.0], time: 31.717
steps: 98854, episodes: 100, mean episode reward: 3200.0, agent episode reward: [1500.0, 1700.0], [1501.2671470962932, 1700.0], time: 31.745
...Finished total of 101 episodes.
2021-10-24 09:04:06.227258: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 09:04:06.231323: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 09:04:06.232123: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55f8f179d4d0 executing computations on platform Host. Devices:
2021-10-24 09:04:06.232144: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 8500.0, agent episode reward: [7250.0, 1250.0], [7252.016490856771, 1250.0], time: 5.868
steps: 2000, episodes: 2, mean episode reward: 1600.0, agent episode reward: [500.0, 1100.0], [501.24123940544195, 1100.0], time: 8.592
steps: 3000, episodes: 3, mean episode reward: 12450.0, agent episode reward: [3750.0, 8700.0], [3751.434071942455, 8700.0], time: 8.792
steps: 4000, episodes: 4, mean episode reward: 15100.0, agent episode reward: [5000.0, 10100.0], [5001.8194929088295, 10100.0], time: 9.151
steps: 5000, episodes: 5, mean episode reward: 11850.0, agent episode reward: [5950.0, 5900.0], [5951.379307470006, 5900.0], time: 9.0
steps: 6000, episodes: 6, mean episode reward: 10850.0, agent episode reward: [3400.0, 7450.0], [3401.4866578101437, 7450.0], time: 10.234
steps: 7000, episodes: 7, mean episode reward: 8500.0, agent episode reward: [1750.0, 6750.0], [1751.4067374545423, 6750.0], time: 9.893
steps: 8000, episodes: 8, mean episode reward: 9350.0, agent episode reward: [1750.0, 7600.0], [1751.5435685152988, 7600.0], time: 10.126
steps: 9000, episodes: 9, mean episode reward: 11700.0, agent episode reward: [3500.0, 8200.0], [3501.4669752589875, 8200.0], time: 10.086
steps: 10000, episodes: 10, mean episode reward: 9450.0, agent episode reward: [9200.0, 250.0], [9202.234598903398, 250.0], time: 11.043
steps: 11000, episodes: 11, mean episode reward: 10700.0, agent episode reward: [8950.0, 1750.0], [8952.365730702817, 1750.0], time: 10.738
steps: 12000, episodes: 12, mean episode reward: 7650.0, agent episode reward: [4650.0, 3000.0], [4651.489347858583, 3000.0], time: 11.16
steps: 13000, episodes: 13, mean episode reward: 9550.0, agent episode reward: [750.0, 8800.0], [751.2516824297586, 8800.0], time: 11.338
steps: 14000, episodes: 14, mean episode reward: 12300.0, agent episode reward: [5200.0, 7100.0], [5201.63960253144, 7100.0], time: 11.622
steps: 15000, episodes: 15, mean episode reward: 12150.0, agent episode reward: [6450.0, 5700.0], [6451.723815420626, 5700.0], time: 11.587
steps: 16000, episodes: 16, mean episode reward: 11750.0, agent episode reward: [6150.0, 5600.0], [6151.609023328356, 5600.0], time: 11.812
steps: 17000, episodes: 17, mean episode reward: 14350.0, agent episode reward: [3000.0, 11350.0], [3001.4235195465576, 11350.0], time: 11.534
steps: 18000, episodes: 18, mean episode reward: 1300.0, agent episode reward: [250.0, 1050.0], [251.27991857441063, 1050.0], time: 11.755
steps: 19000, episodes: 19, mean episode reward: 22400.0, agent episode reward: [16150.0, 6250.0], [16151.74997979796, 6250.0], time: 12.341
steps: 20000, episodes: 20, mean episode reward: 6200.0, agent episode reward: [250.0, 5950.0], [251.31392959179044, 5950.0], time: 12.182
steps: 21000, episodes: 21, mean episode reward: 5100.0, agent episode reward: [2850.0, 2250.0], [2851.7699887670583, 2250.0], time: 12.338
steps: 22000, episodes: 22, mean episode reward: 6900.0, agent episode reward: [6700.0, 200.0], [6701.436414534844, 200.0], time: 12.176
steps: 23000, episodes: 23, mean episode reward: 19750.0, agent episode reward: [12650.0, 7100.0], [12651.642372642038, 7100.0], time: 12.189
steps: 24000, episodes: 24, mean episode reward: 6800.0, agent episode reward: [2200.0, 4600.0], [2201.5272161644143, 4600.0], time: 13.349
steps: 25000, episodes: 25, mean episode reward: 6400.0, agent episode reward: [2950.0, 3450.0], [2951.4439066707105, 3450.0], time: 12.523
steps: 26000, episodes: 26, mean episode reward: 7600.0, agent episode reward: [3200.0, 4400.0], [3201.629725002218, 4400.0], time: 12.944
steps: 27000, episodes: 27, mean episode reward: 12350.0, agent episode reward: [8600.0, 3750.0], [8601.704138463012, 3750.0], time: 13.203
steps: 28000, episodes: 28, mean episode reward: 6700.0, agent episode reward: [4500.0, 2200.0], [4501.566951700764, 2200.0], time: 13.307
steps: 29000, episodes: 29, mean episode reward: 5750.0, agent episode reward: [1650.0, 4100.0], [1651.6267137935727, 4100.0], time: 13.198
steps: 30000, episodes: 30, mean episode reward: 9400.0, agent episode reward: [3450.0, 5950.0], [3451.4205615916026, 5950.0], time: 13.206
steps: 31000, episodes: 31, mean episode reward: 2450.0, agent episode reward: [1450.0, 1000.0], [1451.6696173829134, 1000.0], time: 13.716
steps: 32000, episodes: 32, mean episode reward: 9250.0, agent episode reward: [6500.0, 2750.0], [6501.791316121651, 2750.0], time: 13.82
steps: 32880, episodes: 33, mean episode reward: 11250.0, agent episode reward: [8750.0, 2500.0], [8751.637762135908, 2500.0], time: 11.942
steps: 33880, episodes: 34, mean episode reward: 5400.0, agent episode reward: [3250.0, 2150.0], [3251.4809895902995, 2150.0], time: 14.015
steps: 34880, episodes: 35, mean episode reward: 8500.0, agent episode reward: [7000.0, 1500.0], [7001.608305583966, 1500.0], time: 14.619
steps: 35880, episodes: 36, mean episode reward: 11300.0, agent episode reward: [2650.0, 8650.0], [2651.445077709778, 8650.0], time: 14.044
steps: 36880, episodes: 37, mean episode reward: 6400.0, agent episode reward: [4950.0, 1450.0], [4951.7214818292, 1450.0], time: 14.613
steps: 37880, episodes: 38, mean episode reward: 4200.0, agent episode reward: [250.0, 3950.0], [251.30314584332004, 3950.0], time: 14.605
steps: 38880, episodes: 39, mean episode reward: 10950.0, agent episode reward: [2500.0, 8450.0], [2501.3282426533488, 8450.0], time: 15.156
steps: 39880, episodes: 40, mean episode reward: 10950.0, agent episode reward: [2250.0, 8700.0], [2251.3685497780566, 8700.0], time: 15.109
steps: 40880, episodes: 41, mean episode reward: 7400.0, agent episode reward: [2000.0, 5400.0], [2001.5327026183984, 5400.0], time: 15.34
steps: 41880, episodes: 42, mean episode reward: 11000.0, agent episode reward: [3000.0, 8000.0], [3001.339271793535, 8000.0], time: 15.377
steps: 42880, episodes: 43, mean episode reward: 9900.0, agent episode reward: [6400.0, 3500.0], [6401.532933503304, 3500.0], time: 15.867
steps: 43880, episodes: 44, mean episode reward: 6650.0, agent episode reward: [1250.0, 5400.0], [1251.4271140109545, 5400.0], time: 15.9
steps: 44880, episodes: 45, mean episode reward: 5800.0, agent episode reward: [2950.0, 2850.0], [2951.5073415932907, 2850.0], time: 16.168
steps: 45880, episodes: 46, mean episode reward: 14900.0, agent episode reward: [3500.0, 11400.0], [3501.308486843841, 11400.0], time: 16.281
steps: 46880, episodes: 47, mean episode reward: 12250.0, agent episode reward: [3000.0, 9250.0], [3001.4750391385555, 9250.0], time: 16.86
steps: 47880, episodes: 48, mean episode reward: 14150.0, agent episode reward: [4250.0, 9900.0], [4251.444789373887, 9900.0], time: 16.761
steps: 48880, episodes: 49, mean episode reward: 5050.0, agent episode reward: [2800.0, 2250.0], [2801.7615747534333, 2250.0], time: 16.827
steps: 49880, episodes: 50, mean episode reward: 11050.0, agent episode reward: [3450.0, 7600.0], [3451.4927852461005, 7600.0], time: 17.17
steps: 50880, episodes: 51, mean episode reward: 8500.0, agent episode reward: [4350.0, 4150.0], [4351.694257362996, 4150.0], time: 17.284
steps: 51880, episodes: 52, mean episode reward: 7900.0, agent episode reward: [4650.0, 3250.0], [4651.504444952227, 3250.0], time: 18.189
steps: 52880, episodes: 53, mean episode reward: 8350.0, agent episode reward: [7400.0, 950.0], [7401.598901146323, 950.0], time: 17.944
steps: 53880, episodes: 54, mean episode reward: 11050.0, agent episode reward: [1750.0, 9300.0], [1751.375362559627, 9300.0], time: 18.084
steps: 54880, episodes: 55, mean episode reward: 7450.0, agent episode reward: [3750.0, 3700.0], [3751.430925853401, 3700.0], time: 18.832
steps: 55880, episodes: 56, mean episode reward: 14900.0, agent episode reward: [9700.0, 5200.0], [9701.475080329823, 5200.0], time: 18.648
steps: 56880, episodes: 57, mean episode reward: 9750.0, agent episode reward: [2250.0, 7500.0], [2251.3344179025594, 7500.0], time: 18.953
steps: 57880, episodes: 58, mean episode reward: 11050.0, agent episode reward: [6800.0, 4250.0], [6801.436389196777, 4250.0], time: 19.104
steps: 58880, episodes: 59, mean episode reward: 10350.0, agent episode reward: [5600.0, 4750.0], [5601.545826577002, 4750.0], time: 19.904/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 59880, episodes: 60, mean episode reward: 15150.0, agent episode reward: [7900.0, 7250.0], [7901.56409324255, 7250.0], time: 19.724
steps: 60880, episodes: 61, mean episode reward: 20400.0, agent episode reward: [7500.0, 12900.0], [7501.354688585143, 12900.0], time: 20.047
steps: 61880, episodes: 62, mean episode reward: 5550.0, agent episode reward: [5100.0, 450.0], [5101.421570413984, 450.0], time: 20.298
steps: 62880, episodes: 63, mean episode reward: 6500.0, agent episode reward: [1600.0, 4900.0], [1601.2797882929945, 4900.0], time: 21.049
steps: 63880, episodes: 64, mean episode reward: 2950.0, agent episode reward: [250.0, 2700.0], [251.16447854102128, 2700.0], time: 20.907
steps: 64880, episodes: 65, mean episode reward: 10300.0, agent episode reward: [4900.0, 5400.0], [4901.515296533321, 5400.0], time: 21.247
steps: 65880, episodes: 66, mean episode reward: 12300.0, agent episode reward: [7550.0, 4750.0], [7551.716430953522, 4750.0], time: 21.751
steps: 66880, episodes: 67, mean episode reward: 7000.0, agent episode reward: [2150.0, 4850.0], [2151.332505767012, 4850.0], time: 21.956
steps: 67880, episodes: 68, mean episode reward: 5700.0, agent episode reward: [2250.0, 3450.0], [2251.2922322856543, 3450.0], time: 22.128
steps: 68880, episodes: 69, mean episode reward: 9250.0, agent episode reward: [4000.0, 5250.0], [4001.526515880642, 5250.0], time: 22.402
steps: 69880, episodes: 70, mean episode reward: 2900.0, agent episode reward: [250.0, 2650.0], [251.19250459799096, 2650.0], time: 22.761
steps: 70880, episodes: 71, mean episode reward: 12800.0, agent episode reward: [4650.0, 8150.0], [4651.3690248127605, 8150.0], time: 23.503
steps: 71880, episodes: 72, mean episode reward: 8300.0, agent episode reward: [7400.0, 900.0], [7401.2934562097425, 900.0], time: 23.175
steps: 72880, episodes: 73, mean episode reward: 17950.0, agent episode reward: [15450.0, 2500.0], [15451.891834428336, 2500.0], time: 23.552
steps: 73880, episodes: 74, mean episode reward: 9400.0, agent episode reward: [250.0, 9150.0], [251.24847015304718, 9150.0], time: 24.092
steps: 74880, episodes: 75, mean episode reward: 16750.0, agent episode reward: [5000.0, 11750.0], [5001.45849637322, 11750.0], time: 24.505
steps: 75880, episodes: 76, mean episode reward: 12600.0, agent episode reward: [3750.0, 8850.0], [3751.5927104802636, 8850.0], time: 24.57
steps: 76880, episodes: 77, mean episode reward: 7150.0, agent episode reward: [4400.0, 2750.0], [4401.775214244397, 2750.0], time: 25.02
steps: 77880, episodes: 78, mean episode reward: 7250.0, agent episode reward: [5500.0, 1750.0], [5501.4426470571, 1750.0], time: 25.517
steps: 78880, episodes: 79, mean episode reward: 8850.0, agent episode reward: [7900.0, 950.0], [7901.395931226288, 950.0], time: 25.5
steps: 79880, episodes: 80, mean episode reward: 4200.0, agent episode reward: [3950.0, 250.0], [3951.7599828802427, 250.0], time: 26.029
steps: 80880, episodes: 81, mean episode reward: 7800.0, agent episode reward: [1950.0, 5850.0], [1951.3066613136739, 5850.0], time: 26.164
steps: 81880, episodes: 82, mean episode reward: 10350.0, agent episode reward: [4250.0, 6100.0], [4251.4144812001805, 6100.0], time: 26.538
steps: 82880, episodes: 83, mean episode reward: 9100.0, agent episode reward: [4650.0, 4450.0], [4651.655720888452, 4450.0], time: 26.692
steps: 83880, episodes: 84, mean episode reward: 5400.0, agent episode reward: [500.0, 4900.0], [501.29486032110833, 4900.0], time: 27.183
steps: 84880, episodes: 85, mean episode reward: 3700.0, agent episode reward: [250.0, 3450.0], [251.25150648132063, 3450.0], time: 28.117
steps: 85880, episodes: 86, mean episode reward: 5700.0, agent episode reward: [2700.0, 3000.0], [2701.2413367567, 3000.0], time: 28.118
steps: 86880, episodes: 87, mean episode reward: 11800.0, agent episode reward: [7300.0, 4500.0], [7301.615233411099, 4500.0], time: 28.328
steps: 87880, episodes: 88, mean episode reward: 14700.0, agent episode reward: [6700.0, 8000.0], [6701.573861607661, 8000.0], time: 28.632
steps: 88880, episodes: 89, mean episode reward: 9600.0, agent episode reward: [1000.0, 8600.0], [1001.1890164893789, 8600.0], time: 28.907
steps: 89880, episodes: 90, mean episode reward: 11850.0, agent episode reward: [5750.0, 6100.0], [5751.33388053911, 6100.0], time: 29.128
steps: 90880, episodes: 91, mean episode reward: 3600.0, agent episode reward: [2750.0, 850.0], [2751.253209104229, 850.0], time: 29.255
steps: 91880, episodes: 92, mean episode reward: 22650.0, agent episode reward: [13950.0, 8700.0], [13951.449806492732, 8700.0], time: 29.903
steps: 92880, episodes: 93, mean episode reward: 7000.0, agent episode reward: [650.0, 6350.0], [651.4847450360376, 6350.0], time: 30.005
steps: 93880, episodes: 94, mean episode reward: 5600.0, agent episode reward: [4950.0, 650.0], [4951.352445764553, 650.0], time: 30.762
steps: 94880, episodes: 95, mean episode reward: 7200.0, agent episode reward: [6950.0, 250.0], [6951.39821164769, 250.0], time: 30.826
steps: 95880, episodes: 96, mean episode reward: 5950.0, agent episode reward: [250.0, 5700.0], [251.2393180584424, 5700.0], time: 31.136
steps: 96880, episodes: 97, mean episode reward: 8850.0, agent episode reward: [5700.0, 3150.0], [5701.404684002465, 3150.0], time: 31.317
steps: 97880, episodes: 98, mean episode reward: 5150.0, agent episode reward: [3150.0, 2000.0], [3151.3656542619065, 2000.0], time: 31.8
steps: 98880, episodes: 99, mean episode reward: 7650.0, agent episode reward: [5150.0, 2500.0], [5151.605358045343, 2500.0], time: 32.113
steps: 99880, episodes: 100, mean episode reward: 10000.0, agent episode reward: [3350.0, 6650.0], [3351.3150258350465, 6650.0], time: 32.42
...Finished total of 101 episodes.
2021-10-24 09:35:25.234808: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-24 09:35:25.239064: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-24 09:35:25.239312: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x563904a44cc0 executing computations on platform Host. Devices:
2021-10-24 09:35:25.239347: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Joust
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 7450.0, agent episode reward: [3250.0, 4200.0], [3251.388067870017, 4200.0], time: 5.658
steps: 2000, episodes: 2, mean episode reward: 9100.0, agent episode reward: [250.0, 8850.0], [251.37799078495297, 8850.0], time: 8.399
steps: 3000, episodes: 3, mean episode reward: 5900.0, agent episode reward: [1750.0, 4150.0], [1751.63130846076, 4150.0], time: 8.747
steps: 4000, episodes: 4, mean episode reward: 13050.0, agent episode reward: [10850.0, 2200.0], [10851.605465105218, 2200.0], time: 8.599
steps: 5000, episodes: 5, mean episode reward: 7650.0, agent episode reward: [5150.0, 2500.0], [5151.539508051225, 2500.0], time: 9.469
steps: 6000, episodes: 6, mean episode reward: 9750.0, agent episode reward: [6250.0, 3500.0], [6251.755823095509, 3500.0], time: 9.739
steps: 6973, episodes: 7, mean episode reward: 6600.0, agent episode reward: [2250.0, 4350.0], [2251.410319372644, 4350.0], time: 9.662
steps: 7973, episodes: 8, mean episode reward: 10250.0, agent episode reward: [5150.0, 5100.0], [5151.3876579900425, 5100.0], time: 10.306
steps: 8973, episodes: 9, mean episode reward: 6250.0, agent episode reward: [4550.0, 1700.0], [4551.4911546877265, 1700.0], time: 9.898
steps: 9973, episodes: 10, mean episode reward: 3500.0, agent episode reward: [1400.0, 2100.0], [1401.3026170958483, 2100.0], time: 11.068
steps: 10973, episodes: 11, mean episode reward: 8900.0, agent episode reward: [250.0, 8650.0], [251.36119276926078, 8650.0], time: 10.87
steps: 11973, episodes: 12, mean episode reward: 7350.0, agent episode reward: [3000.0, 4350.0], [3001.5806751818327, 4350.0], time: 11.31
steps: 12973, episodes: 13, mean episode reward: 6600.0, agent episode reward: [2200.0, 4400.0], [2201.291303486627, 4400.0], time: 10.984
steps: 13973, episodes: 14, mean episode reward: 5550.0, agent episode reward: [250.0, 5300.0], [251.2776881484596, 5300.0], time: 11.046
steps: 14973, episodes: 15, mean episode reward: 12500.0, agent episode reward: [5000.0, 7500.0], [5001.411265122352, 7500.0], time: 11.222
steps: 15973, episodes: 16, mean episode reward: 9150.0, agent episode reward: [250.0, 8900.0], [251.25746487855523, 8900.0], time: 12.04
steps: 16973, episodes: 17, mean episode reward: 12850.0, agent episode reward: [2000.0, 10850.0], [2001.3510954608294, 10850.0], time: 11.698
steps: 17973, episodes: 18, mean episode reward: 9200.0, agent episode reward: [7200.0, 2000.0], [7202.015156772808, 2000.0], time: 11.674
StopIteration()
steps: 18973, episodes: 19, mean episode reward: 6550.0, agent episode reward: [6050.0, 500.0], [6051.739360710275, 500.0], time: 11.825
steps: 19973, episodes: 20, mean episode reward: 12700.0, agent episode reward: [250.0, 12450.0], [251.415328294645, 12450.0], time: 12.455
steps: 20973, episodes: 21, mean episode reward: 15650.0, agent episode reward: [6200.0, 9450.0], [6201.653668757326, 9450.0], time: 11.97
steps: 21973, episodes: 22, mean episode reward: 8550.0, agent episode reward: [8300.0, 250.0], [8301.797766887224, 250.0], time: 12.555
steps: 22973, episodes: 23, mean episode reward: 5850.0, agent episode reward: [4850.0, 1000.0], [4851.6492093491925, 1000.0], time: 12.33
steps: 23973, episodes: 24, mean episode reward: 17800.0, agent episode reward: [13550.0, 4250.0], [13551.752485229217, 4250.0], time: 12.874
steps: 24973, episodes: 25, mean episode reward: 16600.0, agent episode reward: [12100.0, 4500.0], [12101.75176913046, 4500.0], time: 12.576
steps: 25973, episodes: 26, mean episode reward: 9450.0, agent episode reward: [4500.0, 4950.0], [4501.311182275683, 4950.0], time: 12.793
steps: 26852, episodes: 27, mean episode reward: 5800.0, agent episode reward: [3000.0, 2800.0], [3001.207789013535, 2800.0], time: 11.325
steps: 27852, episodes: 28, mean episode reward: 10850.0, agent episode reward: [250.0, 10600.0], [251.22964990614523, 10600.0], time: 13.53
steps: 28852, episodes: 29, mean episode reward: 10100.0, agent episode reward: [1250.0, 8850.0], [1251.3138863404217, 8850.0], time: 13.342
steps: 29852, episodes: 30, mean episode reward: 7200.0, agent episode reward: [6450.0, 750.0], [6451.879426724064, 750.0], time: 13.404
steps: 30852, episodes: 31, mean episode reward: 9500.0, agent episode reward: [9250.0, 250.0], [9251.807220992276, 250.0], time: 13.286
steps: 31852, episodes: 32, mean episode reward: 6450.0, agent episode reward: [4200.0, 2250.0], [4202.051279583168, 2250.0], time: 14.218
steps: 32852, episodes: 33, mean episode reward: 12300.0, agent episode reward: [6900.0, 5400.0], [6901.600390988937, 5400.0], time: 14.014
steps: 33852, episodes: 34, mean episode reward: 11300.0, agent episode reward: [7550.0, 3750.0], [7551.586828780294, 3750.0], time: 13.878
steps: 34852, episodes: 35, mean episode reward: 8200.0, agent episode reward: [2250.0, 5950.0], [2251.3589092743027, 5950.0], time: 14.404
steps: 35852, episodes: 36, mean episode reward: 11100.0, agent episode reward: [2000.0, 9100.0], [2001.4276086988896, 9100.0], time: 14.172
steps: 36852, episodes: 37, mean episode reward: 9200.0, agent episode reward: [4950.0, 4250.0], [4951.778056408878, 4250.0], time: 14.423
steps: 37852, episodes: 38, mean episode reward: 8450.0, agent episode reward: [700.0, 7750.0], [701.3326096476626, 7750.0], time: 14.972
steps: 38852, episodes: 39, mean episode reward: 5100.0, agent episode reward: [750.0, 4350.0], [751.3144802317182, 4350.0], time: 15.181
steps: 39852, episodes: 40, mean episode reward: 10400.0, agent episode reward: [8250.0, 2150.0], [8251.672261272945, 2150.0], time: 14.8
steps: 40852, episodes: 41, mean episode reward: 8750.0, agent episode reward: [8250.0, 500.0], [8251.913802231775, 500.0], time: 15.457
steps: 41852, episodes: 42, mean episode reward: 13450.0, agent episode reward: [2750.0, 10700.0], [2751.4955517544054, 10700.0], time: 15.441
steps: 42852, episodes: 43, mean episode reward: 10600.0, agent episode reward: [7100.0, 3500.0], [7101.622622054858, 3500.0], time: 15.801
steps: 43852, episodes: 44, mean episode reward: 10300.0, agent episode reward: [2900.0, 7400.0], [2901.6091868623625, 7400.0], time: 15.638
steps: 44852, episodes: 45, mean episode reward: 9300.0, agent episode reward: [3450.0, 5850.0], [3451.4025377677663, 5850.0], time: 15.821
steps: 45705, episodes: 46, mean episode reward: 6500.0, agent episode reward: [6250.0, 250.0], [6251.318032713532, 250.0], time: 14.0
steps: 46705, episodes: 47, mean episode reward: 13150.0, agent episode reward: [5250.0, 7900.0], [5251.3867925719715, 7900.0], time: 16.354
steps: 47705, episodes: 48, mean episode reward: 2150.0, agent episode reward: [50.0, 2100.0], [51.481820986485275, 2100.0], time: 16.519
steps: 48705, episodes: 49, mean episode reward: 5700.0, agent episode reward: [3450.0, 2250.0], [3451.482057141132, 2250.0], time: 16.976
steps: 49705, episodes: 50, mean episode reward: 8500.0, agent episode reward: [2400.0, 6100.0], [2401.3540185227694, 6100.0], time: 17.261
steps: 50705, episodes: 51, mean episode reward: 5400.0, agent episode reward: [1700.0, 3700.0], [1701.3807359793386, 3700.0], time: 17.555
steps: 51517, episodes: 52, mean episode reward: 5600.0, agent episode reward: [250.0, 5350.0], [251.08708854400467, 5350.0], time: 13.924
steps: 52517, episodes: 53, mean episode reward: 5850.0, agent episode reward: [1700.0, 4150.0], [1701.38688973962, 4150.0], time: 17.978
steps: 53517, episodes: 54, mean episode reward: 8450.0, agent episode reward: [5700.0, 2750.0], [5701.638875587112, 2750.0], time: 18.48
steps: 54517, episodes: 55, mean episode reward: 9200.0, agent episode reward: [3000.0, 6200.0], [3001.4647702706043, 6200.0], time: 18.383
steps: 55517, episodes: 56, mean episode reward: 5400.0, agent episode reward: [5150.0, 250.0], [5151.768486343436, 250.0], time: 18.525
steps: 56517, episodes: 57, mean episode reward: 3900.0, agent episode reward: [3650.0, 250.0], [3651.487337531267, 250.0], time: 19.192
steps: 57517, episodes: 58, mean episode reward: 15800.0, agent episode reward: [6400.0, 9400.0], [6401.507352322248, 9400.0], time: 18.966
steps: 58517, episodes: 59, mean episode reward: 8050.0, agent episode reward: [4100.0, 3950.0], [4101.4869921419, 3950.0], time: 19.11/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 59517, episodes: 60, mean episode reward: 17550.0, agent episode reward: [5700.0, 11850.0], [5701.484946376928, 11850.0], time: 19.595
steps: 60490, episodes: 61, mean episode reward: 2550.0, agent episode reward: [2250.0, 300.0], [2251.4492868417337, 300.0], time: 19.421
steps: 61490, episodes: 62, mean episode reward: 12700.0, agent episode reward: [6500.0, 6200.0], [6501.656431121723, 6200.0], time: 20.152
steps: 62490, episodes: 63, mean episode reward: 5300.0, agent episode reward: [4600.0, 700.0], [4601.546437015075, 700.0], time: 20.329
steps: 63490, episodes: 64, mean episode reward: 9700.0, agent episode reward: [4250.0, 5450.0], [4251.537408905217, 5450.0], time: 21.211
steps: 64490, episodes: 65, mean episode reward: 7350.0, agent episode reward: [3400.0, 3950.0], [3401.7232372079, 3950.0], time: 20.929
steps: 65490, episodes: 66, mean episode reward: 8300.0, agent episode reward: [1500.0, 6800.0], [1501.551655661073, 6800.0], time: 21.57
steps: 66490, episodes: 67, mean episode reward: 9600.0, agent episode reward: [6350.0, 3250.0], [6351.723014922894, 3250.0], time: 21.456
steps: 67490, episodes: 68, mean episode reward: 4400.0, agent episode reward: [2700.0, 1700.0], [2701.5124787730683, 1700.0], time: 21.758
steps: 68490, episodes: 69, mean episode reward: 13400.0, agent episode reward: [10400.0, 3000.0], [10401.95453513022, 3000.0], time: 22.61
steps: 69490, episodes: 70, mean episode reward: 8500.0, agent episode reward: [5600.0, 2900.0], [5601.655922520051, 2900.0], time: 22.85
steps: 70490, episodes: 71, mean episode reward: 7650.0, agent episode reward: [6900.0, 750.0], [6901.7204322886755, 750.0], time: 23.104
steps: 71490, episodes: 72, mean episode reward: 7550.0, agent episode reward: [2700.0, 4850.0], [2701.558103107779, 4850.0], time: 22.905
steps: 72490, episodes: 73, mean episode reward: 8800.0, agent episode reward: [6300.0, 2500.0], [6301.736575510191, 2500.0], time: 23.875
steps: 73490, episodes: 74, mean episode reward: 5600.0, agent episode reward: [3100.0, 2500.0], [3101.7631724672897, 2500.0], time: 23.825
steps: 74490, episodes: 75, mean episode reward: 11400.0, agent episode reward: [6650.0, 4750.0], [6651.517147097333, 4750.0], time: 23.938
steps: 75490, episodes: 76, mean episode reward: 10100.0, agent episode reward: [6950.0, 3150.0], [6951.432828100268, 3150.0], time: 24.418
steps: 76490, episodes: 77, mean episode reward: 6150.0, agent episode reward: [5400.0, 750.0], [5401.737658398138, 750.0], time: 24.63
steps: 77490, episodes: 78, mean episode reward: 11300.0, agent episode reward: [6150.0, 5150.0], [6151.670331123618, 5150.0], time: 24.71
steps: 78490, episodes: 79, mean episode reward: 10550.0, agent episode reward: [2200.0, 8350.0], [2201.2994278086812, 8350.0], time: 25.142
steps: 79490, episodes: 80, mean episode reward: 8650.0, agent episode reward: [3750.0, 4900.0], [3751.520932375336, 4900.0], time: 25.56
steps: 80490, episodes: 81, mean episode reward: 8050.0, agent episode reward: [4700.0, 3350.0], [4701.65014674374, 3350.0], time: 25.763
steps: 81490, episodes: 82, mean episode reward: 6550.0, agent episode reward: [1400.0, 5150.0], [1401.4921955352042, 5150.0], time: 26.081
steps: 82490, episodes: 83, mean episode reward: 10050.0, agent episode reward: [3950.0, 6100.0], [3951.563869660204, 6100.0], time: 26.84
steps: 83286, episodes: 84, mean episode reward: 7350.0, agent episode reward: [5100.0, 2250.0], [5101.507196073586, 2250.0], time: 21.084
steps: 84286, episodes: 85, mean episode reward: 7450.0, agent episode reward: [3700.0, 3750.0], [3701.7469251610155, 3750.0], time: 27.164
steps: 85286, episodes: 86, mean episode reward: 5550.0, agent episode reward: [2700.0, 2850.0], [2701.4223982693366, 2850.0], time: 27.745
steps: 86286, episodes: 87, mean episode reward: 11200.0, agent episode reward: [250.0, 10950.0], [251.4948621560172, 10950.0], time: 27.871
steps: 87275, episodes: 88, mean episode reward: 8850.0, agent episode reward: [2250.0, 6600.0], [2251.4909138832113, 6600.0], time: 27.711
steps: 88275, episodes: 89, mean episode reward: 7150.0, agent episode reward: [4900.0, 2250.0], [4901.716331309942, 2250.0], time: 28.469
steps: 89275, episodes: 90, mean episode reward: 9750.0, agent episode reward: [8500.0, 1250.0], [8502.034916766232, 1250.0], time: 28.816
steps: 90275, episodes: 91, mean episode reward: 8600.0, agent episode reward: [5450.0, 3150.0], [5451.460616037257, 3150.0], time: 29.175
steps: 91275, episodes: 92, mean episode reward: 8050.0, agent episode reward: [5100.0, 2950.0], [5101.460350565089, 2950.0], time: 29.591
steps: 92275, episodes: 93, mean episode reward: 5700.0, agent episode reward: [4950.0, 750.0], [4952.108636839026, 750.0], time: 30.036
steps: 93223, episodes: 94, mean episode reward: 7300.0, agent episode reward: [3050.0, 4250.0], [3051.9928043793343, 4250.0], time: 28.543
steps: 94223, episodes: 95, mean episode reward: 7750.0, agent episode reward: [5000.0, 2750.0], [5001.924404864977, 2750.0], time: 30.587
steps: 95223, episodes: 96, mean episode reward: 7950.0, agent episode reward: [3700.0, 4250.0], [3701.567028832919, 4250.0], time: 30.761
steps: 96180, episodes: 97, mean episode reward: 13000.0, agent episode reward: [5000.0, 8000.0], [5001.551815914424, 8000.0], time: 29.621
steps: 97180, episodes: 98, mean episode reward: 8650.0, agent episode reward: [4750.0, 3900.0], [4751.482151645509, 3900.0], time: 31.387
steps: 98164, episodes: 99, mean episode reward: 5300.0, agent episode reward: [2800.0, 2500.0], [2801.620508259504, 2500.0], time: 31.227
steps: 99164, episodes: 100, mean episode reward: 7150.0, agent episode reward: [1000.0, 6150.0], [1001.598066547618, 6150.0], time: 32.045
...Finished total of 101 episodes.
