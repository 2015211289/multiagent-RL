/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "maddpg_impl/experiments/train.py", line 13, in <module>
    import tensorflow.contrib.layers as layers
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/__init__.py", line 41, in <module>
    from tensorflow.contrib import distributions
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/distributions/__init__.py", line 44, in <module>
    from tensorflow.contrib.distributions.python.ops.estimator import *
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/learn/__init__.py", line 93, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/learn/python/__init__.py", line 28, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/__init__.py", line 30, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py", line 324, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.kmeans import KMeansClustering
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/kmeans.py", line 30, in <module>
    from tensorflow.contrib.factorization.python.ops import clustering_ops
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/contrib/factorization/__init__.py", line 22, in <module>
    from tensorflow.contrib.factorization.python.ops.clustering_ops import *
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 857, in get_code
  File "<frozen importlib._bootstrap_external>", line 525, in _compile_bytecode
KeyboardInterrupt
2021-10-31 18:10:23.582731: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-31 18:10:23.586676: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-31 18:10:23.586873: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55e3270d07f0 executing computations on platform Host. Devices:
2021-10-31 18:10:23.586902: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Box
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 2.9688601077661763], time: 5.729
steps: 2000, episodes: 2, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [3.0, -0.9885017948162543], time: 8.292
steps: 3000, episodes: 3, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.162496273483016], time: 8.084
steps: 4000, episodes: 4, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 1.9731007035185526], time: 8.103
steps: 5000, episodes: 5, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.0818977645503627], time: 8.453
steps: 6000, episodes: 6, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.005152988925619], time: 10.526
steps: 7000, episodes: 7, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 7.914774616084328], time: 9.559
steps: 8000, episodes: 8, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.0708111848919155], time: 9.258
steps: 9000, episodes: 9, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.06944654149293626], time: 9.489
steps: 10000, episodes: 10, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.3144814973408], time: 10.243
steps: 11000, episodes: 11, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.1129183843233355], time: 10.14
steps: 12000, episodes: 12, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.32156874279285], time: 10.333
steps: 13000, episodes: 13, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.348485178240936], time: 10.771
steps: 14000, episodes: 14, mean episode reward: 0.0, agent episode reward: [-19.0, 19.0], [-19.0, 21.16343019799203], time: 10.422
steps: 15000, episodes: 15, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-5.0, 7.31498252251051], time: 10.645
steps: 16000, episodes: 16, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-12.0, 14.150628251009572], time: 11.091
steps: 17000, episodes: 17, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [4.0, -1.8762836554803461], time: 10.809
steps: 18000, episodes: 18, mean episode reward: 0.0, agent episode reward: [-26.0, 26.0], [-26.0, 27.916773606962032], time: 11.079
steps: 19000, episodes: 19, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-8.0, 10.285525704672411], time: 11.76
steps: 20000, episodes: 20, mean episode reward: 0.0, agent episode reward: [-9.0, 9.0], [-9.0, 11.31288476640076], time: 11.444
steps: 21000, episodes: 21, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.457247059509081], time: 11.188
steps: 22000, episodes: 22, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-7.0, 9.480514008661693], time: 11.52
steps: 23000, episodes: 23, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-11.0, 13.11509203757985], time: 11.626
steps: 24000, episodes: 24, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.2519084144474917], time: 11.815
steps: 25000, episodes: 25, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.8181359736093587], time: 11.752
steps: 26000, episodes: 26, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.6437435426690628], time: 11.923
steps: 27000, episodes: 27, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.717417657187819], time: 12.404
steps: 28000, episodes: 28, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.590014301956858], time: 12.319
steps: 29000, episodes: 29, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.2006672206436018], time: 12.596
steps: 30000, episodes: 30, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.46933669481248], time: 13.012
steps: 31000, episodes: 31, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.4427413185920578], time: 12.914
steps: 32000, episodes: 32, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.3460455941508629], time: 13.162
steps: 33000, episodes: 33, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.30471400231353], time: 14.444
steps: 34000, episodes: 34, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.253615711205512], time: 13.365
steps: 35000, episodes: 35, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 4.871606933029407], time: 13.574
steps: 36000, episodes: 36, mean episode reward: 0.0, agent episode reward: [8.0, -8.0], [8.0, -5.751969757956663], time: 14.008
steps: 37000, episodes: 37, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.105594654550519], time: 14.024
steps: 38000, episodes: 38, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 4.969534418052987], time: 14.308
steps: 39000, episodes: 39, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.22267672603205052], time: 14.098
steps: 40000, episodes: 40, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.2423862261904273], time: 14.307
steps: 41000, episodes: 41, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.088003840652698], time: 14.751
steps: 42000, episodes: 42, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-8.0, 10.103618228849884], time: 14.701
steps: 43000, episodes: 43, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-2.0, 4.208471613111782], time: 14.883
steps: 44000, episodes: 44, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.1862802543995232], time: 15.344
steps: 45000, episodes: 45, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [3.0, -0.9475751746000047], time: 15.38
steps: 46000, episodes: 46, mean episode reward: 0.0, agent episode reward: [-9.0, 9.0], [-9.0, 11.18477112786259], time: 15.799
steps: 47000, episodes: 47, mean episode reward: 0.0, agent episode reward: [-10.0, 10.0], [-10.0, 12.88995317407616], time: 15.762
steps: 48000, episodes: 48, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.113962728402331], time: 16.031
steps: 49000, episodes: 49, mean episode reward: 0.0, agent episode reward: [-9.0, 9.0], [-9.0, 11.489666896444158], time: 16.329
steps: 50000, episodes: 50, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-2.0, 4.010188678763412], time: 16.365
steps: 51000, episodes: 51, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.163528049615346], time: 16.66
steps: 52000, episodes: 52, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.0301597205839363], time: 17.016
steps: 53000, episodes: 53, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [6.0, -4.004242891347837], time: 17.041
steps: 54000, episodes: 54, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.2653372217148897], time: 17.324
steps: 55000, episodes: 55, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.119579970281384], time: 15.777
steps: 56000, episodes: 56, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.661709472742722], time: 16.178
steps: 57000, episodes: 57, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.402760753357519], time: 16.338
steps: 58000, episodes: 58, mean episode reward: 0.0, agent episode reward: [7.0, -7.0], [7.0, -5.064623217108454], time: 17.068
steps: 59000, episodes: 59, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.04813588391041214], time: 16.992
steps: 60000, episodes: 60, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.093861265128234], time: 17.248
steps: 61000, episodes: 61, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.626446621299318], time: 17.554
steps: 62000, episodes: 62, mean episode reward: 0.0, agent episode reward: [9.0, -9.0], [9.0, -7.065835826639867], time: 17.749
steps: 63000, episodes: 63, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.544169226114213], time: 18.093
steps: 64000, episodes: 64, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.368607969388077], time: 18.281
steps: 65000, episodes: 65, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-7.0, 9.000176739135835], time: 18.614
steps: 66000, episodes: 66, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 5.984448650064083], time: 19.025
steps: 67000, episodes: 67, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-11.0, 12.939974968881454], time: 19.351
steps: 68000, episodes: 68, mean episode reward: 0.0, agent episode reward: [9.0, -9.0], [9.0, -6.82884150201699], time: 19.527
steps: 69000, episodes: 69, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.5481002740790704], time: 19.826
steps: 70000, episodes: 70, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.197963573654643], time: 20.248
steps: 71000, episodes: 71, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.5187704387406384], time: 20.467
steps: 72000, episodes: 72, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [3.0, -0.8620236257172114], time: 20.509
steps: 73000, episodes: 73, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.459813552165826], time: 21.037
steps: 74000, episodes: 74, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.56323666201053], time: 21.244
steps: 75000, episodes: 75, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.207191587835026], time: 21.567
steps: 76000, episodes: 76, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [6.0, -3.9806652137970677], time: 21.847
steps: 77000, episodes: 77, mean episode reward: 0.0, agent episode reward: [-10.0, 10.0], [-10.0, 11.921000057216055], time: 22.221
steps: 78000, episodes: 78, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.296689516243014], time: 22.44
steps: 79000, episodes: 79, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.006008980274749], time: 23.013
steps: 80000, episodes: 80, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-7.0, 9.091102834106055], time: 23.191
steps: 81000, episodes: 81, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.0321971309959754], time: 23.605
steps: 82000, episodes: 82, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.126100047297303], time: 23.773
steps: 83000, episodes: 83, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-2.0, 4.195413293078594], time: 24.083
steps: 84000, episodes: 84, mean episode reward: 0.0, agent episode reward: [5.0, -5.0], [5.0, -2.8520550788619623], time: 24.382
steps: 85000, episodes: 85, mean episode reward: 0.0, agent episode reward: [-13.0, 13.0], [-13.0, 15.16926213852197], time: 24.802
steps: 86000, episodes: 86, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.2082719970113547], time: 24.887
steps: 87000, episodes: 87, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.609850708271366], time: 25.323
steps: 88000, episodes: 88, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-12.0, 14.025821901908742], time: 25.93
steps: 89000, episodes: 89, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.015132483251625], time: 26.016
steps: 90000, episodes: 90, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.16746212681810105], time: 26.359
steps: 91000, episodes: 91, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-8.0, 10.19597509287217], time: 26.693
steps: 92000, episodes: 92, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.09707117250527], time: 26.967
steps: 93000, episodes: 93, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.5336455093942876], time: 27.242
steps: 94000, episodes: 94, mean episode reward: 0.0, agent episode reward: [-13.0, 13.0], [-13.0, 15.522061201288922], time: 27.685
steps: 95000, episodes: 95, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.3919624283000354], time: 27.914
steps: 96000, episodes: 96, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.17828026940985303], time: 28.219
steps: 97000, episodes: 97, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.475649734461335], time: 28.669
steps: 98000, episodes: 98, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.238572423410205], time: 28.899
steps: 99000, episodes: 99, mean episode reward: 0.0, agent episode reward: [-16.0, 16.0], [-16.0, 18.06311106596791], time: 29.491
steps: 100000, episodes: 100, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.119699121121907], time: 29.662
steps: 101000, episodes: 101, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 2.9267440954408848], time: 29.991
steps: 102000, episodes: 102, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.308458944297047], time: 30.507
steps: 103000, episodes: 103, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.383392306409693], time: 30.788
steps: 104000, episodes: 104, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-2.0, 4.011744903005629], time: 30.961
steps: 105000, episodes: 105, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-11.0, 13.008579547748054], time: 31.321
steps: 106000, episodes: 106, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.0313964808498257], time: 31.772
steps: 107000, episodes: 107, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.659980455586567], time: 32.019
steps: 108000, episodes: 108, mean episode reward: 0.0, agent episode reward: [-14.0, 14.0], [-14.0, 16.10168586124982], time: 32.342
steps: 109000, episodes: 109, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-11.0, 13.330600215046013], time: 32.672
steps: 110000, episodes: 110, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.0989156838889755], time: 33.141
steps: 111000, episodes: 111, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.322371992637481], time: 33.486
steps: 112000, episodes: 112, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.1740712878709094], time: 33.664
steps: 113000, episodes: 113, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.797192591765085], time: 34.1
steps: 114000, episodes: 114, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.249637668876328], time: 34.377
steps: 115000, episodes: 115, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.3858478319315064], time: 34.861
steps: 116000, episodes: 116, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.2761474399639083], time: 35.13
steps: 117000, episodes: 117, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.31474055640558], time: 35.451
steps: 118000, episodes: 118, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.279680844507186], time: 35.77
steps: 119000, episodes: 119, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-12.0, 14.10361241631271], time: 36.113
steps: 120000, episodes: 120, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.4589760298586045], time: 36.536
steps: 121000, episodes: 121, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.438093533182632], time: 36.757
steps: 122000, episodes: 122, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.34952069319240975], time: 37.09
steps: 123000, episodes: 123, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.3528822739247781], time: 37.502
steps: 124000, episodes: 124, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.053331059722707], time: 37.849
steps: 125000, episodes: 125, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-7.0, 9.228358453424766], time: 38.267
steps: 126000, episodes: 126, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.22704293646811], time: 38.539
steps: 127000, episodes: 127, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.331395984825559], time: 38.843
steps: 128000, episodes: 128, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [4.0, -1.7497599894039515], time: 39.134
steps: 129000, episodes: 129, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.5593102168414725], time: 39.66
steps: 130000, episodes: 130, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.298362338336122], time: 39.829
steps: 131000, episodes: 131, mean episode reward: 0.0, agent episode reward: [-15.0, 15.0], [-15.0, 17.144077805106523], time: 40.269
steps: 132000, episodes: 132, mean episode reward: 0.0, agent episode reward: [-9.0, 9.0], [-9.0, 11.331988221873369], time: 40.545
steps: 133000, episodes: 133, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-12.0, 14.423216092717643], time: 40.971
steps: 134000, episodes: 134, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.3230315096967558], time: 41.221
steps: 135000, episodes: 135, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.4363092242577085], time: 41.679
steps: 136000, episodes: 136, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-12.0, 14.31486362936938], time: 41.942
steps: 137000, episodes: 137, mean episode reward: 0.0, agent episode reward: [5.0, -5.0], [5.0, -2.5907162923730613], time: 42.485
steps: 138000, episodes: 138, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.41349356640684], time: 42.668
steps: 139000, episodes: 139, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.160187352980134], time: 42.943
steps: 140000, episodes: 140, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.350805987606259], time: 43.41
steps: 141000, episodes: 141, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.3200729823254163], time: 43.723
steps: 142000, episodes: 142, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.458150611725234], time: 44.094
steps: 143000, episodes: 143, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [3.0, -0.6885291137678913], time: 44.344
steps: 144000, episodes: 144, mean episode reward: 0.0, agent episode reward: [-9.0, 9.0], [-9.0, 11.695000423762957], time: 44.897
steps: 145000, episodes: 145, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [4.0, -1.8126556068310054], time: 44.979
steps: 146000, episodes: 146, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.338642121687912], time: 45.412
steps: 147000, episodes: 147, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.45939235246694576], time: 45.643
steps: 148000, episodes: 148, mean episode reward: 0.0, agent episode reward: [-19.0, 19.0], [-19.0, 21.11301893585992], time: 46.059
steps: 149000, episodes: 149, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.3042143992132732], time: 46.36
steps: 150000, episodes: 150, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.3476107258753003], time: 46.888
steps: 151000, episodes: 151, mean episode reward: 0.0, agent episode reward: [-19.0, 19.0], [-19.0, 21.33344812388409], time: 47.135
steps: 152000, episodes: 152, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.2794861611258148], time: 47.446
steps: 153000, episodes: 153, mean episode reward: 0.0, agent episode reward: [-20.0, 20.0], [-20.0, 22.187716318371205], time: 48.029
steps: 154000, episodes: 154, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.376350679166256], time: 48.033
steps: 155000, episodes: 155, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.2707266535308444], time: 48.443
steps: 156000, episodes: 156, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.2103668332937905], time: 49.574
steps: 157000, episodes: 157, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-7.0, 9.289528317798169], time: 50.639
steps: 158000, episodes: 158, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.324052932592482], time: 50.777
steps: 159000, episodes: 159, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-4.0, 6.157999961830959], time: 51.03
steps: 160000, episodes: 160, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.3006633221996062], time: 51.477
steps: 161000, episodes: 161, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.244776604864095], time: 51.24
steps: 162000, episodes: 162, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.3127406991284576], time: 51.556
steps: 163000, episodes: 163, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.3425705244217494], time: 51.637
steps: 164000, episodes: 164, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.3578908679849542], time: 51.912
steps: 165000, episodes: 165, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.3558962139179016], time: 52.423
steps: 166000, episodes: 166, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-6.0, 8.030835528284955], time: 52.723
steps: 167000, episodes: 167, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.2678971108413672], time: 53.169
steps: 168000, episodes: 168, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [-1.0, 3.2836189237019595], time: 53.379
steps: 169000, episodes: 169, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.473468252532203], time: 53.726
steps: 170000, episodes: 170, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-2.0, 4.206468809789618], time: 54.084
steps: 171000, episodes: 171, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [2.0, 0.667140483882933], time: 54.386
steps: 172000, episodes: 172, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-5.0, 7.253473975997985], time: 54.749
steps: 173000, episodes: 173, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-3.0, 5.197705449775055], time: 55.171
steps: 174000, episodes: 174, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.4239819688821753], time: 55.517
steps: 175000, episodes: 175, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.6297707888000117], time: 55.854
steps: 176000, episodes: 176, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.467972710798164], time: 56.415
steps: 177000, episodes: 177, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.4267492805514066], time: 56.494
steps: 178000, episodes: 178, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-12.0, 14.735160642497611], time: 56.695
steps: 179000, episodes: 179, mean episode reward: 0.0, agent episode reward: [-13.0, 13.0], [-13.0, 14.990700306100175], time: 57.09
steps: 180000, episodes: 180, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [1.0, 1.250182612158117], time: 57.554
steps: 181000, episodes: 181, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.132752719878704], time: 57.981
steps: 182000, episodes: 182, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.269652560206855], time: 58.241
steps: 183000, episodes: 183, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.065940972241921], time: 58.683
steps: 184000, episodes: 184, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.1868975775569095], time: 58.882
steps: 185000, episodes: 185, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.2356686316278074], time: 59.201
steps: 186000, episodes: 186, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-2.0, 4.279313963100358], time: 59.639
steps: 187000, episodes: 187, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.4213308792990618], time: 59.949
steps: 188000, episodes: 188, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.5342619433864986], time: 60.389
steps: 189000, episodes: 189, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.5694868321236632], time: 60.598
steps: 190000, episodes: 190, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [0.0, 2.276671589283137], time: 61.0192021-10-31 20:06:07.169576: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-31 20:06:07.173617: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-31 20:06:07.173794: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x561ddfc99a10 executing computations on platform Host. Devices:
2021-10-31 20:06:07.173811: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Box
adversary agents number is 1
Using good policy TD3 and adv policy TD3
Starting iterations...
steps: 1000, episodes: 1, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [7.836478947578966, -6.0], time: 5.844
steps: 2000, episodes: 2, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6338933843305069, 1.0], time: 9.963
steps: 3000, episodes: 3, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.069567111678837, -3.0], time: 8.516
steps: 4000, episodes: 4, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.1595195650294117, 4.0], time: 8.083
steps: 5000, episodes: 5, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9313600986015833, 0.0], time: 8.284
steps: 6000, episodes: 6, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [5.812416875795186, -4.0], time: 9.626
steps: 7000, episodes: 7, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [7.725384068613778, -6.0], time: 9.188
steps: 8000, episodes: 8, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.06291678227933829, 2.0], time: 9.646
steps: 9000, episodes: 9, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.094759645951342, 6.0], time: 9.428
steps: 10000, episodes: 10, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.162860900168756, 4.0], time: 10.073
steps: 11000, episodes: 11, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6837371159790697, 0.0], time: 10.371
steps: 12000, episodes: 12, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9514585870080654, 0.0], time: 10.332
steps: 13000, episodes: 13, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.067036167107159, -1.0], time: 10.338
steps: 14000, episodes: 14, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.23694640812576281, 2.0], time: 10.853
steps: 15000, episodes: 15, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [4.40033427474383, -2.0], time: 10.775
steps: 16000, episodes: 16, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.049805422938838, 6.0], time: 10.985
steps: 17000, episodes: 17, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.023692372494164, 0.0], time: 10.918
steps: 18000, episodes: 18, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.2183769207213646, 1.0], time: 11.115
steps: 19000, episodes: 19, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-4.534749995027255, 7.0], time: 11.433
steps: 20000, episodes: 20, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.2553092459878785, -1.0], time: 11.232
steps: 21000, episodes: 21, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.18508416299832, -6.0], time: 11.334
steps: 22000, episodes: 22, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.1727737054003515, 1.0], time: 11.893
steps: 23000, episodes: 23, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [4.457328907427372, -2.0], time: 11.609
steps: 24000, episodes: 24, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [6.3903126669983035, -4.0], time: 11.827
steps: 25000, episodes: 25, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.416196201571191, 1.0], time: 12.069
steps: 26000, episodes: 26, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.515459582566518, -3.0], time: 12.002
steps: 27000, episodes: 27, mean episode reward: 0.0, agent episode reward: [8.0, -8.0], [10.210638156098389, -8.0], time: 12.352
steps: 28000, episodes: 28, mean episode reward: 0.0, agent episode reward: [8.0, -8.0], [10.336608834208258, -8.0], time: 12.656
steps: 29000, episodes: 29, mean episode reward: 0.0, agent episode reward: [7.0, -7.0], [9.364764939723347, -7.0], time: 12.594
steps: 30000, episodes: 30, mean episode reward: 0.0, agent episode reward: [5.0, -5.0], [7.246676169079964, -5.0], time: 12.832
steps: 31000, episodes: 31, mean episode reward: 0.0, agent episode reward: [19.0, -19.0], [21.293939904530873, -19.0], time: 12.727
steps: 32000, episodes: 32, mean episode reward: 0.0, agent episode reward: [13.0, -13.0], [15.419031712081809, -13.0], time: 13.11
steps: 33000, episodes: 33, mean episode reward: 0.0, agent episode reward: [9.0, -9.0], [11.181508046671636, -9.0], time: 13.338
steps: 34000, episodes: 34, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.340701671146425, -6.0], time: 13.154
steps: 35000, episodes: 35, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.468167427685762, -1.0], time: 13.453
steps: 36000, episodes: 36, mean episode reward: 0.0, agent episode reward: [5.0, -5.0], [7.064142885603759, -5.0], time: 13.88
steps: 37000, episodes: 37, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.298778926116282, -1.0], time: 13.682
steps: 38000, episodes: 38, mean episode reward: 0.0, agent episode reward: [8.0, -8.0], [10.442216590563728, -8.0], time: 13.994
steps: 39000, episodes: 39, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.506478238343052, -6.0], time: 14.431
steps: 40000, episodes: 40, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.1010740604572646, 0.0], time: 14.364
steps: 41000, episodes: 41, mean episode reward: 0.0, agent episode reward: [8.0, -8.0], [10.107397099316797, -8.0], time: 14.688
steps: 42000, episodes: 42, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.22688640669739, -6.0], time: 14.716
steps: 43000, episodes: 43, mean episode reward: 0.0, agent episode reward: [11.0, -11.0], [13.181843064487031, -11.0], time: 14.944
steps: 44000, episodes: 44, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.4664063739006956, 1.0], time: 15.257
steps: 45000, episodes: 45, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.3285202088992625, -3.0], time: 15.087
steps: 46000, episodes: 46, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [4.354266058996989, -2.0], time: 15.58
steps: 47000, episodes: 47, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.7054898762865447, 1.0], time: 16.07
steps: 48000, episodes: 48, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.6409886937676137, -1.0], time: 16.051
steps: 49000, episodes: 49, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.6006754086951944, 1.0], time: 16.256
steps: 50000, episodes: 50, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.4806539971138437, -1.0], time: 16.732
steps: 51000, episodes: 51, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.2635201953113238, -1.0], time: 16.651
steps: 52000, episodes: 52, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.3060178444180766, 0.0], time: 16.863
steps: 53000, episodes: 53, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.1562832629049713, 0.0], time: 17.333
steps: 54000, episodes: 54, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9214284406745135, 0.0], time: 17.409
steps: 55000, episodes: 55, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.212464762342324, 0.0], time: 17.936
steps: 56000, episodes: 56, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.069915414209044, 0.0], time: 17.872
steps: 57000, episodes: 57, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.3141113197691006, 0.0], time: 18.154
steps: 58000, episodes: 58, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.082725395407206, 0.0], time: 18.58
steps: 59000, episodes: 59, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.1232307110765491, 1.0], time: 18.649
steps: 60000, episodes: 60, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.992586084367807, 0.0], time: 19.034
steps: 61000, episodes: 61, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.1576537935602498, 0.0], time: 19.601
steps: 62000, episodes: 62, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.1843533004777718, 0.0], time: 19.642
steps: 63000, episodes: 63, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9398523675265402, 0.0], time: 19.951
steps: 64000, episodes: 64, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.3494940279503487, -1.0], time: 20.397
steps: 65000, episodes: 65, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.060438825501113, 1.0], time: 20.56
steps: 66000, episodes: 66, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.056124099750298, -3.0], time: 21.088
steps: 67000, episodes: 67, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.9737413484407376, -1.0], time: 21.215
steps: 68000, episodes: 68, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.9128961076465716, -1.0], time: 21.39
steps: 69000, episodes: 69, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.9713170580408798, 3.0], time: 21.818
steps: 70000, episodes: 70, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-3.973629297321831, 6.0], time: 21.822
steps: 71000, episodes: 71, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.980774929714782, -1.0], time: 22.193
steps: 72000, episodes: 72, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.9803556967961573, 3.0], time: 22.74
steps: 73000, episodes: 73, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-6.107490680323584, 8.0], time: 23.033
steps: 74000, episodes: 74, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.0840891811679736, 0.0], time: 23.044
steps: 75000, episodes: 75, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.11726147557, 0.0], time: 23.541
steps: 76000, episodes: 76, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.171899846782154, 1.0], time: 23.652
steps: 77000, episodes: 77, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.074409855528828, -3.0], time: 23.814
steps: 78000, episodes: 78, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.347942401652324, 0.0], time: 24.893
steps: 79000, episodes: 79, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.359181436742024, 0.0], time: 25.4
steps: 80000, episodes: 80, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.978274124598116, -2.0], time: 25.287
steps: 81000, episodes: 81, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.4620070836260965, 0.0], time: 25.453
steps: 82000, episodes: 82, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [4.077550568266432, -2.0], time: 25.671
steps: 83000, episodes: 83, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.0811688317608, -6.0], time: 25.854
steps: 84000, episodes: 84, mean episode reward: 0.0, agent episode reward: [10.0, -10.0], [12.052322585298793, -10.0], time: 26.337
steps: 85000, episodes: 85, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9052986541109433, 0.0], time: 26.509
steps: 86000, episodes: 86, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.01627681984611678, 2.0], time: 26.888
steps: 87000, episodes: 87, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.3408231847203696, 2.0], time: 27.076
steps: 88000, episodes: 88, mean episode reward: 0.0, agent episode reward: [9.0, -9.0], [11.069045028907137, -9.0], time: 27.837
steps: 89000, episodes: 89, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.017278582463502, -6.0], time: 27.941
steps: 90000, episodes: 90, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.415923447950932, 0.0], time: 28.114
steps: 91000, episodes: 91, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.16397014022213688, 2.0], time: 28.285
steps: 92000, episodes: 92, mean episode reward: 0.0, agent episode reward: [10.0, -10.0], [12.151761622148628, -10.0], time: 28.791
steps: 93000, episodes: 93, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.4682657420917877, 0.0], time: 29.044
steps: 94000, episodes: 94, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.07622983001313507, 2.0], time: 29.499
steps: 95000, episodes: 95, mean episode reward: 0.0, agent episode reward: [12.0, -12.0], [14.071275795823793, -12.0], time: 29.803
steps: 96000, episodes: 96, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.144053599817189, 1.0], time: 30.324
steps: 97000, episodes: 97, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [6.023349003753365, -4.0], time: 28.997
steps: 98000, episodes: 98, mean episode reward: 0.0, agent episode reward: [7.0, -7.0], [9.014041451491185, -7.0], time: 29.823
steps: 99000, episodes: 99, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.86604649029013, 1.0], time: 29.575
steps: 100000, episodes: 100, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.8206213085783456, -2.0], time: 29.882
steps: 101000, episodes: 101, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.1598555485822435, 3.0], time: 30.021
steps: 102000, episodes: 102, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.8289901908548156, -2.0], time: 30.039
steps: 103000, episodes: 103, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.8225292047824778, 1.0], time: 30.163
steps: 104000, episodes: 104, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9229722419872437, 0.0], time: 30.196
steps: 105000, episodes: 105, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.7813193401281373, 0.0], time: 30.125
steps: 106000, episodes: 106, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.9620280507003707, -2.0], time: 30.068
steps: 107000, episodes: 107, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-5.120339219692247, 7.0], time: 30.169
steps: 108000, episodes: 108, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-9.284725482312297, 11.0], time: 29.99
steps: 109000, episodes: 109, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.680703236170372, -1.0], time: 30.024
steps: 110000, episodes: 110, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.8426666607180169, 0.0], time: 30.05
steps: 111000, episodes: 111, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.324672261662976, 0.0], time: 30.118
steps: 112000, episodes: 112, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.9733464659938003, -1.0], time: 30.109
steps: 113000, episodes: 113, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.7135300848751065, 0.0], time: 30.02
steps: 114000, episodes: 114, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.7785936698430178, 0.0], time: 29.975
steps: 115000, episodes: 115, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.7816419725246795, 0.0], time: 30.188
steps: 116000, episodes: 116, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.7399125919736995, 0.0], time: 30.06
steps: 117000, episodes: 117, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.7560346964129139, 0.0], time: 30.035
steps: 118000, episodes: 118, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.036225455943562385, 2.0], time: 30.032
steps: 119000, episodes: 119, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9718601229980324, 0.0], time: 30.03
steps: 120000, episodes: 120, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.776978050144811, 0.0], time: 29.919
steps: 121000, episodes: 121, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.8589593059900595, -2.0], time: 30.07
steps: 122000, episodes: 122, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.3597935786191901, 2.0], time: 29.963
steps: 123000, episodes: 123, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9715663476670546, 0.0], time: 29.905
steps: 124000, episodes: 124, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9885409720960776, 0.0], time: 29.915
steps: 125000, episodes: 125, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.423926267972737, 0.0], time: 30.048
steps: 126000, episodes: 126, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.6293582681203838, 3.0], time: 29.995
steps: 127000, episodes: 127, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.525351122883658, 0.0], time: 29.919
steps: 128000, episodes: 128, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.134954938250254, 1.0], time: 29.976
steps: 129000, episodes: 129, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.4357949379824744, 0.0], time: 29.987
steps: 130000, episodes: 130, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.506040938527998, 0.0], time: 30.043
steps: 131000, episodes: 131, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.150713191756318, 0.0], time: 29.988
steps: 132000, episodes: 132, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.468304752828854, 0.0], time: 29.897
steps: 133000, episodes: 133, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.46191979763258, 0.0], time: 30.111
steps: 134000, episodes: 134, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9349097318826247, 0.0], time: 30.016
steps: 135000, episodes: 135, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.967416544219322, 0.0], time: 30.019
steps: 136000, episodes: 136, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.404081204710478, 0.0], time: 30.138
steps: 137000, episodes: 137, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.4098981259865115, 0.0], time: 30.091
steps: 138000, episodes: 138, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.965003745697868, 0.0], time: 30.05
steps: 139000, episodes: 139, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.372042942280865, 0.0], time: 30.107
steps: 140000, episodes: 140, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.20290776459408, 0.0], time: 30.03
steps: 141000, episodes: 141, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.8374220339168001, 0.0], time: 30.102
steps: 142000, episodes: 142, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.8247276119909162, 0.0], time: 30.068
steps: 143000, episodes: 143, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6843848370417325, 0.0], time: 29.972
steps: 144000, episodes: 144, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.8819585872800944, 0.0], time: 29.931
steps: 145000, episodes: 145, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.3620614700425904, 0.0], time: 30.104
steps: 146000, episodes: 146, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.749119331558065, 0.0], time: 29.966
steps: 147000, episodes: 147, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6980718995462007, 0.0], time: 29.949
steps: 148000, episodes: 148, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6383285745733949, 0.0], time: 30.006
steps: 149000, episodes: 149, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.666894820331938, 0.0], time: 30.051
steps: 150000, episodes: 150, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6204274234962133, 0.0], time: 29.884
steps: 151000, episodes: 151, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.652553361235826, 0.0], time: 29.864
steps: 152000, episodes: 152, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.5702619315815781, 1.0], time: 29.963
steps: 153000, episodes: 153, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.660464774565439, 0.0], time: 29.833
steps: 154000, episodes: 154, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.5974577620289756, 0.0], time: 29.936
steps: 155000, episodes: 155, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.4285887236545447, 3.0], time: 30.1
steps: 156000, episodes: 156, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6536197251125608, 1.0], time: 29.917
steps: 157000, episodes: 157, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.336524216281957, 3.0], time: 30.007
steps: 158000, episodes: 158, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6167899067004787, 0.0], time: 29.97
steps: 159000, episodes: 159, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6569097107014525, 0.0], time: 29.937
steps: 160000, episodes: 160, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.2407479968934017, 3.0], time: 29.898
steps: 161000, episodes: 161, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [4.672569892848859, -3.0], time: 29.891
steps: 162000, episodes: 162, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.19612448713220262, 2.0], time: 29.932
steps: 163000, episodes: 163, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.5536772954392615, -2.0], time: 29.913
steps: 164000, episodes: 164, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.7115270782751903, 1.0], time: 29.844
steps: 165000, episodes: 165, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.3826975854051817, 2.0], time: 29.992
steps: 166000, episodes: 166, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.8406368443991505, 0.0], time: 29.797
steps: 167000, episodes: 167, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6098878357000297, 1.0], time: 29.774
steps: 168000, episodes: 168, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.1058239222513904, 2.0], time: 29.931
steps: 169000, episodes: 169, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.661141723467067, 0.0], time: 29.878
steps: 170000, episodes: 170, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.684674688709692, 0.0], time: 29.74
steps: 171000, episodes: 171, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [4.60143554559691, -3.0], time: 29.813
steps: 172000, episodes: 172, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.718200740718028, -1.0], time: 29.799
steps: 173000, episodes: 173, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.688266774761271, -1.0], time: 29.736
steps: 174000, episodes: 174, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.6527559118686534, -2.0], time: 29.82
steps: 175000, episodes: 175, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-3.249009972769241, 5.0], time: 29.939
steps: 176000, episodes: 176, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [4.195095473212419, -2.0], time: 29.893
steps: 177000, episodes: 177, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.6895550398675008, -1.0], time: 29.765
steps: 178000, episodes: 178, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [4.789678107030142, -3.0], time: 29.85
steps: 179000, episodes: 179, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.392162537722532, 4.0], time: 29.827
steps: 180000, episodes: 180, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.676345119351904, -2.0], time: 29.786
steps: 181000, episodes: 181, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.913439125312709, 3.0], time: 29.763
steps: 182000, episodes: 182, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.36498469149529245, 2.0], time: 29.757
steps: 183000, episodes: 183, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.08878784057815409, 2.0], time: 29.802
steps: 184000, episodes: 184, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.8782917702532425, -3.0], time: 29.618
steps: 185000, episodes: 185, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.085953018589851, 6.0], time: 29.783
steps: 186000, episodes: 186, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.7048393390994017, -2.0], time: 29.621
steps: 187000, episodes: 187, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.06324527219961186, 2.0], time: 29.748
steps: 188000, episodes: 188, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [4.247913395771664, -2.0], time: 29.796
steps: 189000, episodes: 189, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.7254927518068133, -1.0], time: 29.689
steps: 190000, episodes: 190, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.7178131393149445, -1.0], time: 29.623
steps: 191000, episodes: 191, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.8844989510425099, 0.0], time: 29.686
steps: 192000, episodes: 192, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-3.2434977971460683, 5.0], time: 29.502
steps: 193000, episodes: 193, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [4.7994291343666085, -3.0], time: 29.784
steps: 194000, episodes: 194, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.804963282284643, 1.0], time: 29.607
steps: 195000, episodes: 195, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.449726796698708, 0.0], time: 29.686
steps: 196000, episodes: 196, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-3.7686539455646675, 6.0], time: 29.572
steps: 197000, episodes: 197, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.3951631350918015, 2.0], time: 29.623
steps: 198000, episodes: 198, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.2845381802924958, 3.0], time: 29.591
steps: 199000, episodes: 199, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.890729582840668, -2.0], time: 29.509
steps: 200000, episodes: 200, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.1879358669729068, 0.0], time: 29.464
steps: 201000, episodes: 201, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.196480135521396, 0.0], time: 29.568
steps: 202000, episodes: 202, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.8985780257669687, 0.0], time: 29.515
steps: 203000, episodes: 203, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.0101950500681625, 0.0], time: 29.604
steps: 204000, episodes: 204, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.634538827643097, -2.0], time: 29.508
steps: 205000, episodes: 205, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-2.971413284472844, 5.0], time: 29.476
steps: 206000, episodes: 206, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-5.211900014045313, 7.0], time: 29.555
steps: 207000, episodes: 207, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.45936825383094, 0.0], time: 29.482
steps: 208000, episodes: 208, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.773834746444816, -3.0], time: 29.478
steps: 209000, episodes: 209, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.8283174474107663, 1.0], time: 29.455
steps: 210000, episodes: 210, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.246575252557517, 3.0], time: 29.52
steps: 211000, episodes: 211, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [4.112508519461196, -2.0], time: 29.482
steps: 212000, episodes: 212, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-3.2261440376250112, 5.0], time: 29.444
steps: 213000, episodes: 213, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6273646125901136, 0.0], time: 29.45
steps: 214000, episodes: 214, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.0426197311791867, 4.0], time: 29.387
steps: 215000, episodes: 215, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.05476688805585945, 2.0], time: 29.761
steps: 216000, episodes: 216, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.9474420981000325, 1.0], time: 29.526
steps: 217000, episodes: 217, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.4262595764712046, 2.0], time: 29.492
steps: 218000, episodes: 218, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [7.835777677916673, -6.0], time: 29.409
steps: 219000, episodes: 219, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.0593301880318275, 6.0], time: 29.508
steps: 220000, episodes: 220, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.0481559543522674, 0.0], time: 29.605
steps: 221000, episodes: 221, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-3.789862251356453, 6.0], time: 30.789
steps: 222000, episodes: 222, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.0522510676251163, 2.0], time: 30.839
steps: 223000, episodes: 223, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9765096121958292, 0.0], time: 30.537
steps: 224000, episodes: 224, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.1386910709551064, 0.0], time: 30.55
steps: 225000, episodes: 225, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-1.9854002198844154, 5.0], time: 30.682
steps: 226000, episodes: 226, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.716254591182556, 1.0], time: 30.431
steps: 227000, episodes: 227, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-1.7128532899973992, 4.0], time: 30.535
steps: 228000, episodes: 228, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.001342625627766, -1.0], time: 30.496
steps: 229000, episodes: 229, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.7965198494630134, 3.0], time: 30.483
steps: 230000, episodes: 230, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.0919216430056267, 0.0], time: 30.622
steps: 231000, episodes: 231, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.256641022302451, 6.0], time: 30.553
steps: 232000, episodes: 232, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [2.0729276606544507, 1.0], time: 30.655
steps: 233000, episodes: 233, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.219549323248675, -6.0], time: 30.491
steps: 234000, episodes: 234, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6932583272448086, 1.0], time: 30.599
steps: 235000, episodes: 235, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.24030504761332008, 2.0], time: 30.639
steps: 236000, episodes: 236, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.2070496277512894, 3.0], time: 30.512
steps: 237000, episodes: 237, mean episode reward: 0.0, agent episode reward: [-19.0, 19.0], [-16.609096089852883, 19.0], time: 30.6
steps: 238000, episodes: 238, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.950587661157346, -1.0], time: 30.61
steps: 239000, episodes: 239, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.791998026296414, -1.0], time: 30.584
steps: 240000, episodes: 240, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-1.7530604305864657, 4.0], time: 30.57
steps: 241000, episodes: 241, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.6695568856136966, -2.0], time: 30.775
steps: 242000, episodes: 242, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [7.871726774838288, -6.0], time: 30.576
steps: 243000, episodes: 243, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.4978604823914865, 2.0], time: 30.707
steps: 244000, episodes: 244, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-2.8570177838098414, 5.0], time: 30.653
steps: 245000, episodes: 245, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.775340704317597, 1.0], time: 30.708
steps: 246000, episodes: 246, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.1916584383982642, 3.0], time: 30.474
steps: 247000, episodes: 247, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6976823049155023, 1.0], time: 30.639
steps: 248000, episodes: 248, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.6540118514832788, 2.0], time: 30.518
steps: 249000, episodes: 249, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9580633296887946, 0.0], time: 30.549
steps: 250000, episodes: 250, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [5.5937388738834795, -4.0], time: 30.54
steps: 251000, episodes: 251, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.42043600822907906, 2.0], time: 30.675
steps: 252000, episodes: 252, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.18166097834533781, 2.0], time: 30.547
steps: 253000, episodes: 253, mean episode reward: 0.0, agent episode reward: [11.0, -11.0], [12.980574152109536, -11.0], time: 30.586
steps: 254000, episodes: 254, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.247562693003573, -1.0], time: 30.564
steps: 255000, episodes: 255, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-3.2273588747497484, 5.0], time: 30.562
steps: 256000, episodes: 256, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.09479546210924, 4.0], time: 30.54
steps: 257000, episodes: 257, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.2765381170888011, 2.0], time: 30.57
steps: 258000, episodes: 258, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.5415313998960556, 0.0], time: 30.655
steps: 259000, episodes: 259, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-8.901809303697723, 12.0], time: 30.558
steps: 260000, episodes: 260, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-3.9721270955540895, 6.0], time: 30.593
steps: 261000, episodes: 261, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.1801598448995234, -1.0], time: 30.631
steps: 262000, episodes: 262, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.809287540899245, -2.0], time: 30.714
steps: 263000, episodes: 263, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [8.274653379008912, -6.0], time: 30.551
steps: 264000, episodes: 264, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.4734127074116868, 2.0], time: 30.642
steps: 265000, episodes: 265, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.3400360754451154, 2.0], time: 30.628
steps: 266000, episodes: 266, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [3.0406219687591243, -1.0], time: 30.582
steps: 267000, episodes: 267, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.104357540393647, -3.0], time: 30.688
steps: 268000, episodes: 268, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [5.8140151253243015, -3.0], time: 30.552
steps: 269000, episodes: 269, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.2808840807387385, 2.0], time: 30.581
steps: 270000, episodes: 270, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-1.7335843523787804, 4.0], time: 30.579
steps: 271000, episodes: 271, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.007718676284516203, 2.0], time: 30.622
steps: 272000, episodes: 272, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.4879813989869346, 4.0], time: 30.568
steps: 273000, episodes: 273, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.7698673302259547, 0.0], time: 30.663
steps: 274000, episodes: 274, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.07804093357354738, 2.0], time: 30.696
steps: 275000, episodes: 275, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6342133981217114, 1.0], time: 30.871
steps: 276000, episodes: 276, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-6.206787867449681, 8.0], time: 30.532
steps: 277000, episodes: 277, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.207044052264904, 0.0], time: 30.643
steps: 278000, episodes: 278, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.4100451534128755, 4.0], time: 30.648
steps: 279000, episodes: 279, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.64834628982376, 0.0], time: 30.624
steps: 280000, episodes: 280, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.112685669440276, 6.0], time: 30.656
steps: 281000, episodes: 281, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6524646164710299, 1.0], time: 30.611
steps: 282000, episodes: 282, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.7887699747919634, -2.0], time: 30.592
steps: 283000, episodes: 283, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.7945766262819202, 3.0], time: 30.564
steps: 284000, episodes: 284, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.509971341569884, -1.0], time: 30.603
steps: 285000, episodes: 285, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.516299282531832, 2.0], time: 30.553
steps: 286000, episodes: 286, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [0.15841623642024644, 2.0], time: 30.697
steps: 287000, episodes: 287, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.2369472150126906, 4.0], time: 30.636
steps: 288000, episodes: 288, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.032143963406086974, 2.0], time: 30.572
steps: 289000, episodes: 289, mean episode reward: 0.0, agent episode reward: [-17.0, 17.0], [-15.106956530258598, 17.0], time: 30.671
steps: 290000, episodes: 290, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.5210934705940039, 2.0], time: 30.553
steps: 291000, episodes: 291, mean episode reward: 0.0, agent episode reward: [-15.0, 15.0], [-12.829494203447952, 15.0], time: 30.531
steps: 292000, episodes: 292, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.5277952595594697, -2.0], time: 30.578
steps: 293000, episodes: 293, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.29275100512115004, 2.0], time: 30.648
steps: 294000, episodes: 294, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-5.712096327808706, 8.0], time: 30.648
steps: 295000, episodes: 295, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-8.978189620397114, 11.0], time: 30.638
steps: 296000, episodes: 296, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.41146742283847965, 2.0], time: 30.698
steps: 297000, episodes: 297, mean episode reward: 0.0, agent episode reward: [-29.0, 29.0], [-26.925374043541037, 29.0], time: 30.548
steps: 298000, episodes: 298, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.579490157810043, -2.0], time: 30.708
steps: 299000, episodes: 299, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [4.6859261079660985, -3.0], time: 30.651
steps: 300000, episodes: 300, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.032404763587748, 6.0], time: 30.836
steps: 301000, episodes: 301, mean episode reward: 0.0, agent episode reward: [-13.0, 13.0], [-9.911089640991356, 13.0], time: 30.67
steps: 302000, episodes: 302, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-2.9740202930000814, 6.0], time: 30.761
steps: 303000, episodes: 303, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-5.413510851906544, 8.0], time: 30.645
steps: 304000, episodes: 304, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.135051197127602, 6.0], time: 30.807
steps: 305000, episodes: 305, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.609422603594201, 0.0], time: 30.937
steps: 306000, episodes: 306, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.3481205523031634, 4.0], time: 30.595
steps: 307000, episodes: 307, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.253558188737508, 4.0], time: 30.67
steps: 308000, episodes: 308, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.14947461944372745, 2.0], time: 30.709
steps: 309000, episodes: 309, mean episode reward: 0.0, agent episode reward: [5.0, -5.0], [6.626539058722857, -5.0], time: 30.695
steps: 310000, episodes: 310, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.5979626479002711, 1.0], time: 30.843
steps: 311000, episodes: 311, mean episode reward: 0.0, agent episode reward: [-27.0, 27.0], [-25.04474123253902, 27.0], time: 30.744
steps: 312000, episodes: 312, mean episode reward: 0.0, agent episode reward: [-13.0, 13.0], [-11.359253040666811, 13.0], time: 30.837
steps: 313000, episodes: 313, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-2.9596735576443485, 5.0], time: 30.914
steps: 314000, episodes: 314, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.3158394344572266, 4.0], time: 30.781
steps: 315000, episodes: 315, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.4199862574621704, 4.0], time: 30.769
steps: 316000, episodes: 316, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.5983067244989373, -1.0], time: 30.897
steps: 317000, episodes: 317, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.0161675346602703, 1.0], time: 30.899
steps: 318000, episodes: 318, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.8296558252685865, 3.0], time: 30.725
steps: 319000, episodes: 319, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.7476954194021683, -1.0], time: 30.758
steps: 320000, episodes: 320, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.393904735287281, 6.0], time: 30.967
steps: 321000, episodes: 321, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-3.407611953687412, 5.0], time: 30.798
steps: 322000, episodes: 322, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-0.7555667714951837, 3.0], time: 30.818
steps: 323000, episodes: 323, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.9441090456715202, 0.0], time: 30.696
steps: 324000, episodes: 324, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-5.294288739392044, 7.0], time: 30.75
steps: 325000, episodes: 325, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.5472980478456735, 0.0], time: 30.897
steps: 326000, episodes: 326, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.541255724272743, -2.0], time: 30.684
steps: 327000, episodes: 327, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [7.652606049478876, -6.0], time: 30.793
steps: 328000, episodes: 328, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6315487113778953, 0.0], time: 30.71
steps: 329000, episodes: 329, mean episode reward: 0.0, agent episode reward: [-20.0, 20.0], [-18.18747589737967, 20.0], time: 30.812
steps: 330000, episodes: 330, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-3.820201822426487, 6.0], time: 30.693
steps: 331000, episodes: 331, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.6029245476473575, -2.0], time: 30.831
steps: 332000, episodes: 332, mean episode reward: 0.0, agent episode reward: [-26.0, 26.0], [-24.13085831058591, 26.0], time: 30.684
steps: 333000, episodes: 333, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6247663345862802, 1.0], time: 31.035
steps: 334000, episodes: 334, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.4821017604794924, 4.0], time: 30.911
steps: 335000, episodes: 335, mean episode reward: 0.0, agent episode reward: [-6.0, 6.0], [-4.367155029178977, 6.0], time: 30.836
steps: 336000, episodes: 336, mean episode reward: 0.0, agent episode reward: [-10.0, 10.0], [-8.434093512492177, 10.0], time: 30.827
steps: 337000, episodes: 337, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.641711944696628, -1.0], time: 30.694
steps: 338000, episodes: 338, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.4714389965991718, 1.0], time: 30.846
steps: 339000, episodes: 339, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.544851567620998, 4.0], time: 30.788
steps: 340000, episodes: 340, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.610764370025492, -1.0], time: 30.793
steps: 341000, episodes: 341, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.643410834532631, -2.0], time: 30.941
steps: 342000, episodes: 342, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.4851425241915672, 3.0], time: 30.823
steps: 343000, episodes: 343, mean episode reward: 0.0, agent episode reward: [-31.0, 31.0], [-29.225189401268626, 31.0], time: 30.731
steps: 344000, episodes: 344, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [2.0426910676057557, 0.0], time: 30.782
steps: 345000, episodes: 345, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.45602493330525246, 2.0], time: 30.701
steps: 346000, episodes: 346, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.5861292268995664, -1.0], time: 30.77
steps: 347000, episodes: 347, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-2.6556795167996086, 5.0], time: 30.705
steps: 348000, episodes: 348, mean episode reward: 0.0, agent episode reward: [6.0, -6.0], [7.5139525555955196, -6.0], time: 30.762
steps: 349000, episodes: 349, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-5.222810593227519, 7.0], time: 30.755
steps: 350000, episodes: 350, mean episode reward: 0.0, agent episode reward: [5.0, -5.0], [6.578734661616461, -5.0], time: 30.677
steps: 351000, episodes: 351, mean episode reward: 0.0, agent episode reward: [-5.0, 5.0], [-2.6437574238231916, 5.0], time: 30.572
steps: 352000, episodes: 352, mean episode reward: 0.0, agent episode reward: [3.0, -3.0], [4.67170301677884, -3.0], time: 30.782
steps: 353000, episodes: 353, mean episode reward: 0.0, agent episode reward: [-12.0, 12.0], [-10.299564991225665, 12.0], time: 30.669
steps: 354000, episodes: 354, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.2122737885436194, 4.0], time: 30.735
steps: 355000, episodes: 355, mean episode reward: 0.0, agent episode reward: [-7.0, 7.0], [-5.403396329576505, 7.0], time: 30.705
steps: 356000, episodes: 356, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.3730673279228767, 4.0], time: 30.814
steps: 357000, episodes: 357, mean episode reward: 0.0, agent episode reward: [-8.0, 8.0], [-6.265232035439983, 8.0], time: 30.711
steps: 358000, episodes: 358, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.4713810522900657, 1.0], time: 30.739
steps: 359000, episodes: 359, mean episode reward: 0.0, agent episode reward: [-4.0, 4.0], [-2.2003454728533263, 4.0], time: 30.743
steps: 360000, episodes: 360, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.5503829851671638, 2.0], time: 30.841
steps: 361000, episodes: 361, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.5541935842289782, -2.0], time: 30.742
steps: 362000, episodes: 362, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-9.000085094966375, 11.0], time: 30.631
steps: 363000, episodes: 363, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [1.1613731269869232, 1.0], time: 30.774
steps: 364000, episodes: 364, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.4518654193093091, 1.0], time: 30.706
steps: 365000, episodes: 365, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.11540625136798241, 2.0], time: 30.666
steps: 366000, episodes: 366, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.35681839681344035, 2.0], time: 30.867
steps: 367000, episodes: 367, mean episode reward: 0.0, agent episode reward: [2.0, -2.0], [3.577360421315129, -2.0], time: 30.61
steps: 368000, episodes: 368, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.34982950668464613, 2.0], time: 30.765
steps: 369000, episodes: 369, mean episode reward: 0.0, agent episode reward: [-9.0, 9.0], [-6.91443387522955, 9.0], time: 30.685
steps: 370000, episodes: 370, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.5560185002578901, 0.0], time: 30.678
steps: 371000, episodes: 371, mean episode reward: 0.0, agent episode reward: [4.0, -4.0], [5.593911230388677, -4.0], time: 30.669
steps: 372000, episodes: 372, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.5358654033940047, -1.0], time: 30.612
steps: 373000, episodes: 373, mean episode reward: 0.0, agent episode reward: [-2.0, 2.0], [-0.530519072322398, 2.0], time: 30.562
steps: 374000, episodes: 374, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [1.6953992046061712, 0.0], time: 30.549
steps: 375000, episodes: 375, mean episode reward: 0.0, agent episode reward: [-3.0, 3.0], [-1.46020576538203, 3.0], time: 30.634
steps: 376000, episodes: 376, mean episode reward: 0.0, agent episode reward: [-11.0, 11.0], [-9.332504908663484, 11.0], time: 30.723
steps: 377000, episodes: 377, mean episode reward: 0.0, agent episode reward: [1.0, -1.0], [2.8571887529327418, -1.0], time: 30.637
steps: 378000, episodes: 378, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.5870182543990056, 1.0], time: 30.713
steps: 379000, episodes: 379, mean episode reward: 0.0, agent episode reward: [-1.0, 1.0], [0.6508102779123575, 1.0], time: 30.6832021-10-31 23:36:11.876712: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-10-31 23:36:11.880618: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-10-31 23:36:11.880755: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x561b6842e790 executing computations on platform Host. Devices:
2021-10-31 23:36:11.880768: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice
  keepdims=keepdims)
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
env is  Mario_Bros
adversary agents number is 0
Using good policy TD3 and adv policy TD3
Starting iterations...
Traceback (most recent call last):
  File "maddpg_impl/experiments/train.py", line 418, in <module>
    train(arglist)
  File "maddpg_impl/experiments/train.py", line 259, in train
    intrinsic_reward_ag = embedding_model_ag.compute_intrinsic_reward(episodic_memory_ag, next_state_emb_ag,new_obs_tensor)
  File "/home/seth/RL/multiagent-RL/maddpg_impl/reward_shaping/embedding_model.py", line 145, in compute_intrinsic_reward
    long_loss = nn.MSELoss()(self.long_embedding(new_obs_tensor),self.long_stable_embedding(new_obs_tensor))
  File "/home/seth/RL/multiagent-RL/maddpg_impl/reward_shaping/embedding_model.py", line 53, in long_embedding
    x = F.relu(self.long_fc(x))
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 91, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/torch/nn/functional.py", line 1676, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
2021-12-06 00:36:49.687062: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-12-06 00:36:49.731552: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-12-06 00:36:49.732207: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55f98be81cb0 executing computations on platform Host. Devices:
2021-12-06 00:36:49.732295: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Mario_Bros
adversary agents number is 1
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 1942, episodes: 1, mean episode reward: 3200.0, agent episode reward: [3200.0, 0.0], [65340.22510013864, 0.0], time: 31.754
steps: 3054, episodes: 2, mean episode reward: 2400.0, agent episode reward: [0.0, 2400.0], [28996.981275098165, 2400.0], time: 15.545
steps: 4135, episodes: 3, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [28791.40110398056, 0.0], time: 14.078
steps: 4817, episodes: 4, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [18557.748426296275, 0.0], time: 10.325
steps: 5636, episodes: 5, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [21084.41786783244, 0.0], time: 10.893
steps: 7190, episodes: 6, mean episode reward: 1600.0, agent episode reward: [800.0, 800.0], [31613.214791676746, 800.0], time: 22.09
steps: 7933, episodes: 7, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [15114.405205313575, 0.0], time: 10.652
StopIteration()
steps: 9797, episodes: 8, mean episode reward: 3200.0, agent episode reward: [0.0, 3200.0], [36651.26096459916, 3200.0], time: 30.697
steps: 10613, episodes: 9, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [23558.843719130215, 0.0], time: 13.851
steps: 12003, episodes: 10, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [36192.135343049114, 0.0], time: 21.483
steps: 13126, episodes: 11, mean episode reward: 1600.0, agent episode reward: [0.0, 1600.0], [28261.422758136403, 1600.0], time: 17.962
steps: 13735, episodes: 12, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [16576.67247482214, 0.0], time: 6.616
steps: 15166, episodes: 13, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [35442.24605478855, 0.0], time: 24.387
steps: 16920, episodes: 14, mean episode reward: 1600.0, agent episode reward: [0.0, 1600.0], [43774.380514504475, 1600.0], time: 30.917
steps: 17824, episodes: 15, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [22258.181688405137, 0.0], time: 15.414
steps: 19010, episodes: 16, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [29138.000705943756, 0.0], time: 18.864
steps: 19803, episodes: 17, mean episode reward: 1600.0, agent episode reward: [1600.0, 0.0], [21477.652955207643, 0.0], time: 14.032
steps: 20427, episodes: 18, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [16250.29393687848, 0.0], time: 9.989
steps: 21705, episodes: 19, mean episode reward: 1600.0, agent episode reward: [1600.0, 0.0], [18097.644635817116, 0.0], time: 19.909
steps: 23422, episodes: 20, mean episode reward: 1600.0, agent episode reward: [800.0, 800.0], [45331.97871551905, 800.0], time: 31.026
steps: 24334, episodes: 21, mean episode reward: 800.0, agent episode reward: [0.0, 800.0], [23582.343623024135, 800.0], time: 14.412
steps: 25201, episodes: 22, mean episode reward: 3200.0, agent episode reward: [2400.0, 800.0], [25612.985545553685, 800.0], time: 14.151
steps: 26422, episodes: 23, mean episode reward: 1600.0, agent episode reward: [800.0, 800.0], [21301.78360048479, 800.0], time: 19.401
steps: 27377, episodes: 24, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [18546.156692189572, 0.0], time: 15.819
steps: 29145, episodes: 25, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [45853.36753175288, 0.0], time: 30.641
steps: 30425, episodes: 26, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [34704.81355470373, 0.0], time: 20.185
steps: 31400, episodes: 27, mean episode reward: 3200.0, agent episode reward: [1600.0, 1600.0], [29652.17661104671, 1600.0], time: 15.969
steps: 32877, episodes: 28, mean episode reward: 1600.0, agent episode reward: [1600.0, 0.0], [40566.59672083842, 0.0], time: 23.698
steps: 34158, episodes: 29, mean episode reward: 3200.0, agent episode reward: [0.0, 3200.0], [9506.307351470377, 3200.0], time: 34.676
steps: 35393, episodes: 30, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [33053.1721420106, 0.0], time: 19.913
StopIteration()
steps: 37393, episodes: 31, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [28462.994454248936, 0.0], time: 36.237
steps: 38396, episodes: 32, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [27419.683485733814, 0.0], time: 15.913
steps: 39638, episodes: 33, mean episode reward: 4000.0, agent episode reward: [800.0, 3200.0], [26442.877986157604, 3200.0], time: 20.184
steps: 40690, episodes: 34, mean episode reward: 800.0, agent episode reward: [800.0, 0.0], [24425.544148464684, 0.0], time: 17.47
steps: 41688, episodes: 35, mean episode reward: 2400.0, agent episode reward: [2400.0, 0.0], [31402.483660899197, 0.0], time: 16.148
steps: 43024, episodes: 36, mean episode reward: 3200.0, agent episode reward: [1600.0, 1600.0], [32449.980847699626, 1600.0], time: 21.327
StopIteration()
steps: 45024, episodes: 37, mean episode reward: 1600.0, agent episode reward: [800.0, 800.0], [56505.138851754666, 800.0], time: 37.084
steps: 45773, episodes: 38, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [21815.30757079681, 0.0], time: 12.053
steps: 47103, episodes: 39, mean episode reward: 3200.0, agent episode reward: [0.0, 3200.0], [13813.941205306195, 3200.0], time: 21.72
steps: 47830, episodes: 40, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [13742.471840718834, 0.0], time: 12.806
steps: 49028, episodes: 41, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [34113.80422068309, 0.0], time: 26.944
steps: 50416, episodes: 42, mean episode reward: 1600.0, agent episode reward: [1600.0, 0.0], [41140.34951716361, 0.0], time: 22.548
steps: 51100, episodes: 43, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [20297.39993232985, 0.0], time: 19.488
steps: 51955, episodes: 44, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [25028.896539262536, 0.0], time: 13.508
steps: 52966, episodes: 45, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [29802.18178982777, 0.0], time: 15.577
StopIteration()
steps: 54966, episodes: 46, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58138.434533750835, 0.0], time: 37.066
steps: 56269, episodes: 47, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [26256.5507545355, 0.0], time: 21.664
steps: 57331, episodes: 48, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [30893.1303496829, 0.0], time: 17.33
steps: 58395, episodes: 49, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [31456.108164946047, 0.0], time: 17.735
steps: 59484, episodes: 50, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [32757.775941027518, 0.0], time: 17.228
steps: 61484, episodes: 51, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58212.702186336115, 0.0], time: 38.012
steps: 63484, episodes: 52, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57605.778416002, 0.0], time: 36.833
steps: 65106, episodes: 53, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [47871.791476356644, 0.0], time: 29.409
steps: 67106, episodes: 54, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57285.69834664172, 0.0], time: 37.004
steps: 68338, episodes: 55, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [37065.58613274353, 0.0], time: 21.906
steps: 70338, episodes: 56, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57699.74013030601, 0.0], time: 36.725
steps: 72338, episodes: 57, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58572.877056604855, 0.0], time: 38.86
StopIteration()
steps: 74338, episodes: 58, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56904.044023198454, 0.0], time: 38.043
steps: 76338, episodes: 59, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59176.48617235958, 0.0], time: 37.385
steps: 78338, episodes: 60, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59916.387587552, 0.0], time: 38.415
steps: 80338, episodes: 61, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60096.77144527816, 0.0], time: 38.551
steps: 82338, episodes: 62, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58795.08427256015, 0.0], time: 36.99
steps: 84338, episodes: 63, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58407.55867690255, 0.0], time: 38.692
steps: 86338, episodes: 64, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59256.20181983673, 0.0], time: 37.217
steps: 87754, episodes: 65, mean episode reward: 1600.0, agent episode reward: [0.0, 1600.0], [46452.86252316329, 1600.0], time: 25.186
steps: 89754, episodes: 66, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60149.488014666844, 0.0], time: 37.483
steps: 91754, episodes: 67, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59321.471404988275, 0.0], time: 38.056
steps: 93754, episodes: 68, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59530.39745453642, 0.0], time: 38.142
steps: 95754, episodes: 69, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59699.183651369895, 0.0], time: 37.744
steps: 97754, episodes: 70, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56605.02351193996, 0.0], time: 38.613
steps: 99754, episodes: 71, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56038.43858644887, 0.0], time: 38.896
steps: 101754, episodes: 72, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57525.84538251049, 0.0], time: 37.284
steps: 103754, episodes: 73, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55814.95886509314, 0.0], time: 38.685
steps: 105754, episodes: 74, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56834.3316464592, 0.0], time: 38.417
steps: 107754, episodes: 75, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53578.2430203879, 0.0], time: 37.7
steps: 109754, episodes: 76, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56907.41799580443, 0.0], time: 38.018
steps: 111754, episodes: 77, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57381.097666369926, 0.0], time: 37.925
steps: 113754, episodes: 78, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56871.85164155342, 0.0], time: 38.51
steps: 115754, episodes: 79, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56950.43108402797, 0.0], time: 38.518
steps: 117754, episodes: 80, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55811.18998439659, 0.0], time: 37.81
steps: 119754, episodes: 81, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56400.91627043472, 0.0], time: 38.589
steps: 121754, episodes: 82, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57106.89568657156, 0.0], time: 38.943
steps: 123754, episodes: 83, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57577.81626106441, 0.0], time: 37.166
steps: 125754, episodes: 84, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54649.705022300375, 0.0], time: 39.586
steps: 127754, episodes: 85, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57862.68418615192, 0.0], time: 38.471
steps: 129754, episodes: 86, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57534.41289790956, 0.0], time: 37.909
steps: 131754, episodes: 87, mean episode reward: 1600.0, agent episode reward: [800.0, 800.0], [55946.08181313507, 800.0], time: 38.854
steps: 133754, episodes: 88, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58094.39840847203, 0.0], time: 39.334
steps: 135754, episodes: 89, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57973.584369958444, 0.0], time: 37.536
steps: 137754, episodes: 90, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56612.266738548424, 0.0], time: 39.493
steps: 139754, episodes: 91, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57821.03029177332, 0.0], time: 33.913
steps: 141754, episodes: 92, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57256.402540596086, 0.0], time: 38.633
steps: 143754, episodes: 93, mean episode reward: 1600.0, agent episode reward: [0.0, 1600.0], [59670.22547296937, 1600.0], time: 39.178
steps: 145754, episodes: 94, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57971.02441822686, 0.0], time: 37.311
steps: 147754, episodes: 95, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58227.36104261437, 0.0], time: 39.233
steps: 149754, episodes: 96, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58490.95012655502, 0.0], time: 38.692
steps: 151754, episodes: 97, mean episode reward: 1600.0, agent episode reward: [800.0, 800.0], [59439.38595551021, 800.0], time: 38.052
steps: 153754, episodes: 98, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58039.41598882503, 0.0], time: 38.385
steps: 155754, episodes: 99, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58170.2306684564, 0.0], time: 38.832
steps: 157754, episodes: 100, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57211.372877380825, 0.0], time: 33.855
steps: 159754, episodes: 101, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59809.26247251367, 0.0], time: 39.23
steps: 161754, episodes: 102, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58344.581472033824, 0.0], time: 38.253
steps: 163754, episodes: 103, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58914.675616838234, 0.0], time: 38.508
steps: 165754, episodes: 104, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58671.52040813768, 0.0], time: 39.164
steps: 167754, episodes: 105, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58353.3914267107, 0.0], time: 37.739
steps: 169754, episodes: 106, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58772.301745231685, 0.0], time: 39.15
steps: 171754, episodes: 107, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57786.678267573276, 0.0], time: 38.534
steps: 173754, episodes: 108, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59232.883416376135, 0.0], time: 38.347
steps: 175754, episodes: 109, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58466.79467656692, 0.0], time: 38.785
steps: 177754, episodes: 110, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58530.14815017931, 0.0], time: 38.901
steps: 179754, episodes: 111, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55899.28899606143, 0.0], time: 37.786
steps: 181754, episodes: 112, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59094.17747830768, 0.0], time: 39.184
steps: 183754, episodes: 113, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58190.259293330506, 0.0], time: 37.64
steps: 185754, episodes: 114, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57695.07408074094, 0.0], time: 38.91
steps: 187754, episodes: 115, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58714.711316385015, 0.0], time: 38.639
steps: 189754, episodes: 116, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59011.121987429455, 0.0], time: 38.138
steps: 191754, episodes: 117, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58939.337674901595, 0.0], time: 39.447
steps: 193754, episodes: 118, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59829.41071385555, 0.0], time: 39.51
steps: 195754, episodes: 119, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59666.030919674246, 0.0], time: 37.646
steps: 197754, episodes: 120, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57875.9488065799, 0.0], time: 39.295
steps: 199754, episodes: 121, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59315.46320392789, 0.0], time: 38.609
steps: 201754, episodes: 122, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59591.96678808866, 0.0], time: 38.019
steps: 203754, episodes: 123, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59084.384303398576, 0.0], time: 38.734
steps: 205754, episodes: 124, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59959.206333175156, 0.0], time: 39.445
steps: 207754, episodes: 125, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58623.53259669097, 0.0], time: 37.537
steps: 209754, episodes: 126, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59531.04740485288, 0.0], time: 39.046
steps: 211754, episodes: 127, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59232.24836751859, 0.0], time: 37.723
steps: 213754, episodes: 128, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59397.47571227734, 0.0], time: 38.701
steps: 215754, episodes: 129, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61421.09876990646, 0.0], time: 38.729
steps: 217754, episodes: 130, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59505.88363628918, 0.0], time: 38.643
steps: 219754, episodes: 131, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59657.134907049294, 0.0], time: 38.714
steps: 221754, episodes: 132, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59685.570440901385, 0.0], time: 39.438
steps: 223754, episodes: 133, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58520.040203782526, 0.0], time: 38.093
steps: 225754, episodes: 134, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60429.436664829744, 0.0], time: 35.098
steps: 227754, episodes: 135, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59667.42297504035, 0.0], time: 39.259
steps: 229754, episodes: 136, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60356.91230485312, 0.0], time: 37.647
steps: 231754, episodes: 137, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59809.795951986904, 0.0], time: 39.212
steps: 233754, episodes: 138, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55399.96271850063, 0.0], time: 37.691
steps: 235754, episodes: 139, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59861.69150943724, 0.0], time: 38.622
steps: 237754, episodes: 140, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60681.43936197406, 0.0], time: 38.724
steps: 239754, episodes: 141, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59199.98103147279, 0.0], time: 38.257
steps: 241754, episodes: 142, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55455.90344828742, 0.0], time: 39.301
steps: 243754, episodes: 143, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59591.54366260824, 0.0], time: 39.171
steps: 245754, episodes: 144, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60383.34970347871, 0.0], time: 37.846
steps: 247754, episodes: 145, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59326.99184444408, 0.0], time: 39.526
steps: 249754, episodes: 146, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59122.890678830365, 0.0], time: 38.772
steps: 251754, episodes: 147, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61257.98676450765, 0.0], time: 38.212
steps: 253754, episodes: 148, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55161.57185252621, 0.0], time: 39.137
steps: 255754, episodes: 149, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60328.07028168015, 0.0], time: 39.412
steps: 257754, episodes: 150, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60719.13728821161, 0.0], time: 37.408
steps: 259754, episodes: 151, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60562.646973649986, 0.0], time: 39.338
steps: 261754, episodes: 152, mean episode reward: 800.0, agent episode reward: [0.0, 800.0], [59182.967944151314, 800.0], time: 38.057
steps: 263754, episodes: 153, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60986.82558316201, 0.0], time: 38.809
steps: 265754, episodes: 154, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60817.627129285705, 0.0], time: 38.827
steps: 267754, episodes: 155, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61441.1628856897, 0.0], time: 38.483
steps: 269754, episodes: 156, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59944.906973782, 0.0], time: 38.326
steps: 271754, episodes: 157, mean episode reward: 1600.0, agent episode reward: [800.0, 800.0], [58441.99199510857, 800.0], time: 39.707
steps: 273754, episodes: 158, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60518.21619950668, 0.0], time: 37.753
steps: 275754, episodes: 159, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60970.59759705523, 0.0], time: 39.518
steps: 277754, episodes: 160, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57771.73237955189, 0.0], time: 38.639
steps: 279754, episodes: 161, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61561.487396156415, 0.0], time: 38.276
steps: 281754, episodes: 162, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60492.99950122845, 0.0], time: 38.792
steps: 283754, episodes: 163, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60769.74076010226, 0.0], time: 39.608
steps: 285754, episodes: 164, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60573.29017170856, 0.0], time: 37.968
steps: 287754, episodes: 165, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61354.9100349554, 0.0], time: 39.158
steps: 289754, episodes: 166, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [62469.128321504446, 0.0], time: 38.505
steps: 291754, episodes: 167, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56894.36385155113, 0.0], time: 38.138
steps: 293754, episodes: 168, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60945.40203473437, 0.0], time: 38.643
steps: 295754, episodes: 169, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61663.61641186745, 0.0], time: 38.395
steps: 297754, episodes: 170, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61412.455011116144, 0.0], time: 37.786
steps: 299754, episodes: 171, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61552.09141552663, 0.0], time: 39.332
steps: 301754, episodes: 172, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61555.998163221935, 0.0], time: 38.019
steps: 303754, episodes: 173, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [62606.13331445528, 0.0], time: 39.979
steps: 305754, episodes: 174, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [63395.809123129235, 0.0], time: 38.556
steps: 307754, episodes: 175, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61784.17307889677, 0.0], time: 38.343
steps: 309754, episodes: 176, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58651.837601223, 0.0], time: 38.92
steps: 311754, episodes: 177, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [63205.21782780456, 0.0], time: 39.489
steps: 313754, episodes: 178, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61729.93135247467, 0.0], time: 37.765
steps: 315754, episodes: 179, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [62241.662870390544, 0.0], time: 39.863
steps: 317754, episodes: 180, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60369.558117931665, 0.0], time: 53.422
steps: 319754, episodes: 181, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61624.17646580701, 0.0], time: 38.358
steps: 321754, episodes: 182, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61434.4469670739, 0.0], time: 38.355
steps: 323754, episodes: 183, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61608.125297013394, 0.0], time: 39.633
steps: 325754, episodes: 184, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60986.828076292004, 0.0], time: 37.683
steps: 327754, episodes: 185, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60430.54036526073, 0.0], time: 39.184
steps: 329754, episodes: 186, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60127.241740047524, 0.0], time: 38.585
steps: 331754, episodes: 187, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [62102.5023035211, 0.0], time: 44.072
steps: 333754, episodes: 188, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [62828.13331158399, 0.0], time: 46.187
steps: 335754, episodes: 189, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [62069.472699009435, 0.0], time: 39.275
steps: 337754, episodes: 190, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61392.808032843954, 0.0], time: 37.778
steps: 339754, episodes: 191, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [61309.35772712719, 0.0], time: 38.546
steps: 341754, episodes: 192, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [62082.60994711131, 0.0], time: 38.663
steps: 343754, episodes: 193, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58480.57593413736, 0.0], time: 38.458
steps: 345754, episodes: 194, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58773.15744846838, 0.0], time: 39.062
steps: 347754, episodes: 195, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57737.89418658464, 0.0], time: 39.265
steps: 349754, episodes: 196, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58199.05529374675, 0.0], time: 37.72
steps: 351754, episodes: 197, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57961.969776517064, 0.0], time: 39.744
steps: 353754, episodes: 198, mean episode reward: 800.0, agent episode reward: [0.0, 800.0], [59235.414623269244, 800.0], time: 38.69
steps: 355754, episodes: 199, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58105.002085349115, 0.0], time: 38.273
steps: 357754, episodes: 200, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58399.93030783197, 0.0], time: 38.777
steps: 359754, episodes: 201, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57968.48313080469, 0.0], time: 39.645
steps: 361754, episodes: 202, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59414.13187211342, 0.0], time: 37.706
steps: 363754, episodes: 203, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58547.46632914757, 0.0], time: 38.934
steps: 365754, episodes: 204, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57355.375551791556, 0.0], time: 34.216
steps: 367754, episodes: 205, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58470.91011833798, 0.0], time: 38.635
steps: 369754, episodes: 206, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58849.44879176031, 0.0], time: 35.104
steps: 371754, episodes: 207, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54986.37620823635, 0.0], time: 38.492
steps: 373754, episodes: 208, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54204.7959866927, 0.0], time: 38.949
steps: 375754, episodes: 209, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58689.39574910873, 0.0], time: 39.24
steps: 377754, episodes: 210, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56998.34697856907, 0.0], time: 37.779
steps: 379754, episodes: 211, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58407.3730330412, 0.0], time: 39.679
steps: 381754, episodes: 212, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58742.68430372561, 0.0], time: 38.682
steps: 383754, episodes: 213, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58922.58319639261, 0.0], time: 38.347
steps: 385754, episodes: 214, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58371.949559145636, 0.0], time: 38.932
steps: 387754, episodes: 215, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58788.251033485256, 0.0], time: 38.073
steps: 389754, episodes: 216, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58153.413265906944, 0.0], time: 37.927
steps: 391754, episodes: 217, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58515.48727425476, 0.0], time: 39.471
steps: 393754, episodes: 218, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57923.091978658915, 0.0], time: 37.664
steps: 395754, episodes: 219, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57623.03515673344, 0.0], time: 39.047
steps: 397754, episodes: 220, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58884.62117323566, 0.0], time: 38.875
steps: 399754, episodes: 221, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58482.4804477381, 0.0], time: 37.908
steps: 401754, episodes: 222, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58036.23483481113, 0.0], time: 38.531
steps: 403754, episodes: 223, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54570.72185187184, 0.0], time: 39.63
steps: 405754, episodes: 224, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58130.394031231735, 0.0], time: 37.703
steps: 407754, episodes: 225, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58529.3934031986, 0.0], time: 39.84
steps: 409754, episodes: 226, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58713.906630149024, 0.0], time: 38.756
steps: 411754, episodes: 227, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58991.38123570949, 0.0], time: 38.186
steps: 413754, episodes: 228, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58120.38311323087, 0.0], time: 38.615
steps: 415754, episodes: 229, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58333.87377290367, 0.0], time: 38.543
steps: 417754, episodes: 230, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58348.4482798709, 0.0], time: 38.292
steps: 419754, episodes: 231, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58382.39181237653, 0.0], time: 35.441
steps: 421754, episodes: 232, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58462.97144625078, 0.0], time: 38.326
steps: 423754, episodes: 233, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59098.72269045852, 0.0], time: 38.905
steps: 425754, episodes: 234, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58384.79755506085, 0.0], time: 39.912
steps: 427754, episodes: 235, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58611.76454475405, 0.0], time: 37.964
steps: 429754, episodes: 236, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58558.700364211116, 0.0], time: 39.784
steps: 431754, episodes: 237, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57455.67458177797, 0.0], time: 38.897
steps: 433754, episodes: 238, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58660.9662937455, 0.0], time: 38.315
steps: 435754, episodes: 239, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59227.09112529791, 0.0], time: 39.063
steps: 437754, episodes: 240, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58827.58422426681, 0.0], time: 39.06
steps: 439754, episodes: 241, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58855.36204988089, 0.0], time: 38.582
steps: 441754, episodes: 242, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58511.89314424186, 0.0], time: 50.675
steps: 443754, episodes: 243, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58508.18356956805, 0.0], time: 39.129
steps: 445754, episodes: 244, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58346.92565674936, 0.0], time: 38.666
steps: 447754, episodes: 245, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58690.46070512836, 0.0], time: 39.035
steps: 449754, episodes: 246, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59546.90896437584, 0.0], time: 39.39
steps: 451754, episodes: 247, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57810.706989280116, 0.0], time: 37.573
steps: 453754, episodes: 248, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55331.95100351868, 0.0], time: 40.037
steps: 455754, episodes: 249, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58301.24627505615, 0.0], time: 39.146
steps: 457754, episodes: 250, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58572.55664615515, 0.0], time: 38.827
steps: 459754, episodes: 251, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54629.37606383872, 0.0], time: 39.27
steps: 461754, episodes: 252, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60086.9321882481, 0.0], time: 39.671
steps: 463754, episodes: 253, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58499.61792276677, 0.0], time: 37.572
steps: 465754, episodes: 254, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58499.122006168065, 0.0], time: 39.732
steps: 467754, episodes: 255, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58867.085860213665, 0.0], time: 38.085
steps: 469754, episodes: 256, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59060.124079851215, 0.0], time: 43.164
steps: 471754, episodes: 257, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58299.95217142167, 0.0], time: 45.397
steps: 473754, episodes: 258, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59219.5546950668, 0.0], time: 38.829
steps: 475754, episodes: 259, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58341.15577678517, 0.0], time: 38.516
steps: 477754, episodes: 260, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59089.5517636815, 0.0], time: 38.857
steps: 479754, episodes: 261, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58794.70917190803, 0.0], time: 39.698
steps: 481754, episodes: 262, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58360.193399286945, 0.0], time: 37.639
steps: 483754, episodes: 263, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58740.91270655549, 0.0], time: 39.659
steps: 485754, episodes: 264, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59434.8742342567, 0.0], time: 53.382
steps: 487754, episodes: 265, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57816.913721110024, 0.0], time: 38.599
steps: 489754, episodes: 266, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54759.641934965504, 0.0], time: 39.017
steps: 491754, episodes: 267, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59017.86015669313, 0.0], time: 39.515
steps: 493754, episodes: 268, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55647.73199781999, 0.0], time: 37.83
steps: 495754, episodes: 269, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58247.874100180175, 0.0], time: 39.661
steps: 497754, episodes: 270, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57640.67808311321, 0.0], time: 39.097
steps: 499754, episodes: 271, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58312.58271115609, 0.0], time: 46.547
steps: 501754, episodes: 272, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58447.527838652735, 0.0], time: 46.217
steps: 503754, episodes: 273, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59435.825491552074, 0.0], time: 39.907
steps: 505754, episodes: 274, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58886.76397626332, 0.0], time: 38.403
steps: 507754, episodes: 275, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58061.46120996488, 0.0], time: 38.753
steps: 509754, episodes: 276, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59232.94044396236, 0.0], time: 39.096
steps: 511754, episodes: 277, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58843.430473169814, 0.0], time: 38.486
steps: 513754, episodes: 278, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58913.457780963785, 0.0], time: 38.982
steps: 515754, episodes: 279, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59608.535745093744, 0.0], time: 39.377
steps: 517754, episodes: 280, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58759.55538394715, 0.0], time: 34.425
steps: 519754, episodes: 281, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58801.47511541047, 0.0], time: 39.277
steps: 521754, episodes: 282, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58374.71189320714, 0.0], time: 39.206
steps: 523754, episodes: 283, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54888.843573988204, 0.0], time: 37.764
steps: 525754, episodes: 284, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58928.99349699982, 0.0], time: 39.871
steps: 527754, episodes: 285, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58605.74646128973, 0.0], time: 38.904
steps: 529754, episodes: 286, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60478.89935419663, 0.0], time: 38.419
steps: 531754, episodes: 287, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59599.74365068321, 0.0], time: 39.255
steps: 533754, episodes: 288, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58728.66425387921, 0.0], time: 39.826
steps: 535754, episodes: 289, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58874.24392043008, 0.0], time: 39.25
steps: 537754, episodes: 290, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59217.74756489992, 0.0], time: 35.462
steps: 539754, episodes: 291, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58802.94387471036, 0.0], time: 39.118
steps: 541754, episodes: 292, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57992.11935911166, 0.0], time: 39.679
steps: 543754, episodes: 293, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58911.12631599849, 0.0], time: 39.945
steps: 545754, episodes: 294, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59129.29601680466, 0.0], time: 38.054
steps: 547754, episodes: 295, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58861.12606926952, 0.0], time: 39.91
steps: 549754, episodes: 296, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58372.59534420574, 0.0], time: 39.318
steps: 551754, episodes: 297, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59432.44378261531, 0.0], time: 38.696
steps: 553754, episodes: 298, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59422.93042424337, 0.0], time: 39.608
steps: 555754, episodes: 299, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59344.76370063261, 0.0], time: 39.956
steps: 557754, episodes: 300, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58251.446464844346, 0.0], time: 38.266
steps: 559754, episodes: 301, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59111.70591462339, 0.0], time: 39.863
steps: 561754, episodes: 302, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58957.97069206439, 0.0], time: 39.263
steps: 563754, episodes: 303, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59026.754309748045, 0.0], time: 38.658
steps: 565754, episodes: 304, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58024.13422827892, 0.0], time: 39.374
steps: 567754, episodes: 305, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58889.07296386882, 0.0], time: 39.025
steps: 569754, episodes: 306, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59673.55394297378, 0.0], time: 37.459
steps: 571754, episodes: 307, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58052.085536363375, 0.0], time: 38.896
steps: 573754, episodes: 308, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57358.90144313486, 0.0], time: 37.42
steps: 575754, episodes: 309, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58803.58630571814, 0.0], time: 38.341
steps: 577754, episodes: 310, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58708.830872472034, 0.0], time: 38.431
steps: 579754, episodes: 311, mean episode reward: 1600.0, agent episode reward: [0.0, 1600.0], [60738.819408752715, 1600.0], time: 38.574
steps: 581754, episodes: 312, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55075.03175790168, 0.0], time: 38.347
steps: 583754, episodes: 313, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59373.06339086125, 0.0], time: 38.848
steps: 585754, episodes: 314, mean episode reward: 3200.0, agent episode reward: [0.0, 3200.0], [59253.694918815694, 3200.0], time: 37.422
steps: 587754, episodes: 315, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59350.94422096101, 0.0], time: 38.741
steps: 589754, episodes: 316, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60352.66562069414, 0.0], time: 38.26
steps: 591754, episodes: 317, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58761.77215285419, 0.0], time: 38.071
steps: 593754, episodes: 318, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58828.86040769597, 0.0], time: 38.3
steps: 595754, episodes: 319, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [59158.37428271217, 0.0], time: 37.961
steps: 597754, episodes: 320, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60129.7235351875, 0.0], time: 37.653
steps: 599754, episodes: 321, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58691.106848286356, 0.0], time: 39.119
steps: 601754, episodes: 322, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58900.25073148069, 0.0], time: 37.651
steps: 603754, episodes: 323, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60350.32285015358, 0.0], time: 39.139
steps: 605754, episodes: 324, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52818.14257461758, 0.0], time: 38.495
steps: 607754, episodes: 325, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [51411.651257667385, 0.0], time: 45.815
steps: 609754, episodes: 326, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52404.24747874088, 0.0], time: 45.185
steps: 611754, episodes: 327, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53552.393294054455, 0.0], time: 39.151
steps: 613754, episodes: 328, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53277.75585719842, 0.0], time: 37.439
steps: 615754, episodes: 329, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52715.739280484995, 0.0], time: 38.325
steps: 617754, episodes: 330, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53433.77013220569, 0.0], time: 38.851
steps: 619754, episodes: 331, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53215.11809320769, 0.0], time: 37.825
steps: 621754, episodes: 332, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52724.199361727326, 0.0], time: 46.215
steps: 623754, episodes: 333, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52220.36292840226, 0.0], time: 45.336
steps: 625754, episodes: 334, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53428.66134917423, 0.0], time: 38.437
steps: 627754, episodes: 335, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53898.031088046984, 0.0], time: 37.881
steps: 629754, episodes: 336, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53575.287396351545, 0.0], time: 38.327
steps: 631754, episodes: 337, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53664.26932201328, 0.0], time: 37.936
steps: 633754, episodes: 338, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53898.36253651056, 0.0], time: 37.658
steps: 635754, episodes: 339, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53273.07053733099, 0.0], time: 39.079
steps: 637754, episodes: 340, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52280.65976609078, 0.0], time: 37.943
steps: 639754, episodes: 341, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53425.57901639663, 0.0], time: 39.335
steps: 641754, episodes: 342, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52716.63374887681, 0.0], time: 38.39
steps: 643754, episodes: 343, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53140.45866893687, 0.0], time: 37.832
steps: 645754, episodes: 344, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52701.83102549082, 0.0], time: 38.374
steps: 647754, episodes: 345, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [51959.70711194313, 0.0], time: 39.249
steps: 649754, episodes: 346, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52921.99703325455, 0.0], time: 37.333
steps: 651754, episodes: 347, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53643.135636787556, 0.0], time: 39.153
steps: 653754, episodes: 348, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52548.5260324578, 0.0], time: 37.32
steps: 655754, episodes: 349, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53346.98392201231, 0.0], time: 38.622
steps: 657754, episodes: 350, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53339.896510597835, 0.0], time: 38.382
steps: 659754, episodes: 351, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53538.64144985574, 0.0], time: 37.847
steps: 661754, episodes: 352, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53872.022987872486, 0.0], time: 38.525
steps: 663754, episodes: 353, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53769.04588058035, 0.0], time: 39.3
steps: 665754, episodes: 354, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53971.10041517533, 0.0], time: 37.266
steps: 667754, episodes: 355, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54493.76074345299, 0.0], time: 39.228
steps: 669754, episodes: 356, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54223.83590307407, 0.0], time: 38.869
steps: 671754, episodes: 357, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54141.73131434746, 0.0], time: 37.99
steps: 673754, episodes: 358, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53554.266915638334, 0.0], time: 38.408
steps: 675754, episodes: 359, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53938.73194376693, 0.0], time: 38.884
steps: 677754, episodes: 360, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53398.651804024776, 0.0], time: 37.427
steps: 679754, episodes: 361, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53795.4139156894, 0.0], time: 39.338
steps: 681754, episodes: 362, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54358.3942183074, 0.0], time: 37.26
steps: 683754, episodes: 363, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53300.97461530181, 0.0], time: 38.387
steps: 685754, episodes: 364, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54082.929848810694, 0.0], time: 38.467
steps: 687754, episodes: 365, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54103.76768671084, 0.0], time: 38.13
steps: 689754, episodes: 366, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53071.90582232767, 0.0], time: 38.147
steps: 691754, episodes: 367, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54283.68050368205, 0.0], time: 39.092
steps: 693754, episodes: 368, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52035.59532313106, 0.0], time: 37.306
steps: 695754, episodes: 369, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [52690.41190974576, 0.0], time: 39.194
steps: 697754, episodes: 370, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54215.010570719634, 0.0], time: 33.624
steps: 699754, episodes: 371, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54489.048218157164, 0.0], time: 38.147
steps: 701754, episodes: 372, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53441.87774597737, 0.0], time: 38.763
steps: 703754, episodes: 373, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53860.70711119301, 0.0], time: 37.255
steps: 705754, episodes: 374, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53072.889288205275, 0.0], time: 39.108
steps: 707754, episodes: 375, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54053.85253560437, 0.0], time: 38.441
steps: 709754, episodes: 376, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53784.95950603513, 0.0], time: 38.027
steps: 711754, episodes: 377, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53832.527690927935, 0.0], time: 38.307
steps: 713754, episodes: 378, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53653.178908204776, 0.0], time: 38.992
steps: 715754, episodes: 379, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54431.97353591961, 0.0], time: 37.542
steps: 717754, episodes: 380, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54733.82770932451, 0.0], time: 38.982
steps: 719754, episodes: 381, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53467.1982362213, 0.0], time: 37.648
steps: 721754, episodes: 382, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54440.79595980822, 0.0], time: 38.75
steps: 723754, episodes: 383, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54981.34427122333, 0.0], time: 38.265
steps: 725754, episodes: 384, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54471.57071802224, 0.0], time: 37.996
steps: 727754, episodes: 385, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54317.22782495525, 0.0], time: 38.745
steps: 729754, episodes: 386, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54864.58730341645, 0.0], time: 39.231
steps: 731754, episodes: 387, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53984.05504709982, 0.0], time: 37.351
steps: 733754, episodes: 388, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53452.54256794327, 0.0], time: 39.086
steps: 735754, episodes: 389, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54295.2816633589, 0.0], time: 38.234
steps: 737754, episodes: 390, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55081.09612057653, 0.0], time: 37.845
steps: 739754, episodes: 391, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54331.04144282626, 0.0], time: 38.66
steps: 741754, episodes: 392, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54988.41765104374, 0.0], time: 38.889
steps: 743754, episodes: 393, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55347.611128804856, 0.0], time: 37.165
steps: 745754, episodes: 394, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54460.73357004903, 0.0], time: 38.763
steps: 747754, episodes: 395, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55109.26247819581, 0.0], time: 37.387
steps: 749754, episodes: 396, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54960.87580141924, 0.0], time: 38.76
steps: 751754, episodes: 397, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53678.27390859107, 0.0], time: 38.867
steps: 753754, episodes: 398, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55617.49612533391, 0.0], time: 38.198
steps: 755754, episodes: 399, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54608.32654986901, 0.0], time: 38.605
steps: 757754, episodes: 400, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56167.28529374652, 0.0], time: 39.139
steps: 759754, episodes: 401, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55056.19212142455, 0.0], time: 37.632
steps: 761754, episodes: 402, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57681.32470156504, 0.0], time: 38.981
steps: 763754, episodes: 403, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55211.34429892341, 0.0], time: 38.273
steps: 765754, episodes: 404, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55778.0822866287, 0.0], time: 37.742
steps: 767754, episodes: 405, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54905.264244839665, 0.0], time: 38.945
steps: 769754, episodes: 406, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54607.15635104651, 0.0], time: 38.0
steps: 771754, episodes: 407, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55217.94656350037, 0.0], time: 37.644
steps: 773754, episodes: 408, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55133.04414531034, 0.0], time: 39.18
steps: 775754, episodes: 409, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55069.92918906289, 0.0], time: 37.405
steps: 777754, episodes: 410, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55841.80343780097, 0.0], time: 39.413
steps: 779754, episodes: 411, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55738.969554286065, 0.0], time: 38.494
steps: 781754, episodes: 412, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56003.820081083046, 0.0], time: 38.169
steps: 783754, episodes: 413, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55630.42416122545, 0.0], time: 38.299
steps: 785754, episodes: 414, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56382.15979791202, 0.0], time: 39.015
steps: 787754, episodes: 415, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55490.81822108986, 0.0], time: 37.207
steps: 789754, episodes: 416, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55377.10082040044, 0.0], time: 38.748
steps: 791754, episodes: 417, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55926.12792714615, 0.0], time: 37.44
steps: 793754, episodes: 418, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [53640.49532731897, 0.0], time: 38.643
steps: 795754, episodes: 419, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55943.560622012155, 0.0], time: 38.334
steps: 797754, episodes: 420, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55367.466408469285, 0.0], time: 38.185
steps: 799754, episodes: 421, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55157.8874307214, 0.0], time: 38.098
steps: 801754, episodes: 422, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55010.62639314304, 0.0], time: 38.901
steps: 803754, episodes: 423, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56546.52864787299, 0.0], time: 37.425
steps: 805754, episodes: 424, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55606.76089265491, 0.0], time: 38.889
steps: 807754, episodes: 425, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55224.808535344455, 0.0], time: 38.529
steps: 809754, episodes: 426, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55725.382818668, 0.0], time: 37.954
steps: 811754, episodes: 427, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56201.54089512014, 0.0], time: 38.569
steps: 813754, episodes: 428, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [54707.78061813628, 0.0], time: 39.003
steps: 815754, episodes: 429, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56378.882615789815, 0.0], time: 37.433
steps: 817754, episodes: 430, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55859.964879930354, 0.0], time: 38.998
steps: 819754, episodes: 431, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55360.96512609715, 0.0], time: 37.358
steps: 821754, episodes: 432, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55918.19624115315, 0.0], time: 37.869
steps: 823754, episodes: 433, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55875.721688508864, 0.0], time: 38.831
steps: 825754, episodes: 434, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56314.187449140256, 0.0], time: 38.103
steps: 827754, episodes: 435, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55222.62745481062, 0.0], time: 38.491
steps: 829754, episodes: 436, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56323.30476876745, 0.0], time: 38.972
steps: 831754, episodes: 437, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56947.52352427932, 0.0], time: 37.098
steps: 833754, episodes: 438, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56074.07806592924, 0.0], time: 38.941
steps: 835754, episodes: 439, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55286.163247778546, 0.0], time: 38.336
steps: 837754, episodes: 440, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56106.15231253774, 0.0], time: 37.97
steps: 839754, episodes: 441, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56163.14687004994, 0.0], time: 38.414
steps: 841754, episodes: 442, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56476.76762989419, 0.0], time: 37.996
steps: 843754, episodes: 443, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55590.20967107552, 0.0], time: 37.664
steps: 845754, episodes: 444, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55378.33712019158, 0.0], time: 39.025
steps: 847754, episodes: 445, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56187.823002397025, 0.0], time: 37.192
steps: 849754, episodes: 446, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56281.68971660508, 0.0], time: 39.492/home/seth/RL/multiagent-RL/maddpg_impl/reward_shaping/embedding_model.py:142: RuntimeWarning: invalid value encountered in sqrt
  s = np.sqrt(np.sum(kernel)) + c

steps: 851754, episodes: 447, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56818.143698814856, 0.0], time: 38.326
steps: 853754, episodes: 448, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56391.84478306406, 0.0], time: 37.753
steps: 855754, episodes: 449, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55790.80031460666, 0.0], time: 38.669
steps: 857754, episodes: 450, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55857.940586119505, 0.0], time: 39.194
steps: 859754, episodes: 451, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55812.25659027563, 0.0], time: 37.282
steps: 861754, episodes: 452, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56341.63911682919, 0.0], time: 38.945
steps: 863754, episodes: 453, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56474.51517919173, 0.0], time: 37.243
steps: 865754, episodes: 454, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56851.010480593875, 0.0], time: 38.225
steps: 867754, episodes: 455, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56179.762000756535, 0.0], time: 38.407
steps: 869754, episodes: 456, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55627.185349843225, 0.0], time: 37.721
steps: 871754, episodes: 457, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57221.439418204485, 0.0], time: 38.49
steps: 873754, episodes: 458, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58712.61636609145, 0.0], time: 39.41
steps: 875754, episodes: 459, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56138.369603346015, 0.0], time: 37.408
steps: 877754, episodes: 460, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55281.27545338732, 0.0], time: 39.096
steps: 879754, episodes: 461, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56458.96808039333, 0.0], time: 38.349
steps: 881754, episodes: 462, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56398.92772752901, 0.0], time: 38.154
steps: 883754, episodes: 463, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56899.91492617428, 0.0], time: 38.599
steps: 885754, episodes: 464, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56926.561771107336, 0.0], time: 37.944
steps: 887754, episodes: 465, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55630.146068135706, 0.0], time: 38.741
steps: 889754, episodes: 466, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56067.30832442529, 0.0], time: 39.186
steps: 891754, episodes: 467, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [60801.18376532452, 0.0], time: 37.495
steps: 893754, episodes: 468, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55700.03364679599, 0.0], time: 38.747
steps: 895754, episodes: 469, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55275.75708030188, 0.0], time: 38.327
steps: 897754, episodes: 470, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55872.72410356815, 0.0], time: 37.819
steps: 899754, episodes: 471, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55858.782315912984, 0.0], time: 38.42
steps: 901754, episodes: 472, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56449.37063203419, 0.0], time: 39.124
steps: 903754, episodes: 473, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56713.64899375745, 0.0], time: 37.583
steps: 905754, episodes: 474, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57174.78981600784, 0.0], time: 39.211
steps: 907754, episodes: 475, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57496.315876705856, 0.0], time: 38.578
steps: 909754, episodes: 476, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56972.126131444624, 0.0], time: 37.91
steps: 911754, episodes: 477, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56920.80280905066, 0.0], time: 38.424
steps: 913754, episodes: 478, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [58600.40236610938, 0.0], time: 38.183
steps: 915754, episodes: 479, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55653.44156317314, 0.0], time: 37.713
steps: 917754, episodes: 480, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57398.96787867209, 0.0], time: 38.935
steps: 919754, episodes: 481, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55049.91230735313, 0.0], time: 37.755
steps: 921754, episodes: 482, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57058.93468754026, 0.0], time: 39.213
steps: 923754, episodes: 483, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56358.36010515028, 0.0], time: 38.649
steps: 925754, episodes: 484, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57537.684037209285, 0.0], time: 38.379
steps: 927754, episodes: 485, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57350.887725708366, 0.0], time: 38.459
steps: 929754, episodes: 486, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56277.368666973445, 0.0], time: 39.353
steps: 931754, episodes: 487, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55935.95145872125, 0.0], time: 37.574
steps: 933754, episodes: 488, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56930.948007541076, 0.0], time: 39.071
steps: 935754, episodes: 489, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56933.06352306191, 0.0], time: 38.515
steps: 937754, episodes: 490, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55655.25796846323, 0.0], time: 38.217
steps: 939754, episodes: 491, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57602.70800708869, 0.0], time: 38.25
steps: 941754, episodes: 492, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57074.60703686939, 0.0], time: 37.915
steps: 943754, episodes: 493, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [57651.73167544417, 0.0], time: 38.043
steps: 945754, episodes: 494, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56423.086999295345, 0.0], time: 39.0
steps: 947754, episodes: 495, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56039.64079116271, 0.0], time: 37.636
steps: 949754, episodes: 496, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55751.290666161425, 0.0], time: 39.128
steps: 951754, episodes: 497, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56737.888721802716, 0.0], time: 38.663
steps: 953754, episodes: 498, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [55305.77544754529, 0.0], time: 37.998
steps: 955754, episodes: 499, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56454.32235593515, 0.0], time: 38.519
steps: 957754, episodes: 500, mean episode reward: 0.0, agent episode reward: [0.0, 0.0], [56133.88525381605, 0.0], time: 38.987
...Finished total of 501 episodes.
2021-12-06 12:33:15.388357: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-12-06 12:33:15.392519: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2021-12-06 12:33:15.392735: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556b2ed91090 executing computations on platform Host. Devices:
2021-12-06 12:33:15.392756: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:157: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:165: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:59: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py:51: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:61: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  warnings.warn("The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/conversions.py:69: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  warnings.warn("The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.")
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:48: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.
  "The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead."
/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base_parallel.py:60: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.
  "The `action_spaces` dictionary is deprecated. Use the `action_space` function instead."
WARNING:tensorflow:From /home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
env is  Wizard_of_Wor
adversary agents number is 1
Using good policy maddpg and adv policy maddpg
Starting iterations...
Traceback (most recent call last):
  File "./maddpg_impl/experiments/train.py", line 419, in <module>
    train(arglist)
  File "./maddpg_impl/experiments/train.py", line 252, in train
    intrinsic_reward_adv = embedding_model_adv.compute_intrinsic_reward(episodic_memory_adv, next_state_emb_adv,new_obs_tensor)
  File "/home/seth/RL/multiagent-RL/maddpg_impl/reward_shaping/embedding_model.py", line 150, in compute_intrinsic_reward
    long_loss = nn.MSELoss()(self.long_embedding(new_obs_tensor),self.long_stable_embedding(new_obs_tensor))
  File "/home/seth/RL/multiagent-RL/maddpg_impl/reward_shaping/embedding_model.py", line 56, in long_embedding
    x = F.relu(self.long_fc(x))
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 91, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/seth/anaconda3/envs/RL/lib/python3.7/site-packages/torch/nn/functional.py", line 1676, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
