# multiagent RL

MPEs，其他没前途

```
python maddpg_impl/experiments/train.py --scenario simple --num-episodes 10000 --reward-shaping-ag --reward-shaping-adv
```
simple

steps: 24975, episodes: 1000, mean episode reward: -36.34611407164568, time: 20.157
agent original episode reward: [-38.32864407390929]
steps: 49975, episodes: 2000, mean episode reward: -8.020974181149247, time: 23.438
agent original episode reward: [-10.584138830929714]
steps: 74975, episodes: 3000, mean episode reward: -3.712442940528463, time: 23.732
agent original episode reward: [-6.466738687149727]
steps: 99975, episodes: 4000, mean episode reward: -3.366530790469512, time: 22.885
agent original episode reward: [-6.096764811252044]
steps: 124975, episodes: 5000, mean episode reward: -3.6626490354070684, time: 25.407
agent original episode reward: [-6.39647575920869]
steps: 149975, episodes: 6000, mean episode reward: -3.2549426195659796, time: 24.752
agent original episode reward: [-5.959914889595211]
steps: 174975, episodes: 7000, mean episode reward: -3.403518373280227, time: 23.383
agent original episode reward: [-6.130013524977343]
steps: 199975, episodes: 8000, mean episode reward: -3.6025480048196012, time: 23.857
agent original episode reward: [-6.341962961945907]
steps: 224975, episodes: 9000, mean episode reward: -3.1651793870065448, time: 23.344
agent original episode reward: [-5.879685151262868]
steps: 249975, episodes: 10000, mean episode reward: -3.2688816867257224, time: 23.573
agent original episode reward: [-5.974002499552302]
steps: 24975, episodes: 1000, mean episode reward: -33.7885209231644, time: 9.06
steps: 49975, episodes: 2000, mean episode reward: -13.308437777869816, time: 13.271
steps: 74975, episodes: 3000, mean episode reward: -6.222815708264185, time: 13.303
steps: 99975, episodes: 4000, mean episode reward: -6.089423640662558, time: 14.426
steps: 124975, episodes: 5000, mean episode reward: -5.855388804712153, time: 12.932
steps: 149975, episodes: 6000, mean episode reward: -5.685922763185008, time: 13.217
steps: 174975, episodes: 7000, mean episode reward: -6.132507737531997, time: 13.778
steps: 199975, episodes: 8000, mean episode reward: -5.988485257087583, time: 14.666
steps: 224975, episodes: 9000, mean episode reward: -5.5886379291521315, time: 13.175
steps: 249975, episodes: 10000, mean episode reward: -6.228843881248522, time: 13.571

simple_adversary
steps: 49975, episodes: 2000, mean episode reward: -11.645646422222487, agent episode reward: [-16.259174104751967, 2.3067638412647384, 2.3067638412647384], time: 46.882
steps: 74975, episodes: 3000, mean episode reward: 3.8494628305229517, agent episode reward: [-9.543227483391165, 6.696345156957059, 6.696345156957059], time: 45.435
steps: 99975, episodes: 4000, mean episode reward: 3.063686187655674, agent episode reward: [-8.984358597525414, 6.024022392590544, 6.024022392590544], time: 47.028
steps: 124975, episodes: 5000, mean episode reward: 2.317952873480835, agent episode reward: [-9.49730080915911, 5.907626841319974, 5.907626841319974], time: 46.145
steps: 149975, episodes: 6000, mean episode reward: 1.8208361134676088, agent episode reward: [-9.952781188520435, 5.886808650994023, 5.886808650994023], time: 46.775
steps: 174975, episodes: 7000, mean episode reward: 1.1445133714896762, agent episode reward: [-9.90502958919189, 5.524771480340783, 5.524771480340783], time: 46.217
steps: 199975, episodes: 8000, mean episode reward: 0.5601862275390093, agent episode reward: [-10.025229534859541, 5.292707881199276, 5.292707881199276], time: 47.877
steps: 224975, episodes: 9000, mean episode reward: 0.015465012247968105, agent episode reward: [-9.857542420207206, 4.9365037162275875, 4.9365037162275875], time: 48.166
steps: 249975, episodes: 10000, mean episode reward: -0.12289679943118116, agent episode reward: [-10.54141945972123, 5.209261330145024, 5.209261330145024], time: 48.203
steps: 24975, episodes: 1000, mean episode reward: -15.574223907331572, agent episode reward: [-34.72696804456749, 9.576372068617959, 9.576372068617959], time: 53.624
adv agent original episode reward: [-36.38703481200973]
agent original episode reward: [7.951935799770319, 7.951935799770319]
steps: 49975, episodes: 2000, mean episode reward: -9.946466519819001, agent episode reward: [-20.093100755139954, 5.073317117660475, 5.073317117660475], time: 68.421
adv agent original episode reward: [-22.574850295793162]
agent original episode reward: [2.6398947992844617, 2.6398947992844617]
steps: 74975, episodes: 3000, mean episode reward: 10.822530118784714, agent episode reward: [-7.237356530943906, 9.02994332486431, 9.02994332486431], time: 69.364
adv agent original episode reward: [-9.597601692029743]
agent original episode reward: [6.7728516712658955, 6.7728516712658955]
steps: 99975, episodes: 4000, mean episode reward: 10.028960830268122, agent episode reward: [-6.6425528028324585, 8.33575681655029, 8.33575681655029], time: 70.263
adv agent original episode reward: [-9.036928409220428]
agent original episode reward: [6.06950157504299, 6.06950157504299]
steps: 124975, episodes: 5000, mean episode reward: 9.539786375911303, agent episode reward: [-7.2962982722360215, 8.418042324073662, 8.418042324073662], time: 68.492
adv agent original episode reward: [-9.748964043115288]
agent original episode reward: [6.099761269218143, 6.099761269218143]
steps: 149975, episodes: 6000, mean episode reward: 10.478456675674794, agent episode reward: [-8.127266964604221, 9.302861820139508, 9.302861820139508], time: 72.866
adv agent original episode reward: [-10.696339621526255]
agent original episode reward: [6.803660494083845, 6.803660494083845]
steps: 174975, episodes: 7000, mean episode reward: 9.464203601776731, agent episode reward: [-7.538764607043789, 8.501484104410261, 8.501484104410261], time: 70.232
adv agent original episode reward: [-10.200330760339504]
agent original episode reward: [5.870726727797765, 5.870726727797765]
steps: 199975, episodes: 8000, mean episode reward: 8.392483168430331, agent episode reward: [-7.389353105027674, 7.890918136729004, 7.890918136729004], time: 64.445
adv agent original episode reward: [-9.98288395531514]
agent original episode reward: [5.347559657566214, 5.347559657566214]
steps: 224975, episodes: 9000, mean episode reward: 7.3086163875312335, agent episode reward: [-7.357495239445716, 7.333055813488474, 7.333055813488474], time: 60.3
adv agent original episode reward: [-9.895365117661994]
agent original episode reward: [4.8719918015028725, 4.8719918015028725]
steps: 249975, episodes: 10000, mean episode reward: 7.855418772544113, agent episode reward: [-7.951567193855478, 7.903492983199796, 7.903492983199796], time: 60.618
adv agent original episode reward: [-10.488273743582388]
agent original episode reward: [5.431666146605077, 5.431666146605077]

simple_tag
steps: 24975, episodes: 1000, mean episode reward: -2.8650227250879854, agent episode reward: [2.13, 2.13, 2.13, -9.255022725087985], time: 60.151
steps: 49975, episodes: 2000, mean episode reward: 6.358272240769987, agent episode reward: [4.71, 4.71, 4.71, -7.7717277592300125], time: 98.742
steps: 74975, episodes: 3000, mean episode reward: 10.41610985259725, agent episode reward: [5.53, 5.53, 5.53, -6.173890147402748], time: 97.989
steps: 99975, episodes: 4000, mean episode reward: 13.327356909450202, agent episode reward: [6.97, 6.97, 6.97, -7.582643090549798], time: 107.741
steps: 124975, episodes: 5000, mean episode reward: 45.778931534881856, agent episode reward: [23.4, 23.4, 23.4, -24.421068465118143], time: 88.559
steps: 149975, episodes: 6000, mean episode reward: 23.905839566215576, agent episode reward: [15.92, 15.92, 15.92, -23.85416043378443], time: 74.101
steps: 174975, episodes: 7000, mean episode reward: 10.817832584134496, agent episode reward: [9.48, 9.48, 9.48, -17.6221674158655], time: 76.379
steps: 199975, episodes: 8000, mean episode reward: 12.37979005048368, agent episode reward: [8.45, 8.45, 8.45, -12.970209949516322], time: 82.762
steps: 224975, episodes: 9000, mean episode reward: 11.652599414825872, agent episode reward: [7.15, 7.15, 7.15, -9.797400585174127], time: 81.615
steps: 249975, episodes: 10000, mean episode reward: 13.02280404655884, agent episode reward: [8.03, 8.03, 8.03, -11.067195953441159], time: 78.915
steps: 24975, episodes: 1000, mean episode reward: 1.8657211160077363, agent episode reward: [3.5270136554127838, 3.5270136554127838, 3.5270136554127838, -8.715319850230618], time: 139.352
adv agent original episode reward: [2.11, 2.11, 2.11]
agent original episode reward: [-10.193638244425726]
steps: 49975, episodes: 2000, mean episode reward: 16.06874697826312, agent episode reward: [6.715713948594773, 6.715713948594773, 6.715713948594773, -4.078394867521197], time: 150.316
adv agent original episode reward: [4.33, 4.33, 4.33]
agent original episode reward: [-6.064945648069015]
steps: 74975, episodes: 3000, mean episode reward: 23.203422742973714, agent episode reward: [9.33281176335301, 9.33281176335301, 9.33281176335301, -4.795012547085319], time: 147.838
adv agent original episode reward: [6.36, 6.36, 6.36]
agent original episode reward: [-7.032699227887287]
steps: 99975, episodes: 4000, mean episode reward: 25.4442412584454, agent episode reward: [10.504668730235, 10.504668730235, 10.504668730235, -6.069764932259604], time: 148.209
adv agent original episode reward: [7.58, 7.58, 7.58]
agent original episode reward: [-8.582817072231505]
steps: 124975, episodes: 5000, mean episode reward: 36.57271633713177, agent episode reward: [15.915597976997663, 15.915597976997663, 15.915597976997663, -11.174077593861208], time: 147.474
adv agent original episode reward: [12.96, 12.96, 12.96]
agent original episode reward: [-13.803332608818797]
steps: 149975, episodes: 6000, mean episode reward: 55.540421076698316, agent episode reward: [26.0418492937717, 26.0418492937717, 26.0418492937717, -22.5851268046168], time: 145.491
adv agent original episode reward: [23.09, 23.09, 23.09]
agent original episode reward: [-25.294899963031774]
steps: 174975, episodes: 7000, mean episode reward: 44.423569122275374, agent episode reward: [23.818513031538256, 23.818513031538256, 23.818513031538256, -27.031969972339386], time: 145.798
adv agent original episode reward: [20.91, 20.91, 20.91]
agent original episode reward: [-29.962662393642674]
steps: 199975, episodes: 8000, mean episode reward: 28.244546278258415, agent episode reward: [16.05835916824473, 16.05835916824473, 16.05835916824473, -19.93053122647579], time: 145.936
adv agent original episode reward: [13.07, 13.07, 13.07]
agent original episode reward: [-23.118055129028324]
steps: 224975, episodes: 9000, mean episode reward: 23.52739757438482, agent episode reward: [12.631813556140601, 12.631813556140601, 12.631813556140601, -14.368043094036976], time: 146.704
adv agent original episode reward: [9.45, 9.45, 9.45]
agent original episode reward: [-17.709216704859962]
steps: 249975, episodes: 10000, mean episode reward: 23.547046578831647, agent episode reward: [11.587567657368675, 11.587567657368675, 11.587567657368675, -11.215656393274374], time: 147.365
adv agent original episode reward: [8.36, 8.36, 8.36]
agent original episode reward: [-14.64864431377723]

simple_speaker_listener
steps: 24975, episodes: 1000, mean episode reward: -140.22403123260224, time: 44.346
agent original episode reward: [-71.88730682051147, -71.88730682051147]
steps: 49975, episodes: 2000, mean episode reward: -82.6530549295371, time: 51.119
agent original episode reward: [-43.51673934709374, -43.51673934709374]
steps: 74975, episodes: 3000, mean episode reward: -48.285429470507516, time: 50.607
agent original episode reward: [-26.55910807061229, -26.55910807061229]
steps: 99975, episodes: 4000, mean episode reward: -43.44010208376127, time: 50.558
agent original episode reward: [-24.413038067685243, -24.413038067685243]
steps: 124975, episodes: 5000, mean episode reward: -38.06179802051515, time: 50.817
agent original episode reward: [-21.887524693224183, -21.887524693224183]
steps: 149975, episodes: 6000, mean episode reward: -39.035399182570515, time: 50.763
agent original episode reward: [-22.49619301108386, -22.49619301108386]
steps: 174975, episodes: 7000, mean episode reward: -37.755480344606056, time: 50.655
agent original episode reward: [-21.97396243421237, -21.97396243421237]
steps: 199975, episodes: 8000, mean episode reward: -35.09319121150809, time: 50.739
agent original episode reward: [-20.76903364363154, -20.76903364363154]
steps: 224975, episodes: 9000, mean episode reward: -36.997487661256805, time: 50.958
agent original episode reward: [-21.854461363685637, -21.854461363685637]
steps: 249975, episodes: 10000, mean episode reward: -36.4626510910893, time: 50.925
agent original episode reward: [-21.718212713439936, -21.718212713439936]

steps: 24975, episodes: 1000, mean episode reward: -153.3029199149384, time: 12.52
steps: 49975, episodes: 2000, mean episode reward: -143.06264352324703, time: 18.992
steps: 74975, episodes: 3000, mean episode reward: -58.83754488678989, time: 18.779
steps: 99975, episodes: 4000, mean episode reward: -51.319366472533325, time: 18.889
steps: 124975, episodes: 5000, mean episode reward: -39.46129414341748, time: 18.931
steps: 149975, episodes: 6000, mean episode reward: -31.59272499466403, time: 18.989
steps: 174975, episodes: 7000, mean episode reward: -31.11522348735243, time: 18.859
steps: 199975, episodes: 8000, mean episode reward: -32.037881805576596, time: 18.957
steps: 224975, episodes: 9000, mean episode reward: -32.55319476323421, time: 19.0
steps: 249975, episodes: 10000, mean episode reward: -32.20633825272329, time: 18.93